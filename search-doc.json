[{"title":"You Win in the Locker Room First","type":0,"sectionRef":"#","url":"docs/books/business-books/you-win-in-the-locker-room-first","content":"You Win in the Locker Room First Website About the authors: Jon Gordon and Mike Smith The 7 C's to build a winning team in sports, business, and life. Culture Everyone creates your cultureSustained culture = sustained successYou have to fight for your culture and team Contagious Contagious with belief and positive attitudeNo \"energy vampires allowed\"No complaining Consistent Consistency wins in the long runIt's not ok to be moodyBe consistent with your desire to be greatComplacency is a disease You become complacent when you start believe prior successes are going to ensure future success Consistently improveBe humble and hungry Humble Don't think you know everything. Be a life long learner who is always seeking ways to learn, grow, and improveSee everyone as teachers and learn from all. Even your competition.Be open to new ideas and strategiesToday's headlines are tomorrows fish wrap Hungry Seek out new ideas, strategies, and ways to push yourself out of your comfort zoneBe willing to pay the price that requires greatnessBecome the hardest working team you know Communicate Listen Lead by walking around Leave footprints in every room of the building Research says you can't be stressed and thankful at the same time. Connect Unplug from technology and plug into people Commitment Serve to be greatCommitment is spelled T-I-M-EIt's not about youLose your ego Care Care MoreLove Tough The Big C Coaching The Other Big C Character","keywords":""},{"title":"Enterprise Engineering Culture","type":0,"sectionRef":"#","url":"blog/2019/08/07/engineering-culture","content":"","keywords":""},{"title":"What is an Engineering Culture?","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#what-is-an-engineering-culture","content":"What is engineering culture and what does it mean to large enterprises? Engineering culture is a term that is often heard at corporations, but is not specifically defined for those trying to live it. Putting it simply, an engineering culture is a common direction of views and values which drive innovation, planning, designing, architecting, and development behaviors. These behaviors can range from how software is developed, to how documentation is maintained, to how teams handle issues, and how and when work is done. While it's important to provide a clear vision and sense of direction for the organization, to expect all employees in a single organization to share exactly the same engineering values and practices is unrealistic. This is why engineering culture should be defined to support the unique needs of separate lines of business across a company. As long as we share the common goals of improving the industry in which we work and reducing costs, there's wiggle room in the \"how\" and it's up to engineers to pave the path and educate others we work with on what works for individual teams. There are three main categories that drive a positive engineering culture: ownership, accountability, and transparency. "},{"title":"Three Pillars of Engineering Culture","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#three-pillars-of-engineering-culture","content":""},{"title":"Ownership","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#ownership","content":"Ownership is essential to building a strong engineering culture because there needs to be a clearly defined owner or expert for each component of an application. This is important because there are many areas at many large organizations that do not have definite ownership and responsibilities resulting in operational corrections passed off from team to team. Ownership is a team concept. Ownership or expertise does not imply that only an individual is the owner of anything, but rather, a team is responsible for owning their service or application and being the experts in a given area. Having direct ownership of your creations and code helps fill gaps that existed previously. Ownership forces a culture where there's less of a \"that's not my job\" mentality. Instead of cold-transferring an incident or ticket over the wall or handing something off, the team becomes directly responsible for the entirety of a service or application, including front end, middle tier, back end, database development, storage, load balancing, and security. There is also much less time wasted with approvals or access issues to different areas of an application. For example, an application owner at a large organization today may not know what kind of database their application utilizes or how the disaster recovery redundancy is instrumented because \"there is a team that handles that for us.\" Having this total ownership over the entirety of a product drives teams to know their applications better, resulting in better monitoring, reduced time to detection, and quicker recoveries. Ownership should not impact caring about other aspects of the business, which could include searching for new solutions or potential alternatives to issues or the way things are done currently at the enterprise, both for the owners of a specific service or application and the people taking advice or seeking assistance from owners or experts. It means taking an interest in upstream and downstream dependencies of 'your' system, ensuring a seamless customer experience; which is what ownership is all about. "},{"title":"Accountability","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#accountability","content":"Being accountable is a direct result of ownership. You and your team own the code and your team is responsible to support it. Accountability creates more incentive to do things well if the writers of code deal with problems directly. It naturally compels us to focus on details and taking responsibility for all components of your service or application, ultimately resulting in better quality. There is far more incentive to write better code and test more frequently and thoroughly when the owners and creators of code are also responsible for supporting it once it is pushed to a production environment. There is also more drive to create more detailed, accurate, and more frequently updated documentation surrounding how to support a product or service. Having accountability helps ensure that projects and milestones stay on schedule. When there is more control over the entirety of the service or application, it makes it more possible to meet deadlines by removing obstacles or blockers that may exist in a managed service model. The direction that should be strived for is one where the days of submitting a ticket to another team and waiting for your item to be prioritized in their queue are gone. With a DevOps model and push for engineering culture, you are accountable for the results you get and when you get them. Accountability encourages collaboration with peers as you own a product with your teammates. Together you are responsible for both the deliverables and success of your product. "},{"title":"Transparency","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#transparency","content":"Transparency is the foundation of a great engineering culture because knowledge should be shared and encouraged to be leveraged in new and unique ways. A great engineering culture builds upon each others' successes and failures. The reason an engineer puts time and effort into figuring out a new solution or getting a new product working is so others will use it and build on it. An example of this may be where it takes an engineer a substantial amount of time to install and learn Prometheus and document all the steps to get it working and someone then turns around and is able to implement Prometheus in their own environment for their application in a few short hours. Engineers should be encouraged by that behavior because it means that they were successful! Someone else has now benefitted from the engineer's failures and successes and was able to instrument their own Prometheus service because the documentation and environment was open to view. They are also able to view the code and make modifications to fit their own needs and expand the knowledge base and community further. The example above touches on the fact that transparency allows for greater potential for contribution. This is one of the staples of an engineering culture and building on work that others have done. The opportunity for contribution is powerful since it's unclear who may contribute and in what ways they may contribute. Ensuring there is opportunity is a must if others are to make an impact. Being transparent in all areas creates a natural shared consciousness. Getting large groups of people to move in the same direction and act as a more efficient unit is difficult. When everyone has a better idea of the goals of the organization, they are more likely to move that way. Generally, being honest and transparent will lead to people being confident you are always being mindful to work with the best intentions of the company and the people around you and this results in a natural feeling of assuming positive intent from coworkers and leadership. Being honest and transparent results in building better relationships over time and gives a more clear direction of work to be done. Those better relationships and better understand of work to be done builds trust. One of the best questions you can ask yourself in any situation in the workplace is, \"what can I do in this situation to build more trust?\" To simply state it, many corporate cultures lack trust. When more trust is built, it results in greater autonomy among persons within the company, but still with the same direction and goals in mind. This autonomy leads to better and faster results. "},{"title":"More Thoughts on Engineering Culture","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#more-thoughts-on-engineering-culture","content":""},{"title":"Opensource","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#opensource","content":"The opensource philosophy should be embraced as opensource software is an essential solution to creating a strong engineering culture. The power of opensource is not just the cost benefits they offer from vended solutions, but the transparency it offers, the global community it provides, and its opportunity for contribution from many. Scalability and cost are big factors in decisions made at large companies. Opensource allows for better scalability because of the lack of licensing agreements or vendor restrictions placed on software and tools. Another way opensource allows for better scalability is the flexibility they provide in distribution of agents or other components that are designed to deliver data or provide other functionalities. A simple example of this could be using a server management solution developed in house to control VM servers using attribute tags to deploy an opensource monitoring solution or different agents or configuration settings to those servers. Imagine automatically pushing out a new monitoring agent version to over 76,000 virtual servers leveraging different technologies and hardware at once. By using opensource software we are no longer reliant on closed source where trust is placed on a company or vendor who wrote the code. We are free to see the source code and better understand how our own applications function in conjunction with other products. This is important in understanding our application fully and being more accountable for the results of our applications. Ultimately, we need to be responsible for the successes and failures of our applications and using opensource software helps reduce reliance on vendors or external companies. We are not expectant on them to aid us and we eliminate the option to place blame on them when applications are not running as intended. On the flip side of this perspective, while opensource software is often a better option than commercial solutions, that does not mean that it is the end all be all. It should be highlighted that there are times and situations where it makes sense to use vended solutions and we need to use our best judgement when weighing options. Leveraging and contributing to opensource projects is a selfless endeavor. There is a lot of opportunity for personal development and opensource removes barriers between innovators and allows for more knowledge from employees and fellow engineers to be shared. There is something fulfilling in creating something of value for other developers and employees within an organization or the broader community. By contributing to opensource solutions you are dedicating your time and talents for the benefit of others and the greater good of software development. You are reducing certain innovations or ideas from being locked behind patents, which almost certainly results in hindering progress from just about every direction. "},{"title":"Focus on Productivity, Not Presence","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#focus-on-productivity-not-presence","content":"This is a seemingly unrelated, yet important aspect of an engineering culture. In a world where the line between work and leisure has become much more fine, office presence or screen time should not be scrutinized as heavily as it has been in the past. For many of us in IT, work and personal life mesh pretty closely with laptop computers, email and instant messaging on our smart devices, and an almost instant feed of information at our fingertips. With this in mind, it's important to recognize that freedom to work when and where we are most productive should be encouraged. No one should mind if documentation gets written at 1am or a feature card gets closed on a Saturday or if a developer pushes new code to a development environment at noon on a Tuesday. If work can be done at desireable days and times, productivity, creativity, and employee satisfaction are increased. The use and acceptance of collaboration tools as an appropriate medium to share information have given us a big leap in efficiency and flexibility to do our work. With tools such as Slack and Github, we are able to be huge contributors to this paradigm shift in when and where work can be completed. In an engineering culture, where we want to promote transparency and collaboration, the use of these types of tools should increase if we want more information to be accessible to more contributors. This is where the real power of these tools is. It's in the subscriber base and the acceptance of their use. By speaking a common language and agreeing to send and retrieve data from these tools as sources, we create a strong community with an organized and simple way to communicate and share thoughts and ideas across every timezone. An example of another collaboration tool is the popular forum, Stack Overflow. Many of us reference this site multiple times a week to assist in developing solutions or addressing issues that we see during our workday. Think about what makes this tool a valuable resource for many of us... It's not in the UI or that is provides some functionality that other tools do not. As discussed, with tools such as Github and Slack, the value lies in the community and subscriber base that have accepted it as a staple in the development community and what information that community has placed in a single location. With collaboration tools we are able to increase productivity by increasing collaboration and reduce some of the obstacles that once existed with a traditional brick and mortar office setting. This is not to say that there is not still great value in building relationships, connecting with people in person, and sharing ideas \"the old fashioned way.\" These collaboration tools should just be utilized in addition to building great relationships with coworkers and fellow nerds around the world. At the end of the day, results are more important than everything else. When, where, and how you accomplish tasks is often irrelevant of the overarching goals. When the outcome is a focus rather than how many hours you sat in your office chair, employee satisfaction increases, which leads to better, more timely, quality results and a more enjoyable culture for everyone. "},{"title":"A Culture Where Failure is Celebrated","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#a-culture-where-failure-is-celebrated","content":"The most important part of creating an engineering culture is creating an environment where failure is seen as a good thing; A culture where creativity can thrive and where we truly celebrate and take positives away when things don't go as planned. In general, freedom and rapid recovery are better than trying to prevent error. If too much error prevention exists it hinders inventive, creative work. This isn't to say not to plan changes and do your due diligence in development and staging environments and celebrating your production application blowing up, but more of an encouragement to try new things and see how they play out. Aim to fail fast, so if you can try something and show that it does not work the way it originally was thought to or certain solution is not a good fit for a particular use case, than it should be done... because much of the time knowing how not to do something or what not to do adds value in the journey of getting to a better solution or a more correct answer. Failing fast also helps with reduction in premature optimization. Premature optimization is where time is wasted on addressing inefficiencies in the wrong areas or when too much time is spent on something that the organization may never actually use or need. Aiming to fail fast is a mindset that will improve efficiencies in your organization and allow you to focus and spend more time and energy on value adding, customer driven work items. There are situations where failure is seen as embarrassing. An example of this may be where an application team sets up raw error rate monitoring for the first time and discover their raw error rate is over 50 percent. Rather than share that information openly, the app team chooses to keep it to themselves out of fear of exposure or further embarrassment. This should not be the case. It's true an over 50 percent raw error rate is not something to be desired, but it should be a great indication that there is opportunity for improvement and growth to both the application itself and those supporting the application. The goal is not to be perfect, but to be better than yesterday. We need to empower employees and coworkers. We need to work to remove roadblocks and processes that prevent doers from doing. There is a lot of talent within organizations and the globally development community. We need to encourage those talented people and remind them they have permission to go out and make things happen. Along with having permission to make things happen, it is important to be reminded that we have permission to fail as well and actually encourage it. As long as efforts are being made to better yourself and your cause and your experiences are documented, the results of your efforts should be appreciated and seen as valuable contributions to the enterprise. In short, failures are the most powerful way in which people learn. By not systematically and culturally taking inevitable failures as a positive, learning, and growing experience, both individuals and the corporation are stunted and failures remain as problems long term. "},{"title":"Conclusion","type":1,"pageTitle":"Enterprise Engineering Culture","url":"blog/2019/08/07/engineering-culture#conclusion","content":"A strong engineering culture is an attitude and mindset much more than it is a tool, job, or individual person. It's a culture with a \"can do\" attitude of problem solving and organizational self-reliance driven by those in development and operations roles working together. Creating an engineering culture is an effort that will take time to unfold. We do not have all the answers in what the best ways to move forward are or what the future may hold. But what is known is how enterprises operate in IT is not good enough. Evolving customer demands and expectations require us to adapt to deliver more quality results for less cost at a faster rate than ever before. Challenge all members of an organization to adapt to change and work together to create solutions to meet customer demands and the enterprises' endeavors. "},{"title":"An American Sickness","type":0,"sectionRef":"#","url":"docs/books/healthcare-books/an-american-sickness","content":"","keywords":""},{"title":"Introduction - Complaint: Unaffordable Healthcare","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#introduction---complaint-unaffordable-healthcare","content":"The American medical system has stopped focusing on health and science. Instead it attends more or less on profits. The healthcare system is in disarray. A common hospital experience includes dozens of doctors sending separate bills that are often undecipherable. The bills are large and and insurance pays a fraction. Imagine if you paid for an airplane ticket and then got separate, inscrutable bills from the airline. Separate bills from the airline, the pilot, the copilot, and the flight attendants. That's how the healthcare market works. In no other industry do prices for a product vary by a factor depending on where it is purchased, as is the case for bills for echocardiograms, MRI scans, and blood tests to gauge thyroid functions, to vitamin D levels. The price of a Prius at a dealership in Princeton, New Jersey isn't five times higher than what you would pay for a Prius in Hackensack or twice the price of a Prius in New Mexico. The price of that car at the very same dealer doesn't depend on your employer, if your self-employed, or unemployed. Why is that the case for healthcare? The United States spends nearly one-fifth of its gross domestic product on healthcare, more than $3 trillion per year. H&P - History and Physical - an organized and disciplined form of record keeping that every doctor uses. The H&P is a remarkable template for understanding complex problems, such as sorting out a patient's multitude of symptoms in order to diagnose properly and to allow for effective treatment. The H&P has predictable components: Chief complaint - What major symptoms does the patient notice?History of the present illness and review of symptoms - How did the problem evolve?Diagnosis and treatment - What is the underlying cause? What can be done to resolve the patient's illness or symptoms? The chief complaint is typically huge, expensive medical care that doesn't reliably deliver quality results. The economist Adam Smith spoke of an \"invisible hand\" with respect to income distribution. In American healthcare, there's a different type of invisible hand at work: it's on the till. "},{"title":"Chapter 1 - The Age of Insurance","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-1---the-age-of-insurance","content":"Amenities and marketing matter more than good care. Prices will rise to whatever the market will bear. "},{"title":"The History: In the Beginning","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-history-in-the-beginning","content":"The earliest health policies were designed to compensate for income lost while workers were ill. The original purpose of health insurance was to mitigate financial disasters brought about by serious illness The first health care insurance model was called Blue Cross Plans. The goal was not to make profit, but to protect patient savings and keep hospitals afloat Once acceptance of health insurance was widespread, a domino effect ensued: hospitals adapted to its financial incentives, which changed how doctors practiced medicine, which revolutionized the types of drugs and devices that manufacturers made and marketed. The money chase was on: no one was protecting the patients. "},{"title":"Chapter 2 - The Age of Hospitals","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-2---the-age-of-hospitals","content":"The cost of hospital services has grown faster than costs in other parts of our healthcare system. From 1997 to 2012, cost of hospital services grew 149% and physician service costs grew 55%. Market economists refer to hospitals as \"sharks\" or \"spending machines.\" With few market forces to curb their behavior, they raise prices as much as they can. Because most hospitals are non-profits, they have no shareholders to answer to and cannot legally show a profit... so they spend money on executive compensation and building fancy gardens and lobbies. There are armies of consultants whose job it is to improve revenue, compensation, and get their slice of the pie. 10% to 15% of revenue goes to billing and collection companies and contractors do things like claims and preapprovals... Those jobs don't even exist in Europe. There is so much surplus capacity, which should result in lower costs, but it doesn't... We get the opposite. It's a market failure but it follows certain logic. It's not a healthcare system. It's an industry. At every point there is a way to make money. "},{"title":"A Doctor's View: One Hospital's Journey from Charity to Profit","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-doctors-view-one-hospitals-journey-from-charity-to-profit","content":"Healthcare has become a great way for the Catholic Church, in particular, to collect money. Eight of the ten largest nonprofit hospital systems in the United States have religious affiliations and names. In 1960, Dr. Albert Starr, a Portland heart surgeon, revolutionized the treatment of heart disease by coinventing and then implanting the first successful artificial heart valve. Millions of patients who would otherwise be dead have benefited from that discovery. Dr. Starr was a medical rock star, a hero, a genuine draw. Yet he was never on a billboard... Marketing and advertising were considered unethical and looked down on. It just wasn't done at that time. In the 1960s, Medicare arrived to cover hospital payments. Between 1968 and 1980 the number of Americans under 65 covered by good, private insurance was at its peak (about 80%, compared to about 67% in 2007). Because patients were no longer forking out cash or writing checks for their care, hospitals began charging more for their services. The original Blues plan at Baylor had paid by the week for hospitalization, but now hospitals like Providence charged for each service at each encounter. In a world populated by doctors, nurses, and nuns, no one really knew how to figure out how much it cost the hospital to remove an appendix, for example. There was no harm in aiming high though, because insurers usually paid whatever was requested. More money began coming in to hospitals, so there was a need to hire more business people to manage it. In the early 1980s, the increasingly powerful hospital administrators held degrees in business or healthcare administration and they paid more attention to the bottom line than to the tradition of medicine. By the late 1990s, Providence hospital no longer wanted pay an ER or clinic doctors a salary. They treated them as independent contractors. That turned doctors into a business. They negotiated contracts that stipulated what percent of revenues that were deserved. Economies of scale don't translate to lower prices. With their market power, big providers can simply demand more. "},{"title":"Behind the Scenes: How Hospitals Got New Business Models","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#behind-the-scenes-how-hospitals-got-new-business-models","content":"The financial structure of a hospital, due to legal and philosophical constraints, has not developed in the same manners as that of commercial corporations. Hospitals are big businesses. Benefactors alone cannot subsidize hospital operations. There is no such thing as a fixed price for a procedure or test. The uninsured pay the highest prices of all. Hospitals' administrators, controllers, and finance committees need accurate cost findings. Similar to executives of supermarkets or department store chains. According to a 2011 article in the newspaper published by the American Medical Association, more business experience was needed to be a hospital executive. The Chief Medical Officer (CMO) remains a common leadership position, but hospitals are creating positions such as chiefs of physical relations, integration, and medical informatics. "},{"title":"Enter the Consultants","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#enter-the-consultants","content":"To ensure the U.S. healthcare system would grow in a cost-effective way, Congress passed a law in 1974 requiring state health-planning agencies to grant approval before hospitals could build new facilities or purchase expensive technology. Agencies granted \"certificates of need\" indicating the community would benefit from the investment. The goal was to avoid duplication of services and overbuilding. That law was rescinded in 1987, resulting in hospitals building whatever they wanted, so long as their revenues supported it. Medical purchases became investments and the medical arms race between hospitals was on. In 2005, Deloitte hired Tommy Thompson (who had previously been secretary of health and human services) as chairman of its global healthcare practice, thereby lending a big name to its outreach in the new sector. Restructuring a hospital as if it were a steel mill or chicken processing plant seemed uncouth to some boards; but second-tier and financially struggling hospitals opened their doors to consultants. Hospital reimbursement is a strategic puzzle. All hospitals have a master price list and adjusting it to maximize income was the focus of Deloitte's strategy. To squeeze more money from the purse, Deloitte advised hospitals to stop billing for items such as gauze rolls, which insurers rarely reimbursed, and to boost charges for services like OR time, oxygen therapy, and prescription drugs. The strategy was all about optimizing payments by raising prices on certain items to cover the cost of other items. While it's legal, it's ethically dubious and not good for patients. It certainly increases the cost of healthcare. For the business departments of hospitals and doctors on staff, the discovery was transformational. The billed price of an item could be completely decoupled from its actual cost. Items that had previously been included in the charge for the operating room or a hospital day could be billed separately. Just about every hospital employs strategic billing, which is enabled and supported by consultants and healthcare advisory firms. Deloitte is ranked number one by revenue in all areas of healthcare consulting - life sciences, payer, provider, and government health. In 2014 it announced record revenues of 34.2 billion dollars, fueled by more than 17% growth in the sector. "},{"title":"Strategic Billing 101: Upcoding and Facility Fees Lead to a $3,400 Needle Stick","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategic-billing-101-upcoding-and-facility-fees-lead-to-a-3400-needle-stick","content":"Upcoding: Doctors were charging things as simple as blood draws as level 5 on a 1 - 5 scale. \"Things that were written down and what was actually done bore little resemblance,\" recalls Dr. Richards. Coding creep (upcoding) grew bolder - until it made no sense at all. If needle sticks and eye infections are coded as level 5, what was a crushed chest from a car accident or a heart attack? Dr. Donald Berwick, a former Medicare administrator, told the CPI that he believed most doctors were not breaking the law, just \"learning how to play the game.\" Around 2000, a hospital decided it would no longer pay physicians a fixed salary; instead, they would be compensated in proportion to the relative value units (RVUs) of the care they dispensed. RVUs are a measure of productivity used to determine medical billing. Facility fees also played a role in the rising cost of healthcare. Facility fees were a logical outgrowth of a period of rapid scientific progress in medicine, which allowed many treatments to move to an outpatient setting (a good thing, right?). Improvements in anesthesia, pain medicine, minimally invasive surgery, and biopsy techniques meant that many procedures and operations could be safely performed without an overnight stay. Because hospitals had traditionally charged a day rate for inpatients, it made sense that insurers (including Medicare) had largely accepted their new practice of charging facility fees for major outpatient care. Facility fees are a unique construct of American healthcare and its business model. Hospitals in Europe don't have them. Nor do other types of businesses in the United States. As Yevgeniy Feyman observed in Health Affairs Blog, \"When you buy anything - a watch, a car, even groceries - you pay a single price for the goods. The Walgreens down the street doesn't add a separate charge to cover its rent, utilities, or the cost of refrigeration units.\" "},{"title":"Closing Departments: Make Money or Die","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#closing-departments-make-money-or-die","content":"The new hospital consultants were experts not just in raising revenues with strategic pricing but also in corporate restructuring. Hospitals traditionally had departments that perennially lost money: emergency rooms, labor and delivery, dialysis centers, drug and treatment programs, and outpatient clinics in poorer neighborhoods that served Medicaid populations. These were part of their mission and a moral obligation (importantly, they also received federal funding to subsidize them). But by the early 2000s, every department had to carry its own weight. As obesity rates climbed, medical equipment companies devised new operations using new products to help combat the condition, and bariatric surgery was a boom field. Companies, hospitals, and doctors' groups lobbied successfully to have insurers pay for it all. Being overweight was rebranded a disease. New machines were purchased based not on medical necessity or even utility but according to financial calculation. Rule 1: More treatment is always better. Default to the most expensive option. "},{"title":"The Business of Medical Training: Hospitals and Cheap Labor","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-business-of-medical-training-hospitals-and-cheap-labor","content":"By 2014 hospitals received about $15 billion a year in government subsidies to support graduate medical education, a number that had been \"increasing for decades.\" These payments include \"direct payments\" to compensate for the theoretical inefficiencies and institutional sacrifice involved in training new doctors, such as longer hospital stays and the need to order more tests for teaching purposes. Studies have not provided proof of those purported inefficiencies and sacrifices that merited special compensation. Stays in teaching hospitals are no longer than those without residents, for example. In fact, there is much to suggest that hospitals have turned residencies into another profitable business. They are learning but are also effectively low-wage labor. The median cost to a hospital for each full-time resident in 2013 was $134,803. That includes a salary of between $50,000 and $80,000. Federal support translates into about $100,000 per resident per year. Researchers have calculated that the value of the work each resident performs annually is $232,726. "},{"title":"The Emergence of Hospital-Hotels","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-emergence-of-hospital-hotels","content":"\"There are an infinite number of ways to spend: amenities, scanners, salaries, building more. Medicare pays hospitals a bonus for performing well on patient surveys, \"so if a patients asks for a test and it won't hurt, they'll get it. It's good for Press Ganey scores. It takes more time and trouble to explain to a patient why they don't need an X-ray, for example.\" "},{"title":"The Most Profitable Nonprofits: The Evolution of Hospital Charity","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-most-profitable-nonprofits-the-evolution-of-hospital-charity","content":"Not-for-profit hospitals are now just as profitable as capitalist corporations, but the excess money flowing in isn't called \"profit\" - it's \"operating surplus.\" The Pittsburgh Post-Gazette has uncovered many of the relevant numbers on UPMC: it is \"Allegheny County's largest property owner, with 656 acres,\" 86% of which is tax exempt. If it were not classified as a nonprofit, \"UPMC would owe the city $20 million more in taxes every year.\" "},{"title":"Observation Status: A New Financial Purgatory in the Hospital","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#observation-status-a-new-financial-purgatory-in-the-hospital","content":"The idea of admitting a patient for \"observation\" has long been an important medical tool but was likely far more useful a few decades ago. As observation has become less important for diagnosis, it has become more important for lucrative billing construct, manipulated by hospitals, insurers, and nursing homes. Medicare penalizes hospitals if patients bounce back thirty days after discharge - the \"readmission penalty.\" But if they were never officially admitted and were merely under \"observation\" they couldn't bounce back. For hospitals and insurers, observation status has benefit. For patients it is a disaster. In 2015 President Obama signed a bill requiring hospitals to notify patients receiving more than twenty-four hours of observation care of their status as outpatients and its varied implications. But that notification comes after the fact, when patients are already supine in their hospital bed and can do little about it. "},{"title":"Chapter 3 - The Age of Physicians","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-3---the-age-of-physicians","content":"Surgeon pledge in 1990: \"I will set my fees commensurate with the services rendered. I will take no part in any arrangement such as fee splitting or itinerant surgery. In 1990 the American College of Surgeons felt it was immoral to be an itinerant surgeon. The current surgeon pledge, adopted in 2004, had been amended to remove the ethical limitations. Surgeons are not the only group of doctors watering down their moral commitments. The American Medical Association's code of ethics has similarly been diluted. "},{"title":"What is a Doctor's Work Worth?","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#what-is-a-doctors-work-worth","content":"Being a doctor is deserving of reasonable compensation, but what is seen as reasonable has escalated to unusual heights. Doctors make more in the US than in other countries and the gap is high in specialties. Primary care doctors make about 40% more in the US than their German counterparts.US orthopedic surgeons make more than twice as much as similar German specialists. As the number of salaries of hospital administrators has risen, doctors have wanted a cut of the profits. "},{"title":"The Price of a Cure: A Brief History of Doctors' Finances","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-price-of-a-cure-a-brief-history-of-doctors-finances","content":"More competitors vying for business doesn't mean better prices; it can drive prices up, not down. According to the Medical Group Management Association, median physician income for all primary care doctors increased 9.9% from 2000 - 2004, compared with 15.8% increase for all non-primary care specialists. Median income for hematologists and oncologists increased 35.6% to $350,000 and median income for diagnostic radiologists increased 36.2% to $407,000. Average real income for all Americans dropped 3% during that time. "},{"title":"Doctors Get the Key to the Vault","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#doctors-get-the-key-to-the-vault","content":"With the new system, constant updates were needed to determine fair value and payment. Medicare assigned that task to the American Medical Association (AMA). Allowing the AMA to determine doctors' payments is similar to letting the American Petroleum Institute decide what BP, Shell, and Exxon Mobile can charge us not just for gas, but for wind and solar power as well. The AMA says the RUC represents the entire medical profession, but it is not really representative of practicing doctors. Researchers have determined the all-important \"time\" component assigned to procedures is often wildly inaccurate. Better surgical approaches, computerized equipment, and more effective anesthesia means surgical times have often declined dramatically since the RBRVS was first used in 1992. The RUC determines the time it takes to perform a service by polling several dozen specialists who do the procedure, which is essentially asking them if they want to be paid more or not... "},{"title":"Strategy #1: Doctor-Entrepreneur-Owner","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-1-doctor-entrepreneur-owner","content":"Ambulatory surgery centers (ASCs) became increasingly popular over the 1980s, 1990s, and into the 2000s. More were owned and run by individual doctors and investors. Doctors saw new income potential, because they could charge facility fees; room rentals for the suites where they applied their trade. Neurologists opened sleep centers where patients could have their brain activity measured and breathing monitored for sleep apnea. Insurance was often billed from $5,000 to $10,000 a night. Regulators tried in vain to curb the practice by requiring physicians to disclose to patients in advance... But these disclosures usually came in a pile of paperwork to sign. Minor interventions doctors had once performed in the office were moved into their centers for financial benefit. "},{"title":"Strategy #2: The Hidden Doctors and Their Mysterious Bills","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-2-the-hidden-doctors-and-their-mysterious-bills","content":"There are physicians who contribute to your care whom you may never see: pathologists, anesthesiologists, radiologists, and emergency medicine docs. The first three are often referred to as the NPC (\"no patient contact:) specialists. Historically, these physicians were hospital employees and their services were embedded in hospital charges just like bandages and beds. They were paid decent salaries but no more. It's surprising today they account for some of the highest, most confusing doctors' bills you're likely to get. By 2014, 65% of the nation's five thousand hospitals had contracted out their emergency department staffing/management function. For those in the industry, it was a win-win situation. Hospitals no longer had to buy malpractice or health insurance or figure out how to staff vacations. Doctors could charge what they thought they were worth. But for patients, this meant the proliferation of separate bills for these doctors' services, from companies with mysterious return addresses in distant states. The game with PARE specialists is they began to refuse to contract with any plan or insurer. \"These guys decide not to contract, so they can charge whatever they want. The patient is over the barrel.\" "},{"title":"Strategy #3: The Pacts Doctors Make","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-3-the-pacts-doctors-make","content":"In 1964 a statute called the Emergency Medical Treatment and Labor Act (EMTALA) was designed to force hospitals to remember their long standing obligations in an era of more commercialized healthcare. All patients who turned up at the emergency room had to be treated, regardless of their ability to pay. Hospitals couldn't turn away sick patients or pregnant women who were poor. But EMTALA doesn't apply to physicians, who are free to pick and choose which patients to accept. "},{"title":"Strategy #4: One Doctor for the Price of Two","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-4-one-doctor-for-the-price-of-two","content":"Over the last 20 years many doctors have managed the feat of having two of themselves, at least for billing purposes. They do it with physician extenders. Physician extender is an umbrella term that refers to the trained ancillary personnel who help doctors and surgeons care for patients, including nurse practitioners, surgical technicians, physician assistants, and midwives. Extending has also become a useful billing construct, enabling doctors to bill for work done by the extenders who work for and with them, as if the doctors themselves were personally dispensing the care. Slowly but surely, the idea that extenders' work constituted as \"billable hours\" became commonplace. The doctor never saw the patient, never had any interaction with the patient, and yet a service can be billed under that physician? Under many private payers, it is not fraud. "},{"title":"Strategy #5: Selling Stuff - Buy and Bill","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-5-selling-stuff---buy-and-bill","content":"More treatment is always better. Default to the most expensive option. Whistle blower lawsuits filed against makers of drugs and devices, alleging bribes and kickbacks to induce doctors to use and overuse high priced products, have become a staple of legal dockets Oncologists prospered buying chemotherapy drugs from manufacturers and infusing them in the office, generally with a hefty markup, a practice known as \"buy and bill.\" As the wholesale price of new drugs jumped again and again, doctors had little motivation to complain because they were allowed a markup that was often a set percentage of cost. Doctors who used more expensive drugs earned far more. The practice of buy and bill increased dramatically in the late 1990s and into the new century. As technologies age, prices can rise rather than fall. There is no free choice. Patients are stuck and they are stuck buying American. "},{"title":"Strategy #6: Upgrades","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-6-upgrades","content":"Medical progress had proved great for patients but not for ophthalmologists' incomes. Doctors have done their best to fix that shortfall. After discovering that ophthalmologic practices were advertising laser cataract surgery, Medicare issued a \"guidance.\" Making an incision - whether by scalpel or laser - was considered part of the surgery fee and could not be billed separately. If patients really wanted \"laser\" surgery they would - once again - have to pay for it themselves. Amenities and marketing matter more than good care. "},{"title":"Strategy #7: Adopt a Surgical Procedure - Money, Money Everywhere","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#strategy-7-adopt-a-surgical-procedure---money-money-everywhere","content":"Thousands of physicians made more than $1,000,000 each from Medicare in 2012 and dozens more than $10,000,000. Medicine is a business. It won't police itself. People have lost faith in the American medical profession - that they would act differently than other businesses - but they were wrong. "},{"title":"Chapter 4 - The Age of Pharmaceuticals","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-4---the-age-of-pharmaceuticals","content":"More competitors vying for business doesn't mean better prices; it can drive prices up, not down. As technologies age, prices can rise rather that fall. Nearly all American manufacturers of mesalamine products figured out ways to move their tax base overseas, using a controversial practice called an inversion that many legal experts regard as tax evasion. "},{"title":"The Modest History of Making Medicines","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-modest-history-of-making-medicines","content":"Brand makers ability to claim extended patent protection, coupled with generics competitors' constant challenges to weak patents, meant that the amendment instead ushered in an era in which multimillion dollar court battles over patents now precede (and delay) each generic entry, driving prices up in the process. A strong patent system and no pricing restriction led to a profound shift of pharmaceutical investment into the United States and away from Europe, where government price setting was increasingly common. By 2002 global drugmakers were spending 82 percent of their money in the United States. The US pharmaceutical industry has grown twice as fast as the economy at large since 1990. There is no free choice. Patients are stuck. And they're stuck buying American. "},{"title":"New Policies Transform an Industry","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#new-policies-transform-an-industry","content":"For much of medical history and into the 1980s, drugs were cheap. Vaccines cost pennies and antibiotics and epinephrine shots were a few dollars. Even the most exotic medicines couldn't list for more than a few hundred dollars a dose. The emergence of successful drug therapy in response of the crisis of HIV/AIDS permanently shifted the long-standing business paradigm of drug approval, pricing, and value. A lifetime of treatment is preferable to cure. Drug companies that used interim measures to prove the success of their products often made empty promises to do follow-up studies to makes sure products approved through these proxy targets actually yielded long-term improvement for patients. An in-depth data investigation by the Milwaukee Journal Sentinel and MedPageToday in 2014 revealed that, thanks to surrogate endpoints, 74% of cancer drugs approved by the FDA during the previous decade ultimately did not extend life by even a single day. Direct-to-consumer drug advertising rose from $166 million in 1993 to $4.2 billion in 2005, and in 2006 it made up nearly 40% of total pharmaceutical promotional spending. In 2000 Merck spent more advertising its new painkiller, Vioxx ($160 million), than Budweiser ($146 million), Pepsi ($126 million), or Nike ($78 million). In 2004 Merck withdrew Vioxx from the market. The company ultimately paid a criminal fine of $950 million for its marketing and sales tactics. Drug advertising is now a constant in our lives. The Supreme Court has protected drug advertising under the guise of free speech. The US one of two countries that allow it, along with New Zealand. "},{"title":"Non-24; or, it Pays to Advertise","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#non-24-or-it-pays-to-advertise","content":"Vanda, a boutique drugmaker based in Washington, DC, had developed a new molecule, called tasimelteon, which resembled melatonin and attached to a melatonin receptor, producing a similar effect. Dr. Sack was an expert on the panel that had unanimously recommended the approval a few months before. He explained he came out to thinking our system of drug approval doesn't make any sense. Because of the FDA's criterion we were following to determine efficacy, he was forced to vote yes, but it made him uncomfortable as the drug was not demonstrably better than melatonin. FDA panels don't consider the cost of products they review. Vanada's Hetlioz was priced at $96,000 ($8,000 a month or $267 per dose)... Now there is a prescription drug that is not as effective as melatonin (which you can buy for $6 for a bottle of over 100 pills). It's eye popping that we could get into this situation and that the new drug can be advertised on TV. Because melatonin is a product of nature the molecule can't be patented. No company will go through the expensive testing and application process to get melatonin formulated as a prescription drug. It's sold as a nutritional supplement and not regulated or tested with great exactitude. The proper dosing of melatonin has never been studied to FDA standards.... But the chemically created analog tasimelteon can be patented. TL:DR Vanda and Hetlioz is a scam. Now that we rely on the profit incentive to motivate drug research, we learn only what the industry deems is profitable for us to find out. "},{"title":"Patent Plays","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#patent-plays","content":"Drug manufacturers seized on the opportunity to get new patents for asthma inhalers with new propellants, removing from the market generic inhalers that had long been available. The usual price of the most common inhaled asthma medicine, albuterol, rose from about $10 to over $100. It costs $7 to $9 in Australia, where it's still generic and even sold over the counter. There are endless ways to extend your product's value through manipulating FDA policies and patent law. File a lawsuitModify the deliveryPlay to the FDA's weaknesses and blind spotsTake it over the counterCreate a combination File a lawsuit - Getting a patent only costs $20,000 and applications are unlimited. There's strong motivation for drug companies to file weak patents. Brand makers take out weak patents, generics challenge them, and the brand makers answer back with an objection. Under Hatch-Waxman, that objection alone sets off a mandatory thirty-month halt in the FDA's consideration of the generic entrant. Modify the delivery - Product hopping - moving products from one form to another. For example, changing a drug from a pill to a chewable tablet. Since generics must be identical in dosage and form to the brand-name drug for a pharmacist to substitute, companies can make moves to succeeded in delaying competition from generics for years. Play to the FDA's weakness and blindspots - Sprays, lotions, and creams don't have to play legal games to fend off competition from generics since these formulations are hard to replicate according to the FDA's extracting standards. For pills, it's relatively straightforward to show the chemical composition is equivalent to the brand and to measure that it produces the same level of medication in the blood. Take it over the counter - Changing a drug's legal status so it can be sold over the counter is sometimes a profitable patent play for pharmaceutical companies (known as an \"OTC switch\"), but it can be a risky decision. Prescriptions are key to charging high prices (because insurers pay) and holding a pharmaceutical monopoly intact. In much of the world, products go over the counter when they are deemed safe enough to take without a doctor's intervention. In the US, it is a business calculation. More treatment is always better, default to the most expensive option. An OTC switch makes products available to a wider audience of buyers but manufacturers have to lower the price considerable because insurance is no longer involved. They also have to establish brand recognition with consumers. According to US law, the same product cannot be on the market as both a prescription and an over-the-counter product. FDA grants any company that takes a prescription drug to OTC status three years of market exclusivity for that sales route, during which other manufacturers are forbidden from making store-brand copies. Create a combination - Patient outrage over the high price of patent plays was the last business barrier drugmakers had to resolve. As pharmaceutical prices increased, insurers imposed co-payments that were a percentage of the bill to encourage the use of cheaper alternatives. "},{"title":"Desperate Patients and Bribes, or Charity?","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#desperate-patients-and-bribes-or-charity","content":"More competitors vying for business doesn't mean better prices. It can drive prices up, not down. Prices will rise to whatever the market will bear. "},{"title":"The Rise of Invisible Robber Barons","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-rise-of-invisible-robber-barons","content":"A different class of backroom mercenaries entered the medical fray in the late 1980s and early 1990s. The pharmacy benefit manager (PBM). Today there are a handful of huge PBMs - Express Scripts, CVS Caremark, OptumRX - and they have enormous sway over your care. PBMs are for profit companies that make money by pocketing a percentage of the discounts they negotiate. Items that end up on the formularies of covered drugs and devices aren't always the ones patients need most or those that work best, but rather the ones on which the PBM has wrangled the best deal, with the best negotiated profit margin. "},{"title":"\"We Regret to Inform You That the Generic Isn't Available","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#we-regret-to-inform-you-that-the-generic-isnt-available","content":"Shortages of cheaper, affordable essential medicines in hospitals have become the new normal. "},{"title":"Generic Sticker Shock at the Pharmacy Counter","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#generic-sticker-shock-at-the-pharmacy-counter","content":"Over 80% of prescriptions in the US are filled with generics, which is a higher percentage than most developed countries. But some generics are no longer a bargain. Intense competition among manufacturers of generic drugs in the early twenty-first century had resulted in some rock-bottom prices for consumers. But the smaller profit margins, plus the lure of easier money to be made by producing more lucrative items, drove some of these manufacturers to quit making older cheap drugs, thereby handing near monopolies to those who remained in the market. To really bring down prices through market competition, probably four or five generics would need to be on the shelves, and that level of competition was rare. As a result, after 2010 generics often settled into a price point just slightly below the brand-name drug. "},{"title":"A Myth: At Least We Get Drugs First","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-myth-at-least-we-get-drugs-first","content":"America pays double, sometimes triple, what other developed countries spend on drugs but takes comfort in getting new treatments and cures first. That is often true, but not always. Cost-saving new treatments have had a maddeningly hard time finding their way into the American bazaar, because lower prices for patients often translated into loss of income for someone with greater clout. "},{"title":"Chapter 5 - The Age of Medical Devices","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-5---the-age-of-medical-devices","content":"There is no such thing as a fixed price for a procedure or test. The uninsured pay the highest prices of all. The medical device sector has quietly matured, devising new financial models and methods that have escaped in-depth scrutiny. The relative insulation makes sense. You likely don't know the name of the catheter that delivered the wire-mesh stent into your heart, or which company made it, and are even less likely to have any idea about its price. Yet these pieces of hardware are frequently the single biggest item on your medical bill, costing many thousands, if not tens of thousands of dollars. Medical devices essentially have no real set price at all. Each year, more people seem to get involved, and each takes a commission or adds a markup as the device moves along the long road from the factory to your body. Device manufacturers are a tight-knit oligopoly with nearly absolute control of distribution. "},{"title":"The Secret History of Medical Devices and Their Regulation","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-secret-history-of-medical-devices-and-their-regulation","content":"Today there is generally far less careful scrutiny of new devices than of new drugs, even though mist drugs can be stopped in an instant if problems emerge and many devices are permanently implanted in the body. Device companies have become the darlings of venture capital and there has been a proliferation of device patents. "},{"title":"Joints Are a Big Business (But Don't Expect a Warranty)","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#joints-are-a-big-business-but-dont-expect-a-warranty","content":"Note to patients: precise success/failure percentages given for a procedure should be a red flag. There is no free choice. Patients are stuck. They are stuck buying American. More treatment is always better. Default to the most expensive option. In an example of medical device pricing given in the book, an insurer would not cover a device, but the hospital would lease it for $10,000. The doctor quoted $7,000. The patient called the manufacturer, who proposed $5,000. She explained to the manufacturer she could not afford it and she only had $1,778 in her health savings account. Their reply was, \"we'll take it!\" The 510(k) program has led to a number of medical disasters. There are no standards for billing. There's money to be made in billing for anything and everything. The most effective way to make sure that the newest, most expensive products ended up in the highest number of patients was in forming tight, codependent relationships with orthopedic surgeons. \"Manufacturers marketed heavily to doctors - \"We have this great new thing!\" With the 510(k) programs, device makers could readily reward the surgeons with patents and profits or funding for pet research projects. In recent years, hospitals have begun fighting back against the medical device industry, suddenly demanding that device reps, like lobbyists, be registered and get permission each time they enter the premises. "},{"title":"How to Get into a Patient's Heart? Follow the Money","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#how-to-get-into-a-patients-heart-follow-the-money","content":"Like drugmakers, device manufacturers have another big line of business to invest in, one that has nothing to do with serving patients: suing over intellectual property. Who really pays for the price of all the legal maneuvering? Patients, of course. "},{"title":"Dr. Elliot's Lonely Crusades","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#dr-elliots-lonely-crusades","content":"There is no free choice. Patients are stuck. They are stuck buying American. Doctors are too. "},{"title":"Chapter 6 - The Age of Testing and Ancillary Services","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-6---the-age-of-testing-and-ancillary-services","content":"In 2014 UnitedHealthcare paid laboratories between $17 and $618 for vitamin D tests, and those done in hospital labs were by far the most costly. There is no such thing as a fixed price for a procedure or test. The uninsured pay the highest prices of all. American patients desire high-priced exploration and treatment if it means their illness will be fixed on the spot. Over the past decade, testing, medical equipment, and ancillary services became to hospitals and clinics what booze is to restaurants: high-profit-margin items that can be billed for nearly any amount. Better still, many insurers require no co-pay for these items once patients have met their deductible, so most patients don't care how much they cost. "},{"title":"An Experiment in Billing","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#an-experiment-in-billing","content":"Prior to 2007 imaging was the fastest-growing component of healthcare costs all over the nation. In Japan, where the price for tests is determined by a panel of doctors, economists, and policy makers, an MRI costs $160 using the most advanced machines; $133 on machines with moderate resolution; $92 for the most primitive version. That allows for a decent profit on a technology that is over a quarter century old. It doesn't include the radiologists $45 fee. Prices will rise to whatever the market will bear. A resident at a medical center in New York noted when his hospital hired a new radiologist, the volume of a sophisticated and extremely profitable test called CT angiography suddenly quadrupled. Everyone noticed this shift and understood the motives... The employees were told to keep their mouths shut about the issue. "},{"title":"A Snip and $,1000: How Pathology Evolved into Big Business","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-snip-and-1000-how-pathology-evolved-into-big-business","content":"Patients typically have no choice over where their tissue fragments get sent and doctors frequently have no idea of the charges. "},{"title":"Taken for a Ride: Private Equity Ambulance Care","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#taken-for-a-ride-private-equity-ambulance-care","content":"Ambulance companies occupy an indispensible position in the hospital supply chain and one that proved easy to monetize: they bring in the paying customers. By 2011 ambulance transport was no longer primarily a charitable service but such a good business that America's two largest private sector providers, Rural/Metro and American Medical Response, were both bought by private equity firms that year. Buoyed by the potential for ambulance revenues, cities and counties also began billing insurance and charging more. Richard Dickinson recalls, \"The Fire Department's attitude, which the City Council bought, was they could keep raising the ambulance transportation rates because private insurers always paid. For individuals with no insurance, often the City Council would tell them to ignore the bills. After 3 or 4 bills were sent, LAFD would write off the account receivable.\" The Fire Department of the City of New York raised its base charges for a ride from $515 to $740, even though Medicare determined it was worth only about $243.57 Ambulance rides generate more than $200 million for the city each year. "},{"title":"No Pain, Big Gains: The Business of Physical Therapy","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#no-pain-big-gains-the-business-of-physical-therapy","content":"In 1979 Medicare limited the amount it would pay for outpatient physical therapy to only $100/year. That amount increased to $500 in 1982, $900 in 1994, and $1500 in 1999. "},{"title":"Preying on the Elderly for Profit","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#preying-on-the-elderly-for-profit","content":"Home evaluation companies like CenseoHealth are a new kind of investor-owned healthcare business that has thrived in the past five years. Well insured Americans suffer often from too much treatment - particularly as they age - with tests and services meted out not for health, but for money. "},{"title":"Chapter 7 - The Age of Contractors: Billing, Coding, Collections, and New Medical Businesses","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-7---the-age-of-contractors-billing-coding-collections-and-new-medical-businesses","content":"There are no standards for billing. There's money to be made in billing for anything and everything. There is no such thing as a fixed price for a procedure or test. The uninsured pay the highest price of all. Economies of scale don't translate to lower prices. With their marketing power, big providers can simply demand more. "},{"title":"A Brief History of Medical Coding","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-brief-history-of-medical-coding","content":"The historical purpose of coding was strictly epidemiologic - to classify and track causes of death and prevent the spread of infections among populations that spoke different languages. Medical coding systems originated at about the same time as the bubonic plague. Coders who work for hospitals strive to get money. The coders employed by insurers try to deny claims as overreaching. Coders who audit Medicare charts look for abuses that need to be punished. Suddenly, coding meant big bucks and a new industry thrived. For-profit colleges began offering medical coding degrees, and required internships soon followed. The latest iteration of international disease codes, ICD-10, was completed in 1992 and has been used by the rest of the world for decades. Why did it take the United States until 2015 to fully deploy it? The problem was the United States' medical billing system and how to game it was built around ICD-9. Changing to a new system that identified medical conditions more specifically using new numerical codes was good for international epidemiology - but terrible for the business of American medicine. "},{"title":"The Verifiers and Certifiers","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-verifiers-and-certifiers","content":"Wasteful bureaucratic processes have been a lucrative job creator for the business of healthcare. They also siphon hours away from what patients want from their doctor and what good doctors want to provide: human contact. "},{"title":"Chapter 8 - The Age of Research and Good Work for Profit: The Perversion of a Noble Enterprise","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-8---the-age-of-research-and-good-work-for-profit-the-perversion-of-a-noble-enterprise","content":"A lifetime of treatment is preferable to a cure. "},{"title":"A New Model: The Changing Vision of Medical Charity","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-new-model-the-changing-vision-of-medical-charity","content":"At a booth in the exhibition hall, kids earn beach balls or tattoos for answering a simple question on a white board: \"What is your wish for the future of diabetes?\" The answers are mostly the same: \"A cure.\" But most of the exhibitors and sponsors who help underwrite the conference are focused on something else: selling ever-costlier treatments and supplies. Between 2010 and 2015, the cost of insulin and other products used to manage diabetes skyrocketed, despite a few significant advances in treatment. The recommended wholesale price of different forms of insulin rose between 127 and 325 percent. As technologies age, prices can rise rather than fall. "},{"title":"The Invention of Venture Philanthropy","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-invention-of-venture-philanthropy","content":"Foundations had an enticing new business model: \"venture philanthropy\"; that is, investing money in drug, device, and biotech companies with the expectation of financial return. Venture philanthropy had successfully propelled the innovation, but there was mixed motivation to make sure it was affordable. Today, every piece of good work in medicine seems to need a promising business model. "},{"title":"A Bible with Ads","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-bible-with-ads","content":"Physician's Desk Reference (PDR). The PDR cataloged doses, side effects, half-lives, interactions, chemical structures, and results of clinical trials for all approved medicines. Some prescribing programs were touting price transparency, but the information was vague and not \"actionable,\" meaning not specific enough to allow a patient to make a different purchase. "},{"title":"Entering the Gilded Age","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#entering-the-gilded-age","content":"The AMA's (American Medical Association) primary focus in the past ten to fifteen years has been medical finance - filing lawsuits against insurers over how to calculate \"usual, customary, and reasonable\" and opposing Medicare's public release of payments to physicians, for example. "},{"title":"Turf Battles and Trade Wars: Medical Societies Form Super PACs","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#turf-battles-and-trade-wars-medical-societies-form-super-pacs","content":"The medical industry has become the country's biggest lobbying force, spending nearly half a billion dollars each year. In 2015 the oil and gas industry spent $130 million, securities and investment firms about $100 million, and the defense/aerospace industry a mere $75 million. The Illinois State Society successfully championed legislation allowing doctors to supervise up to five full-time physicians assistants and bill as if they delivered the care themselves. A doctor in Illinois can now be in six places at once. "},{"title":"Who Wrote the Guidelines?","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#who-wrote-the-guidelines","content":"More treatment is always better. Default to the most expensive option. Dr. Norton felt as if he had been used: \"They say, we fight to protect patients, but what they really mean is we fight to get exorbitant fees for unneeded procedures.\" Dr. Norton's experience highlights a flaw in how we pursue evidence-based medicine. The specialists who make money from procedures create the guidelines for when and how often they should be performed. "},{"title":"Chapter 9 - The Age of Conglomerates","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-9---the-age-of-conglomerates","content":"There are no standards for billing. There's money to be made in billing for anything and everything. Hospitals without competition could more easily get away with rendering only profitable services. "},{"title":"The Doctors' Faustian Bargain","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-doctors-faustian-bargain","content":"Dr. Alexander Lakowsky sold his practice to Sutter. He was forced to place the good of the conglomerate over the good of his patients. There was heavy pressure to refer his patients for testing and training within the Sutter system, which was overpriced. He wasn't able to advocate for his patients well. "},{"title":"The Electronic Medical Record as a Business Weapon","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#the-electronic-medical-record-as-a-business-weapon","content":"The first step under Obama's HITECH Act was to require doctors and hospitals to go electronic with their record keeping. The second step, required a few years later, was termed \"meaningful use\" - prodding doctors to and hospitals to deploy the new technology for the good of the patient: sending records to other hospitals or doctors with a click of the mouse or allowing patients to review lab results at home without needing to talk to a doctor. The government invested heavily in the EMR as a tool to enable good patient care, with the idea that they would allow the sharing of medical records between a sick person's physicians. But competing health systems have little financial incentive to do so. Instead, they have frequently become tools for conglomerates to protect market share or dominate their market. In the United States, EMRs have evolved to put business before patients. Disjointed and siloed, they have not delivered on their promise. "},{"title":"A Part-Time Emergency Room","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#a-part-time-emergency-room","content":"Most states that allow freestanding ERs at least insist they stay open 24/7 and take all emergencies regardless of the patient's ability to pay - though they cannot accept Medicare or Medicaid, since government insurers insist that to qualify as an ER and bill ER rates, an emergency room must be physically connected to a hospital. "},{"title":"Consolidation: Rebranding Doctors' Offices","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#consolidation-rebranding-doctors-offices","content":"Big hospitals hold enormous leverage in setting the terms of insurance contracts, because insurers need them in their networks. "},{"title":"Endgame","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#endgame","content":""},{"title":"Chapter 10 - The Age of Healthcare as Pure Business","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-10---the-age-of-healthcare-as-pure-business","content":"Our healthcare system today treats illness and wellness as just another object of commerce: Revenue generation. Supply chain optimization. Minimization of tax liability. Innovative business modeling. Things sold. Services rendered. Bills to be paid. Even with cheaper drugs being developed, they were still sold at the same price as to not look inferior to existing drugs. More competitors vying for business doesn't mean better prices; it can drive prices up, not down. As technologies age, prices can rise rather than fall. There were significant and much desired advances in the hemophilia industry - advances that could have and should have come before patent expiration induced companies to make the effort. "},{"title":"Chapter 11 - The Age of the Affordable Care Act (ACA)","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-11---the-age-of-the-affordable-care-act-aca","content":"Even the well-intentioned provisions that managed to survive the tortuous congressional negotiations over the ACA have been in practice diluted and perverted, as providers find ways to maximize revenue by gaming its rules. Obamacare, as it has come to be called, was supposed to build a first line of defense against the expenses of American medicine. It is, in some respects, revolutionary, a declaration that decent healthcare for every American is a guiding principle and government responsibility. It created some important incentives and rules to nudge the profit-oriented system toward better serving patients. In healthcare, entrepreneurship outsmarts regulation every time. "},{"title":"Costs Take a U-turn: Lesson of Medicare Drug Coverage","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#costs-take-a-u-turn-lesson-of-medicare-drug-coverage","content":"Medicare Part D is prescription drug coverage for Medicare beneficiaries enacted in 2006 under Republican President, George W Bush. The government hadn't previously offered assistance to patients 65 and older, but with the cost of medicine increases in the 1990s, reports of seniors not taking their medicine because it was too expensive were going viral. Part D had a doughnut hole... If a patient's cost exceeded $2960/year, the patient paid 100% of the cost until they hit the $4,700 mark for their medicines, at which point Medicare paid 95% of the costs after. This was to discourage patients taking more medicines or more expensive medicines than they may need. Once seniors were guaranteed drug coverages and were only paying co-payments, drug companies and insurers raised prices and co-pays to discourage using the service. "},{"title":"How Politicians Undermine the ACA","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#how-politicians-undermine-the-aca","content":"The public option refers to the notion that there would be on offer a national health insurance plan or that the government would allow anyone to choose Medicare as his insurer. That was jettisoned after the administration judged it would prevent the ACA's passage because of objections from the insurance industry. "},{"title":"How Insurers Undermine the ACA","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#how-insurers-undermine-the-aca","content":"Most of the plans offered through the ACA are so-called high-deductible plans, and the law has helped spread this type of coverage. A 2015 study found when patients were switched to a high-deductible health plan, they didn't become smarter, more cost-conscious shoppers for medical care. The exorbitant prices demanded by the U.S. healthcare system meant they mostly just avoided any interactions with medicine at all. "},{"title":"How Hospitals and Doctors Undermine the ACA","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#how-hospitals-and-doctors-undermine-the-aca","content":"The small financial incentives to encourage good behavior and coordinate medical care often paled compared to the profit that could be garnered by creative or aggressive billing that tested the boundaries of the law. Fifty percent of colonoscopies involve removing polyp, but if one was removed many doctors said it was no longer a free screening procedure. The Affordable Care Act didn't mean that tests were no longer free if something was found and removed for scrutiny. The U.S. healthcare system gradually evolved sector by sector, hospital by hospital, doctor by doctor. What the players are doing is, technically speaking, perfectly legal. Participants in the marketplace respond to the incentives and opportunities a market allows. That's what they're supposed to do. Each component of the system is genuinely convinced that it's not so bad, not responsible for our $3 trillion medical bill. Someone else is more to blame. Drug spending is only 10% of the national health budget. Nursing homes are only responsible for 5% of health costs. Payments to doctors only 20%! Dermatology accounts for only 4% of Medical expenditures. Each segment of our medical system is convinced that its charges are reasonable. But put all the little excess together and you get healthcare that is much worse and much costlier than the sum of its parts. We, the patients, are stuck in the middle, and it seems we've reached critical condition. "},{"title":"Chapter 12 - The High Price of Patient Complacency","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-12---the-high-price-of-patient-complacency","content":"We have votes, and we have a voice - as patients, we must dare to raise it. It is often only after Americans get sick in other countries they understand just how broken their own system is. The United States will find its own particular healthcare solution. A wide array of tools is available for reforming and reclaiming our healthcare system for patients, building on the foundation of the ACA or its replacement. "},{"title":"Chapter 13 - Doctors' Bills","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-13---doctors-bills","content":""},{"title":"Part 1. What You Can Start Doing Now - Doctor Bills","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-1-what-you-can-start-doing-now---doctor-bills","content":"We all need to be more \"difficult\" patients to share in decision-making and to expect more discussion - and that is absolutely true about costs. Every patient can help to ward off unwanted healthcare costs by asking questions about care and its price tag beforehand. Choosing Your Doctor# When selecting a primary care doctor, there are some basic questions you should ask about the business structure of the practice that will impact your bills. Is the practice owned by a hospital or licensed as a surgery center? Will you refer me only to other physicians in my insurance network, or explain why in advance if you can't?If I need blood work or radiology testing, can you send me to an in-network lab? Will there be charges for phone advice or filling out forms? Is there an annual practice fee? If I'm hospitalized, will you be seeing me in the hospital? What is your coverage on weekends?  In Your Doctor's Office# Here are some questions every doctor or healthcare provider should be able to answer for you at a doctor's appointment. How much will this test/surgery/exam cost? How will this test/surgery/exam change my treatment? Which blood test are you ordering? What X-Ray? Why? Are there cheaper alternatives that are equally good, or nearly so? Where will this test/surgery/exam be performed - at the hospital, surgery center, or in the office - and how does that place impact the price? Who else will be involved in my treatment? Will I be getting a separate bill from another provider? Can you recommend someone in my insurance network?  The Value of Waiting Before Seeking Treatment# The healthcare industry spends nearly $15 billion on advertising annually to encourage worry. That's good business, but not smart medicine. The value of so-called watchful waiting is taught in medical school, but is terrifically underused in American medicine because it isn't at all profitable. The \"do something!\" strategy prevails. Don't rush to the doctor. Give your body a chance to heal. Start with some advice from a pharmacist (their knowledge is terribly underutilized in our current system). "},{"title":"Part 2. System Change: What We Want from Doctors","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-2-system-change-what-we-want-from-doctors","content":"In training, doctors are generally taught little to nothing about the cost of healthcare. This ignorance has profound implications both for their future practices and for us. Prices should be included on order sheets or, at the very least, hospitals should provide medical staff with their master list - the chargemaster - so they can educate themselves about what services the hospital charges and for how much. The chain reaction to price transparency begins with patients posing the same simple question: \"How much does that cost?\" Malpractice and Medical School Reform# About three quarters of doctors say that worries about malpractice suits at least occasionally influence their practice and encourage excessive test ordering. Reforming the malpractice system will also be good for patients. The most frequent beneficiaries of our current system are lawyers. Malpractice reform is atop the wish list of every physician. Here are a few suggestions: Place limits on noneconomic damagesEncourage arbitrationOffer warranties and guarantees If we want the best, most caring doctors to join the battle against cost, we need a fairer way to finance medical education. The huge expense of medical school may encourage some students to choose profitable specialties in order to easily pay back loans and deter others from entering fields where pay is relatively low, such as neurology, endocrinology, and family practice. Price transparency should be mandated Not-for-profit hospitals should be not for profit. Period. These organizations currently hide large profits in excessive executive compensation, excessive lavish building projects, bloated foundations, and other 'shell games.' Physicians should be prohibited from owning medical facilities of any nature that they could potentially refer patients to. Examples include surgicenters, laboratories, imaging equipment, etc. This is clearly a conflict of interest and should be illegal. The best doctors will not be threatened by questioning, skeptical patients. They are too discomfited with the way we deliver medicine and the prices we pay. Their goals are the same as yours. "},{"title":"Chapter 14 - Hospital Bills","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-14---hospital-bills","content":"Note: "},{"title":"Part 1. What You Can Start Doing Now - Hospital Bills","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-1-what-you-can-start-doing-now---hospital-bills","content":"Vet Your Hospital# Scientific studies do not demonstrate a correlation between price and quality of care. In the Hospital# You can protect your financial health while in the hospital by asking the right questions. Hospitals have built huge oversupply of private rooms, though insurers frequently won't cover their cost. In the pages of admitting documents you'll have to sign, there is inevitably one concerning your willingness to accept financial responsibility for charges not covered by your insurer. Be clear on the terms of your stay in the hospital: Are you being admitted or held under \"observation status\"? Ask point-blank. If you're feeling well enough, ask to know the identity of every unfamiliar person who appears at your bedside, what he or she is doing, and who sent him or her. If the hospital tries to send you home with equipment you don't need, refuse it, even if it's \"covered by your insurance.\"  Dealing with Bills# If you receive an outrageous bill from a hospital, a testing center, or a medical office, don't wait - negotiate! When a hospital bill arrives in the mail, request complete itemization. Check the bill against the notes you made while you were in the hospital. Protest bills in writing to create a record. Argue against surprise out-of-network bills.  "},{"title":"Part 2. System Change: What We Want from Hospitals and Hospital Regulators","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-2-system-change-what-we-want-from-hospitals-and-hospital-regulators","content":"Patient-Friendly Price Disclosure# Every hospital has a master price list - the infamous and generally hidden chargemaster - often developed with input from business consultants. Chargemasters do not have to be presented in any standard format, making prices nearly impossible to compare from one hospital to the next. The California chargemasters provide so much disorganized information that essentially none of it is useful for the consumer-patient. The other forty-nine states do not require hospitals to disclose their chargemasters at all. In fact, hospitals fiercely guard these price lists. California laws may have planted a seed for change. Hospitals should publish their chargemasters in a standard, easy-to-understand format. Use Antitrust Law to Break Up Oversize Hospital Conglomerates# Healthcare has now reached the point where consolidation is leading to price inflation, with little benefit to patients. In this country, it's not illegal to have a monopoly and to use it to raise your price. Standards for Hospital Billing and Collection# There is no industry wide or legal standard about when medical accounts should be considered seriously delinquent. One hospital might send a bill to collections after 30 days, another after 120, and another not at all. The U.S. Consumer Financial Protection Bureau has recommended reforming medical billing by including more fair and predictable timetables. "},{"title":"Chapter 15 - Insurance Costs","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-15---insurance-costs","content":""},{"title":"Part 1. What You Can Start Doing Now","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-1-what-you-can-start-doing-now","content":"Selecting a Plan# Shopping for health insurance is perhaps the hardest task facing any consumer. Today you must consider the entire dizzying package. The premium: the amount you pay each month to be insured. The deductibles: the amount you pay each month to be insured. The co-payments and coinsurance: the patient's contribution for each medical purchase or encounter. The out-of-pocket maximum: the limit on how much you will be expected to pay in a given year, if you've played by the plan's rules. The network: the collection of contracted doctors and hospitals you must use in order to have the plan cover your care or, at least, to reap the plan's full financial benefits and promises. PremiumsDeductiblesCo-payments and coinsuranceAnnual out-of-pocket maximumThe networkHMO optionsIs a nonprofit right for you? "},{"title":"Part 2. System Change: What We Want from Insurance","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-2-system-change-what-we-want-from-insurance","content":"State Insurance Regulators# Consumers should be able to expect some fair standards from their health insurance policies: The network provider contract must be in force for the same period of time as the health insurance policy. If a procedure is listed as covered under a plan, then all tests and ancillary services normally associated with the procedure should also be covered. Provider directories must be kept up to date. A patient should pay only in-network fees if there is no other choice but to use a facility or a provider outside the network of treatment. Directories should not be able to list meaningless categories to describe doctors and hospitals. Streamline statements that define annual \"out-of-pocket\" maximum payments.  Hire consumer advocates, not industry gadflies, as state insurance overseers. More Creative Plan Designs# Reference PricingBundling Bundled payments give the medical world new incentives to find out what treatments actually work best. Insurance policies designed to meet patient needs "},{"title":"Chapter 16 - Drug and Medical Device Costs","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-16---drug-and-medical-device-costs","content":"Americans comparison shop for cars, breads, electronics, and just about everything else except for prescription drugs. "},{"title":"Part 1. What You Can Start Doing Right Now","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-1-what-you-can-start-doing-right-now","content":"Many expensive prescription medicines are just reformulations of older versions of the same drugs creatively repatented, not improved. Shop Around# The pharmacist may tell you if you have insurance you \"have to use it.\" That is not the case: no law, no regulation, no waiver you've signed stipulates that you cannot pay cash for a prescription. What the pharmacy wants is the higher price and profit margin it obtains by forcing you to buy medicine through your insurance policy. Exercise your power as a consumer and either complain or have your prescription transferred to another pharmacy. Consider Imports# If you need to take a drug but really can't afford it, you may want to consider buying it from a source outside the United States. The cost is likely to be one-third to one-half of the U.S. price. If you're traveling to a country whose health system you trust, buy refills of medicines there. Rely on overseas mail-order pharmacies to order long-term medicines whose efficacy can be clearly measured.  Be Skeptical of Advertising and Marketing# "},{"title":"Part 2. System Change: What We Want from Manufacturers and Regulators","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-2-system-change-what-we-want-from-manufacturers-and-regulators","content":"Doctors abroad are dumbstruck when they hear the prices we pay for pharmaceuticals. Import DrugsGive pharmacists more prescribing powerThe FDA should reform the patent process and revamp drug and device approvalNegotiate national pricesPromote pharmaceutical transparency, cost-effectiveness at all stages of the approval process Doctors and Professional Organizations: Ally with Patients, not Pharma# When choosing treatments for a patient, we have to consider the financial strains they may cause alongside the benefits they might deliver. Use Charity Navigator to check how a charity spends its donations. "},{"title":"Chapter 17 - Bills for Tests and Ancillary Services","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-17---bills-for-tests-and-ancillary-services","content":"In the past, the costs of tests and ancillary services used to be covered by insurance with little or no patient contribution; today patients are often expected to foot at least part of the bill. Protect your wallet: do not have any test or service performed by a provider outside your network. As a general rule, avoid having your ordinary blood and fluid specimens sent to a hospital lab. Ask the commercial lab for a printout of your results. Some situations demand top-dollar testing.  "},{"title":"Chapter 18 - Better Healthcare in a Digital Age","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#chapter-18---better-healthcare-in-a-digital-age","content":"In the last twenty-five years, nearly every aspect of our day-to-day lives has been made easier by digital technology: banking, watching films, traveling, communicating with loved ones near and far away, purchasing a new home. But healthcare is the exception to the rule. That is not because of a lack of investment. Silicon Valley is hot on healthcare. Health technology can be deployed for enormous patient good, but often all it offers up are useless, but profitable, services. If a company exists to untangle or parse the data in our convoluted system, the real answer is not to add another layer designed by entrepreneurs looking for profit, but to make the system simpler. "},{"title":"Part 1. How You Can (and Can't) Benefit from New Technologies Now","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-1-how-you-can-and-cant-benefit-from-new-technologies-now","content":"WearablesTechnology-enhanced screeningTelehealth "},{"title":"Part 2. System Change: What We Want from Technology","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#part-2-system-change-what-we-want-from-technology","content":"We could be deriving enormous benefits from new technology, if only it was deployed with more forethought about how it might contribute to high-quality affordable healthcare rather than just bureaucratic record keeping and industry profit. The Means to Possess Your Medical Data# Right now our medical information is held captive in disparate doctors' offices and hospitals. That isolation translates into pointless expenses, because patients frequently have tests repeated. It means worse care, because doctors don't have previous test results for comparison. Full utility of stored medical information online (through start up companies like PicnicHealth) is limited at this time because many hospitals and doctors are not capable of sending or receiving your information in a standardized format. All that information could be placed on a chip card (or a smart device!) to carry in a wallet, which could be scanned by each new provider. It could be stored in a national data collection system akin to a credit agency like Equifax. If a public agency kept all records of all your medical encounters in a secure and searchable form, they could be made available to providers of your choosing. Connected Programs for Pricing and Scheduling# Thanks to digital technology you can price, book, and pay for a hotel in Tbilisi, Georgia, on your computer. Why can't you do the same for an X-ray down the road or a doctor's appointment? "},{"title":"Epilogue","type":1,"pageTitle":"An American Sickness","url":"docs/books/healthcare-books/an-american-sickness#epilogue","content":"The mission of this book is to advocate for a return to a system of affordable, evidence-based, patient-centered care. No one player created the mess that is the $3 trillion American medical system. People in every sector of medicine are feeding at the trough: insurers, hospitals, doctors, manufacturers, politicians, regulators, charities, and more. People in sectors that have nothing to do with health - banking, real estate, and tech have also somehow found a way to extort cash from patients. They all need to change their money-chasing ways. To make that happen, we patients will need to change our ways too. We must become bolder, more active, and more thoughtful about what we demand of our healthcare and the people who deliver it. We must be more engaged in finding and pressing the political levers to promote the evolution of the medical care we deserve. It's our health, the future of our children, and our nation. High-priced healthcare is America's sickness and we are all paying, being robbed. When the medical industry presents us with the false choice of your money or your life, it's time for us all to take a stand for the latter. "},{"title":"The Checklist Manifesto","type":0,"sectionRef":"#","url":"docs/books/business-books/the-checklist-manifesto","content":"","keywords":""},{"title":"Introduction","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#introduction","content":"The reason avoidable failures are common and persistent is because the volume and complexity of what we know has exceeded our individual ability to deliver its benefits correctly, safely, or reliably. Knowledge has both saved us and burdened us. "},{"title":"Chapter 1: The Problem of Extreme Complexity","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-1-the-problem-of-extreme-complexity","content":""},{"title":"Chapter 2: The Checklist","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-2-the-checklist","content":""},{"title":"Chapter 3: The End of the Master Builder","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-3-the-end-of-the-master-builder","content":"Checklists seem able to defend anyone, even the experienced, against failure in many more tasks than we realize. How can we be sure we have the right information at hand to complete a task and how can we be sure we are applying that knowledge correctly? A building is like a body. It has a skin, a skeletal system (frame), a vascular system (plumbing), a breathing system (ventalation), and a nervous system (wiring). Construction projects involve many different trades and each trade contributes separately, yet must fit together and make sense as a whole. We see similar situations in IT. How do we understand the entirety of an application to seamlessly create building and maintenance checklists for all the different contributors? It is wise not to soley depend on the wisdom of the single individual, even an experienced engineer. Depending on the wisdom of the group and putting more sets of eyes on problems. Trust the power of communication when unexpected issues arise. "},{"title":"Chapter 4: The Idea","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-4-the-idea","content":"Give people the room to adapt, based on their experience and expertise. All that should be expected is that people talk to each other and take responsibility. That is what works. Under conditions of true complexity - where the knowledge required exceeds that of any individual and unpredictability reigns - efforts to dictate every step from the center will fail. People need room to act and adapt. "},{"title":"Chapter 5: The First Try","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-5-the-first-try","content":"Investigators at John Hopkins and elsewhere had observed when nurses were given a chance to say their names and mention concerns at the beginning of a case, they were more likely to note problems and offer solutions. "},{"title":"Chapter 6: The Checklist Factory","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-6-the-checklist-factory","content":"Bad checklists are vague and imprecise. They are too long; they are hard to use; they are impractical. They are made by desk jockeys with no awareness of the situations in which they are to be deployed. They treat the people using the tool as dumb and try to spell out every single step. They turn people's brains off rather than turn them on. Good checklists are precise. They are efficient, to the point, and easy to use even in difficult situations. They do not spell everything out. A checklist cannot fly an airplane, perform heart surgery, or make production changes to an application. Instead, they simply provide reminders of only the most critical and important steps - steps that even highly trained and skilled professionals could miss. Good checklists are above all, practical. The power of checklists is limited. They can help experts remember how to manage a complex process or configure a complex machine. They can make priorities clearer and prompt people to function better as a team. Checklists cannot make anyone follow them. Pilots turn to their checklists for two reasons. First, they are trained to do so. They learn from the beginning of flight school that their memory and judgement are unreliable and that lives depend on their recognizing that fact. Second, the checklists have proved their worth - they work. However much pilots are taught to trust their procedures more than their instincts, that doesn't mean they do so blindly. When you are making a checklist, there are a number of key decisions. You must define a clear pause point at which the checklist is supposed to be used (unless the moment is obvious, like a warning light or alert goes off). You must decide if you want to create a DO-CONFIRM checklist or a READ-DO checklist. DO-CONFIRM checklist - a list that is checked after tasks or jobs are performed from memory and experience to ensure that everything was done that was supposed to be. READ-DO checklist - a list that is carried out step by step and tasks or jobs are checked off as they are performed. This type pf checklist is more like a recipe. Checklists cannot be lengthy. A general rule of thumb is to keep checklists between five and nine items, which is the limit of working memory. After about sixty to ninety seconds at a given pause point, a checklist often becomes a distraction from other things. People start \"shortcutting.\" Steps get missed. Keep lists short by focusing on \"the killer items.\" No matter how careful we are or how much thought we put in to a checklist, they have to be tested in the real world. This can be more complicated than expected, but it is necessary. Certain critical or important steps may be intentionally omitted if it is proven professionals rarely fail to perform the step. An example of this may be a professional pilot almost never fails to notify radio control of landing in emergencies. If this step is almost never missed, it should NOT be included in the checklist. It is common to misconceive how checklists functionin complex lines of work. They are not comprehensive how-to guides. They are quick and simple tools aimed to buttress the skills of expert professionals. A reason for delay in adopting change is not usually unwillingness or laziness. It is more often the necessary knowledge has not been translated into a simple, usable, and systematic form. "},{"title":"Chapter 7: The Test","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-7-the-test","content":"Using a checklist involved a major cultural change - a shift in authority, responsibility, and expectations. However straightforward a checklist might appear, if you are used to getting along without one, incorporating one into the routine is not always a smooth process. "},{"title":"Chapter 8: The Hero in the Age of Checklists","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-8-the-hero-in-the-age-of-checklists","content":"Even the most expert among us can gain from searching out the patterns of mistakes and failures and putting a few checks in place. Just ticking boxes is not the ultimate goal here. Embracing a culture of teamwork and discipline is. We can devise emergency checklists, like aviation has, for nonroutine situations. The fear people have about the idea of adherence to protocol is rigidity. They imagine mindless automatons, heads down in a checklist, incapable of looking out their windshield and coping with the real world in front of them. What you find however, is when a checklist is well made, exactly the opposite proves true. The checklist gets the dumb stuff out of the way, the routines your brain shouldn't have to occupy itself with and lets it rise above to focus on the hard stuff. All learned occupations have a definition of professionalism, a code of conduct. It is where they spell out their ideals and duties. The codes are sometimes stated, sometimes just understood. But they all have at least three common elements. An expectation of selflessness. We who accept responsibility for others - whether we are doctors, lawyers, teachers, public authorities, soldiers, or pilots - will place the needs and concerns of those who depend on us above our own.An expectation of skill. We will aim for excellence in our knowledge and expertise.An expectation of trustworthiness. We will be responsible in our personal behavior toward our changes.Aviators add a fourth expectation. Discipline. Discipline in following prudent procedure and in functioning with others. Discipline is hard - harder tan trustworthiness, skill, and perhaps even selflessness. We are by nature flawed and inconsistent creatures. We can't even keep from snacking between meals. We are not built for discipline. We are built for novelty and excitement, not for careful attention to detail. Discipline is something we have to work at. Airline manufacturers put a publication date on all their checklists, and there is a reason why. They are expected to change with time. In the end, a checklist is only an aid. If it doesn't aid, it's not right. We have recently turned to the computer as our aid. Computers hold out the prospect of automation as our bulwark against failure. Indeed, they can take huge numbers of tasks off our hands. Without question, technology can increase our capabilities, but there is a lot technology can't do. Technology does not deal well with the unpredictable, manage uncertainty, construct entire buildings, or perform a lifesaving operation. In some ways, technology has complicated these matters. It has added another layer of complexity to the systems we depend on and have given us entirely new kinds of failures to contend with. Anyone who understands systems will know immediately that optimizing parts is not a good route to system excellence. An example of this is the famous thought experiment of trying to build the world's greatest car by assembling the world's greatest car parts. Parts connected were the engine of a Ferrari, the brakes of a Porsche, the suspension of a BMW, and the body of a Volvo. Of course, the result was nothing close to a great car. The result was a very expensive pile of junk. We don't study routine failures in teaching, in law, in government programs, in the finance industry, or elsewhere. We don't look for patterns of our recurrent mistakes or devise and refine potential solutions for them. When we look closely, we recognize the same balls being dropped over and over, even by those of great ability and determination. We know the patterns. We see the costs. It's time to try something else. Try a checklist. "},{"title":"Chapter 9: The Save","type":1,"pageTitle":"The Checklist Manifesto","url":"docs/books/business-books/the-checklist-manifesto#chapter-9-the-save","content":""},{"title":"AngularJS","type":0,"sectionRef":"#","url":"docs/dev/angular/angularjs","content":"","keywords":""},{"title":"High Level","type":1,"pageTitle":"AngularJS","url":"docs/dev/angular/angularjs#high-level","content":"A module contains the different components of an AngularJS app A controller manages the app’s data An expression displays values on the page A filter formats the value of an expression module (defines application) -> directive (extend HTML) -> model -> AngularJS is a JavaScript framework. It can be added to an HTML page with a <script> tag. AngularJS extends HTML attributes with Directives, and binds data to HTML with Expressions. AngularJS is a JavaScript framework written in JavaScript. AngularJS is distributed as a JavaScript file, and can be added to a web page with a script tag. "},{"title":"NG Directives","type":1,"pageTitle":"AngularJS","url":"docs/dev/angular/angularjs#ng-directives","content":"AngularJS extends HTML with ng-directives. The ng-app directive defines an AngularJS application. The ng-model directive binds the value of HTML controls (input, select, textarea) to application data. The ng-bind directive binds application data to the HTML view. AngularJS starts automatically when the web page has loaded. The ng-app directive tells AngularJS that the <div> element is the \"owner\" of an AngularJS application. The ng-model directive binds the value of the input field to the application variable name. The ng-bind directive binds the content of the <p> element to the application variable name. "},{"title":"AngularJS Directives","type":1,"pageTitle":"AngularJS","url":"docs/dev/angular/angularjs#angularjs-directives","content":"AngularJS directives are HTML attributes with a prefix of ng. The ng-init directive initializes AngularJS application variables. "},{"title":"AngularJS Expressions","type":1,"pageTitle":"AngularJS","url":"docs/dev/angular/angularjs#angularjs-expressions","content":"AngularJS expressions are written inside double braces: {{ expression }}. AngularJS will \"output\" data exactly where the expression is written: AngularJS expressions bind AngularJS data to HTML the same way as the ng-bind directive. AngularJS expressions can also be written inside a directive: ng-bind=\"expression\". AngularJS will resolve the expression, and return the result exactly where the expression is written. AngularJS expressions are much like JavaScript expressions: They can contain literals, operators, and variables. Example {{ 5 + 5 }} or {{ firstName + \" \" + lastName }} "},{"title":"AngularJS Applications","type":1,"pageTitle":"AngularJS","url":"docs/dev/angular/angularjs#angularjs-applications","content":"AngularJS modules define AngularJS applications. AngularJS controllers control AngularJS applications. The ng-app directive defines the application, the ng-controller directive defines the controller. "},{"title":"Scope","type":1,"pageTitle":"AngularJS","url":"docs/dev/angular/angularjs#scope","content":"If we consider an AngularJS application to consist of: View, which is the HTML. Model, which is the data available for the current view. Controller, which is the JavaScript function that makes/changes/removes/controls the data. Then the scope is the Model. The scope is a JavaScript object with properties and methods, which are available for both the view and the controller. "},{"title":"Development","type":0,"sectionRef":"#","url":"docs/dev/development","content":"Placeholder for new Development documentation.","keywords":""},{"title":"The Phoenix Project","type":0,"sectionRef":"#","url":"docs/books/tech-books/phoenix-project","content":"","keywords":""},{"title":"Chapter 1: Tuesday, September 2","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-1-tuesday-september-2","content":"Bill Palmer, Director of Midrange Technology Operationsresponsible for availability of his IT groupDo more with less, maintain competitiveness, and reduce costsIT systems should be reliable and available, and for the business to be able to depend on them "},{"title":"Chapter 2: Tuesday, September 2","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-2-tuesday-september-2","content":"Payroll issue is a giant dumpsterfireSAN issue should have affected more than it did.ask better questions for better resultsprioritization comes in to play here with which issues are to be tackled first "},{"title":"Chapter 3: Tuesday, September 2","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-3-tuesday-september-2","content":"Change to Social Security numbers caused issue in Payroll outage?Change management is IMPORTANTDidn't get Timekeeping app or SAN online on time. =( "},{"title":"Chapter 4: Wednesday, September 3","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-4-wednesday-september-3","content":"Managers arguing about who's fault the problem is and importance of the projectGiven 9 days to release project phoenixIssues caused by other issuesThey need more than one Brent (he is a bottleneck in the company's success)Bill's laptop upgrade fails, along with many others in the companyTOO MUCH PAPERWORK FOR CHANGE CONTROL "},{"title":"Chapter 5: Thursday, September 4","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-5-thursday-september-4","content":"Can't make new commitments to people without understanding what current commitments areWhen prioritizing, make sure you are aware of what is getting deprioritized "},{"title":"Chapter 6: Friday, September 5","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-6-friday-september-5","content":"What defines a change?Silly to dwell on sunk costs. Whether it be in time or money. Keep RowingEmployees get excited when processes are simple and they are empowered to do work.The amount of changes that flowed in due to the simplicity of the new note card change process is an example of this "},{"title":"Chapter 7: Friday, September 5","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-7-friday-september-5","content":"What is \"work\"WIP or Work In Progress (Inventory) - Silent KillerIT Operations is responsible for 4 types of work Business Project WorkInternal IT Project WorkChange WorkUnplanned Work (Anti-Work) Theory of ConstraintsLean Production or theToyota Production SystemTotal Quality ManagementBottlenecks must be accounted for FIRST. All other improvements are an illusion as they will always still be waiting on the bottleneck (and potentially making the bottleneck worse)The job of IT Operations is to ensure the fast, predictable, and uninterrupted flow of planned work that delivers value to the business while minimizing the impact and disruption of unplanned work, so that we may provide stable, predictable, and secure IT service.The Three Ways The First Way helps us understand how to create fast flow of work as it moves from Development into IT Operations, because that is what is between the business and the customerThe Second Way shows us how to shorten and amplify feedback loops, so we can fix quaility at the source and avoid reworkThe Third Way shows us how to create a culture that simultaneously fosters experimentation, learning from failure, and understanding that repetition and practice are the prerequisites to mastery "},{"title":"Chapter 8: Monday, September 8","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-8-monday-september-8","content":"Change Management Delegation "},{"title":"Chapter 9: Tuesday, September 9","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-9-tuesday-september-9","content":"Everything is documented in change management to ensure we know what is affected by our changes. "},{"title":"Chapter 10: Thursday, September 11","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-10-thursday-september-11","content":"Brent is the only one who knows where to look to fix thingsNo calls or tickets are logged because the paperwork takes longer than fixing the actual issue (this should be irrelevant... Ticket always)Document all fixes and add to knowledge base. Create a culture that embraces this. "},{"title":"Chapter 11: Thursday, September 11","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-11-thursday-september-11","content":"Figuring our how to remove Brent as a bottleneck and challenges associated with it "},{"title":"Chapter 12: Friday, September 12","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-12-friday-september-12","content":"Terrible deployment of Phoenix is an understatement "},{"title":"Chapter 13: Monday, September 15","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-13-monday-september-15","content":"Credit card security codes are revealedAudits were not taken care of "},{"title":"Chapter 14: Tuesday, September 16","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-14-tuesday-september-16","content":"Threat to outsource ITChris and Bill start to talk to one another to form a partnership between Dev and Operations "},{"title":"Chapter 15: Wednesday, September 17","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-15-wednesday-september-17","content":"First change management successDevOps is about minimizing unplanned workUnplanned work kills the ability to do planned work -Murphy's Law :)There will always be unplanned work. Ability to handle unplanned work efficiently is extremely important.The Goal by Dr. Eli Goldratt5 Focusing Steps Identify Contraint(Identified Brent as Bottleneck early in the book)Exploit the Constraint - make sure the constraint is not allowed to waste any time. Ever. Should never wait on any resource and should always be working on highest priority commitmentSubordinate (Adjust to) the Constraint  no sense in having things pile up in front of the bottleneck. Match the pace of what the bottleneck can handle until bottleneck is improved Elevate the ConstraintRepeat the Process Optimizing until the current constraint is gone and you look for the next constraint in the systemIncorporate Nonfunctional Requirements into designBrent knows the most about where technical debt exists and is needed in architecture and code built/designed for Operations meetings Know what work matters to achieve business objectives "},{"title":"Chapter 16: Thursday, September 18","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-16-thursday-september-18","content":"CEO Steve is threatening to make the situation worse againBill quits his job? Part 2 "},{"title":"Chapter 17: Monday, September 22","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-17-monday-september-22","content":"Steve has a realization that he was a jerk and offers Bill his job back "},{"title":"Chapter18: Tuesday, September 23","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter18-tuesday-september-23","content":"Five Dysfunctions of a Team by Patrick Lencioni "},{"title":"Chapter 19: Tuesday, September 23","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-19-tuesday-september-23","content":"analysis of demand capacity should be performed before accepting new worktechnical debt comes from taking short cutswhen all you do is react, that leaves little time to do the mental work of planning "},{"title":"Chapter 20: Friday, September 26","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-20-friday-september-26","content":"Priority 1 is whoever is yelling the loudest. This is not how it should be.Workers prioritizing work based on personal incentives (taken out to lunch)Understanding the flow of work is the key to achieving The First Way (how to create a fast flow of work)Every work center is made up of 4 things (the 4 Ms): The MachineThe ManThe MethodThe Measures Properly elevating preventative work is at the heart of programs like Total Productive Maintenance (been embraced by the LEAN community)The Third Way is all about ensuring that we're continually putting tension on the system, continually reinforcing habits, and improving somethingResilience Engineering tells us we should routinely inject faults into the system, doing them frequently, to make them less painful (Improvement Kata)^^^ Ask Patrick and Dave about this point.... Not really clear what that means for us from a monitoring perspectiveCritical part of The Second Way is making wait times visible. Knowing when your work is spending days (months in Optum's case) in someone else's queue because there are not enough resources or because some tasks take too longDoes a project help with scalability, availability, sustainability, security, supportability, or the defensibility of the organization? "},{"title":"Chapter 21: Friday, September 26","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-21-friday-september-26","content":"You win when you protect the organization without putting meaningless work into the IT system. You win even more when you can take meaningless work out of the IT system. "},{"title":"Chapter 22: Monday, September 29","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-22-monday-september-29","content":"Most important projects increases capacity at constraints "},{"title":"Chapter 23: Tuesday, October 7","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-23-tuesday-october-7","content":"Wait time - percentage of time busy / percentage of time idle "},{"title":"Chapter 24: Saturday, October 11","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-24-saturday-october-11","content":"John wants to set up a meeting with Dick "},{"title":"Chapter 25: Tuesday, October 14","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-25-tuesday-october-14","content":"The First Way: Systems thinking, always confirming that the entire organization achieves its goal, not just one part of it.Organizational key Performance Indication (KPI)IT needs preventative maintenance "},{"title":"Chapter 26: Friday, October 17","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-26-friday-october-17","content":"Phoenix should not have been approved "},{"title":"Chapter 27: Tuesday, October 21","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-27-tuesday-october-21","content":"IT was out of scope for the audits. New security processes to be put in place. "},{"title":"Chapter 28: Monday, October 27","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-28-monday-october-27","content":"Phoenix Deployment 2 goes much smoother, but there are still a lot of bumps. Biggest was caused by a change Brent made by an out of scope Sarah project. "},{"title":"Chapter 29: Monday, November 3rd","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-29-monday-november-3rd","content":"Flow of work should ideally go in one direction only. Forward.First discussions of an agile release cycle (done with SWAT team) Part 3 "},{"title":"Chapter 30: Monday, November 3rd","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-30-monday-november-3rd","content":"take time - the cycle time needed in order to keep up with customer demandsingle-minute exchange of die - Toyota solved issue of not keeping up with customer demand by reducing die changeover time from days to minutes.Until something is in production, it adds no value... It is just WIP stuck in the systemtreat infrastructure as code (What does that mean, exactly? How do we treat code at Optum?)deployment pipelineversion controlautomate environment creation processCONTINUAL EXPERIMENTATION "},{"title":"Chapter 31: Monday, November 3","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-31-monday-november-3","content":"10 Deploys a day does not seem so far off. :) "},{"title":"Chapter 32: Monday, November 10","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-32-monday-november-10","content":"Project UnicornAutomated Infrastructure Environment creation for identical Dev, QA, and ProdBrent gets pulled away from project Unicorn "},{"title":"Chapter 33: Tuesday, November 11","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-33-tuesday-november-11","content":"New DevOps procedures lead to multiple, massive successes "},{"title":"Chapter 34: Friday, November 28","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-34-friday-november-28","content":"Mainframe outsourcing is to be brought back in house "},{"title":"Chapter 35: Friday, January 9","type":1,"pageTitle":"The Phoenix Project","url":"docs/books/tech-books/phoenix-project#chapter-35-friday-january-9","content":"The Third Way - Create a culture that reinforces the value of taking risks and learning from failure. There is a need for repetition and practice to create mastery.Improvements of daily work should show where it needs to be: in daily workUnderstanding what technology can and can't do has become a core competency that every part of business must have.Bill to be the next COO of Parts UnlimitedWhen IT fails, the business fails. If IT is organized so that it can win, the business wins.Erik asks Bill to write, The DevOps Cookbook 4 Major Points 4 Different types of workWork In Process (WIP) - the silent killerThe Three WaysThe Theory of Constraints DevOps is a combination of: Lean ManufacturingSafety Culture "},{"title":"Ansible Overview","type":0,"sectionRef":"#","url":"docs/dev/ansible/ansible-overview","content":"","keywords":""},{"title":"Playbooks","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#playbooks","content":"Plain text YAML files that describe the desired state of something. Human and machine readable Can be used to build entire application environments "},{"title":"Variables","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#variables","content":"Many different ways to source variables Playbooks Files Inventories (group vars and host vars) Command Line Discovered Variables (facts) Ansible Tower  "},{"title":"Inventories","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#inventories","content":"Static lines of servers Ranges Other custom things Dynamic lists of servers: AWS, Azure, Google Cloud Platform, etc. Ultimately, lists of things you want to automate across. Playbooks contain playsplays contain taskstasks call modules Tasks run sequentially Handlers are triggered by tasks, and are run once, at the end of plays. "},{"title":"Modules","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#modules","content":"There are over 450 Ansible provided modules that automate nearly every part of your environment. Standard Structure: module: directive1=value directive2=value  "},{"title":"Using Ansible","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#using-ansible","content":"How to run: Ad-hoc - Call directly from the command line. Ansible 'inventory' -mPlaybook - ansible-playbookAutomation Framework - ansible tower "},{"title":"Check Mode","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#check-mode","content":"Dry run for ad-hoc commands and Playbooks Validate playbook runs before making state changes on target systems "},{"title":"Inventory","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#inventory","content":"Defaults to being saved in /etc/ansible/hosts Specify different inventory using <path option on the command line The preferred practice in Ansible is to not store variables in the main inventory file. "},{"title":"YAML","type":1,"pageTitle":"Ansible Overview","url":"docs/dev/ansible/ansible-overview#yaml","content":"YAML Ain't Markup Language YAML is a human friendly data serialization standard for all programming languages. Ansible uses YAML because it is easier for humans to read and write than other common data formats like XML or JSON. Further, there are libraries available in most programming languages for working with YAML.YAML files begin with --- and end with ...All members of a list are lines beginning at the same indentation level starting with a \"- \" (a dash and a space)A dictionary is represented in a simple key: value form (there must be a space after the colon)More complex data structures are possible, such as lists of dictionaries, dictionaries whose values are lists, or a mix of both.Dictionaries and lists can also be represented in abbreviated form. Example:  Copy --- martin: {name: Martin Dev, job: developer, skills: Elite} fruits: ['Apple','Orange','Strawberry','Mango'] Ansible does not use boolean values often, but they exist. Yes, no, true, or false may be used. Inventories Items to automate https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html YAML Syntax This is important as YAML is used to write Ansible Playbooks https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html#yaml-syntax Ansible Facts Setup Task - This task is run whenever a playbook is run and it gathers a whole host of facts about the remote machine(s) — IP addresses, disks, OS version… etc. Ansible refers to these as “facts”. Ansible Custom Facts https://medium.com/@jezhalford/ansible-custom-facts-1e1d1bf65db8 Custom configuration of ansible facts. On any Ansible controlled host — that is, the remote machine that is being controlled and not the machine on which the playbook is run — you just need to create a directory at /etc/ansible/facts.d. Inside this directory, you can place one or more *.fact files. These are files that return JSON data, which will then be included in the raft of facts that Ansible gathers at the start of each playbook run. You can name them anything you like, as long as they use the *.fact extension Gather Facts Facts are described in the playbooks section and represent discovered variables about a system. These can be used to implement conditional execution of tasks but also just to get ad-hoc information about your system. You can see all facts via: $ ansible all -m setup It’s also possible to filter this output to just export certain facts Facts are information derived from speaking with your remote systems. An example of this might be the IP address of the remote host, or what the operating system is. To see what information is available, try the following: ansible hostname -m setup This will return a large amount of variable data As discussed in the playbooks chapter, Ansible facts are a way of getting data about remote systems for use in playbook variables. Usually these are discovered automatically by the setup module in Ansible. Users can also write custom facts modules, as described in the API guide. However, what if you want to have a simple way to provide system or user provided data for use in Ansible variables, without writing a fact module? “Facts.d” is one mechanism for users to control some aspect of how their systems are managed. If a remotely managed system has an /etc/ansible/facts.d directory, any files in this directory ending in .fact, can be JSON, INI, or executable files returning JSON, and these can supply local facts in Ansible. An alternate directory can be specified using the fact_path play keyword. For example, assume /etc/ansible/facts.d/preferences.fact contains: [general] asdf=1 bar=2 This will produce a hash variable fact named general with asdf and bar as members. To validate this, run the following: ansible <hostname> -m setup -a \"filter=ansible_local\" And you will see the following fact added: \"ansible_local\": { \"preferences\": { \"general\": { \"asdf\" : \"1\", \"bar\": \"2\" } } } And this data can be accessed in a template/playbook as: {{ ansible_local.preferences.general.asdf }} The local namespace prevents any user supplied fact from overriding system facts or variables defined elsewhere in the playbook. To run a custom fact once the files are in place, you must specify the custom fact path (/home/vtguser/ansibleplaybooks/facts.d) : ansible apsrt3596 -m setup -a \"fact_path='/home/vtguser/ansibleplaybooks/facts.d'\" > /home/vtguser/ansibleplaybooks/customfacts/export/<whatever>.txt Run a playbook CustomFacts.yml --limit apsrt3596 https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#information-discovered-from-systems-facts ansible xaas -i /home/vtguser/ansibleplaybooks/sandbox -m setup -a 'gather_subset=ohai' --tree /home/vtguser/ansibleplaybooks/ohai/ohaidata/ ansible xaas -i /home/vtguser/ansibleplaybooks/sandbox -m setup -a 'gather_subset=ohai' --tree /home/vtguser/ansibleplaybooks/ set up task to write python script - skip in one level and go from there Maybe dump data to: rdb - relational databse Research: ansible filters ansible tasks json should be good enough to compare outputs. Run with ansible and chef and diff outputs - figure out what info is relevant in chef and ansible :x which python #/bin/python +x https://linuxacademy.com/community/posts/show/topic/25690-gather-server-info-and-save-in-csv-file-with-ansbile https://github.com/fboender/ansible-cmdb https://github.com/fboender/ansible-cmdb/blob/master/src/ansible-cmdb.py Plural Sight "},{"title":"Java","type":0,"sectionRef":"#","url":"docs/dev/java/java-index","content":"Java","keywords":""},{"title":"Markdown Syntax","type":0,"sectionRef":"#","url":"docs/dev/git/markdown-syntax","content":"","keywords":""},{"title":"Header 2","type":1,"pageTitle":"Markdown Syntax","url":"docs/dev/git/markdown-syntax#header-2","content":""},{"title":"Header 3","type":1,"pageTitle":"Markdown Syntax","url":"docs/dev/git/markdown-syntax#header-3","content":"Header 4# Header 5# Header 6# Closing hash marks are optional on all levels Header 1 Header 2 Header 3 Header 4 Header 5 Header 6 Definition Lists Using Markdown GitHub Enterprise uses a syntax called Markdown to help you add basic text formatting to Issues, Pull Requests, and files with the .md extension. Commonly Used Markdown Syntax Header# The # indicates a Header. # = Header 1, ## = Header 2, etc. List item A single * or - followed by a space will create a bulleted list. Bold item Two asterix ** on either side of a string will make that text bold.  Checklist A - followed by a space and [ ] will create a handy checklist in your issue or pull request. @mention When you @mention someone in an issue, they will receive a notification - even if they are not currently subscribed to the issue or watching the repository. #975 A # followed by the number of an issue or pull request (without a space) in the same repository will create a cross-link. 😃 Tone is easily lost in written communication. To help, GitHub allows you to drop emoji into your comments. Simply surround the emoji id with :. To say down here.↩ "},{"title":"Git Overview","type":0,"sectionRef":"#","url":"docs/dev/git/git-overview","content":"","keywords":""},{"title":"Github Enterprise","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#github-enterprise","content":""},{"title":"What is GitHub Enterprise?","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#what-is-github-enterprise","content":"GitHub Enterprise is a collaboration platform built on top of a distributed version control system called Git. GitHub Enterprise is focused on developers, the people who code and create software. Our focus is also the people who partner with and employ developers, who are encouraging them to build amazing things. GitHub Enterprise is similar to the public GitHub site except that it's an internally-hosted application which means that all the data stays within our internal UHG network. We do all we can to help unlock the creativity of developers and to foster a community of developers that can come together—as individuals and in teams—to create the future of software and make a difference in the world. GitHub Enterprise concentrates on three things: Building a technology platform that is like no other, on which developers can create, share and grow the best code possibleNurturing a community for developers; a safe and collaborative place that facilitates sharing, amplifies creativity, and supports the principles of open sourceProviding access, opening up a community of opportunity, where new developers can be born and where experienced developers can hone their skills and expand their knowledge In addition to being a place to host and share your Git projects, GitHub Enterprise provides a number of features to help you and your team collaborate more effectively. These features include: IssuesPull RequestsProjectsOrganizations and Teams "},{"title":"Repositories","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#repositories","content":"Creating a New Repository Hit the plus sign in Git and then click the \"new Repository\" buttonName the repo, select public or private, depending on how secure you want your repository, and click \"initialize this repo with a readme\" and click the \"Create\" button. "},{"title":"Exploring a GitHub Enterprise Repository","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#exploring-a-github-enterprise-repository","content":"A repository is the most basic element of GitHub Enterprise. It is easiest to imagine as a project's folder. However, unlike an ordinary folder on your laptop, a GitHub Enterprise repository offers simple yet powerful tools for collaborating with others. A repository contains all of the project files (including documentation), and stores each file's revision history. Whether you are just curious or you are a major contributor, knowing your way around a repository is essential User Accounts vs. Organization Accounts There are two account types in GitHub Enterprise, user accounts and organization accounts. While there are many differences in these account types, one of the more notable differences is how you handle permissions. User Accounts When you signed up for GitHub Enterprise, you were automatically given a user account. Permissions for a user account are simple, you add people as collaborators to specific repositories to give them full read-write access to the project. Organization Accounts Organization accounts provide more granular control over repository permissions. In an organization account you create teams of people and then give those teams access to specific repositories. Permissions can be assigned at the team level (e.g, read, write, or admin). "},{"title":"Repository Navigation","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#repository-navigation","content":"Code The code view is where you will find the files included in the repository. These files may contain the project code, documentation, and other important files. We also call this view the root of the project. Any changes to these files will be tracked via Git version control. Issues Issues are used to track bugs and feature requests. Issues can be assigned to specific team members and are designed to encourage discussion and collaboration. Pull Requests A Pull Request represents a change, such as adding, modifying, or deleting files, which the author would like to make to the repository. Pull Requests help you write better software by facilitating code review and showing the status of any automated tests. Projects Projects allow you to visualize your work with Kanban style boards. Projects can be created at the repository or organization level. Wiki Wikis in GitHub Enterprise can be used to communicate project details, display user documentation, or almost anything your heart desires. And of course, GitHub Enterprise helps you keep track of the edits to your Wiki Pulse Pulse is your project's dash board. It contains information on the work that has been completed and the work in progress. Graphs Graphs provide a more granular view into the repository activity, including who has contributed, when the work is being done, and who has forked the repository. README.md The README.md is a special file that we recommend all repositories contain. GitHub Enterprise looks for this file and helpfully displays it below the repository. The README should explain the project and point readers to helpful information within the project. CONTRIBUTING.md The CONTRIBUTING.md is another special file that is used to describe the process for collaborating on the repository. The link to the CONTRIBUTING.md file is shown when a user attempts to create a new issue or pull request. ISSUE_TEMPLATE.md The ISSUE_TEMPLATE.md (and its twin the pull request template) are used to generate templated starter text for your project issues. Any time someone opens an issue, the content in the template will be pre-populated in the issue body. "},{"title":"Issues","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#issues","content":"Using GitHub Enterprise Issues In GitHub Enterprise, you will use issues to record and discuss ideas, enhancements, tasks, and bugs. Issues make collaboration easier by: Replacing email for project discussions, ensuring everyone on the team has the complete story, both now and in the future.Allowing you to cross-link to related issues and pull requests.Creating a single, comprehensive record of how and why you made certain decisions.Allowing you to easily pull the right people into a conversation with @ mentions and team mentions. Activity: Creating A GitHub Issue Follow these steps to create an issue in the class repository: Click the Issues tab.Click New Issue.Type the following in the Subject line: YOUR-USERNAME WorkflowIn the body of the issue, include the text below: YOUR-USERNAME will choose an image, add a caption, and add both to a file.  Create a branch Edit the file Commit the changes Create a Pull Request Request a Review Make more changes Get an approval Merge the Pull Request Copy to clipboard Error Copied "},{"title":"Pages","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#pages","content":"Introduction to GitHub Pages GitHub Pages enable you to host free, static web pages directly from your GitHub Enterprise repositories. Several of the projects we use in class will use GitHub Pages as the deployment strategy. We will barely scratch the surface in this class, but there are a few things you need to know: You can create two types of websites, a user/organization site or a project site. We will be working with project websites.For a project site, GitHub Enterprise will only serve the content on a specific branch. Depending on the settings for your repository, GitHub Enterprise can serve your site from a master or gh-pages branch, or a /docs folder on the master branch.The rendered sites for our projects will appear at https://github.optum.com/pages/GHE-Training/repo-name. "},{"title":"Github Flow","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#github-flow","content":"Understanding the GitHub flow In this section, we discuss the collaborative workflow enabled by GitHub. The Essential GitHub Workflow The GitHub flow is a lightweight workflow that allows you to experiment with new ideas safely, without fear of compromising a project. Branching is a key concept you will need to understand. Everything in GitHub lives on a branch. By default, the \"blessed\" or \"canonical\" version of your project lives on a branch called master. This branch can actually be named anything, as we will see in a few minutes. When you are ready to experiment with a new feature or fix an issue, you create a new branch of the project. The branch will look exactly like master at first, but any changes you make will only be reflected in your branch. Such a new branch is often called a \"feature\" branch. As you make changes to the files within the project, you will commit your changes to the feature branch. When you are ready to start a discussion about your changes, you will open a pull request. A pull request doesn't need to be a perfect work of art - it is meant to be a starting point that will be further refined and polished through the efforts of the project team. When the changes contained in the pull request are approved, the feature branch is merged onto the master branch. In the next section, you will learn how to put this GitHub workflow into practice. Exploring Here are some interesting things you can check out later: guides.github.com/introduction/flow/ - An interactive review of the GitHub Workflow. "},{"title":"Basic Commands","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#basic-commands","content":"git init -- Initialized a new Git Repo git clone 'github url' --- Clones a Github repo to your local repo git remote –v --- Displays communication path from the local to GitHub repos git status --- Displays the status of modified files to be committed git add 'filename' --- Prepares a specific file to be committed from your local repo git add * --- Prepares all of your files in your local repo to be committed git commit –m ‘add your message’ --- Commits your file to GitHub with a version message git push origin 'branch' --- Uploads and commits file or files to the GitHub repos git pull git pull --prune git config --global fetch.prune "},{"title":"Basic Branch Commands","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#basic-branch-commands","content":"git branch --- Lists all branches in current repository git branch 'branch name' --- Creates a new branch git checkout 'branch name' --- Checks out branch and allows editing git merge 'branch name' --- Combines checked out branch back into master branch git branch --merged - see which branches are safe to delete git branch -d 'branch-name' - delete local branch To set pruning the remote tracking branches to be set as your default behavior when you pull, you can use the following configuration option: git config --global fetch.prune true "},{"title":"Basic Log Commands","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#basic-log-commands","content":"git log git log --oneline git log --oneline --graph git log --oneline --graph --decorate git log --oneline --graph --decorate --all git log --stat git log --patch git bisect git show SHA git revert (Generally safe since it creates a new commit.) git commit --amend (Only use on local commits.) git reset (Only use on local commits.) git cherry-pick (Only use on local commits.) git rebase (Only use on local commits.) git reset HEAD~ git reset 'commit id' "},{"title":"Basic Tag Commands","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#basic-tag-commands","content":"remove remote tags - git ls-remote --tags --refs origin | cut -f2 | xargs git push origin --delete remove local tags - git push --delete origin 'tagname' "},{"title":"Github Jekyll","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#github-jekyll","content":"Install Ruby Install Jekyll Enable Admin Privileges Open Terminal Run: gem install bundle jekyll jekyll new 'github repository' --force Example: jekyll new documentation --force Add this line to your Jekyll site's Gemfile: gem \"just-the-docs\" And add this line to your Jekyll site's _config.yml: theme: just-the-docs And then execute: $ bundle To test locally, run: bundle exec jekyll serve Then open a browser and navigate to localhost:4000 "},{"title":"Git Configuration Levels","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#git-configuration-levels","content":""},{"title":"Local Git Configuration","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#local-git-configuration","content":"In this section, we will prepare your local environment to work with Git. "},{"title":"Checking Your Git Version","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#checking-your-git-version","content":"First, let's confirm your Git Installation: $ git --version $ git version 2.11.0 Copy to clipboard Error Copied If you do not see a git version listed or this command returns an error, you may need to install Git. To get the latest version of Git, see Downloading and Installing Git. Git Configuration Levels Git allows you to set configuration options at three different levels. --system These are system-wide configurations. Saving configurations under --system on your work computer should be avoided in favor of --global and --local. These settings apply to all users on this computer and are usually stored in /etc/gitconfig. --global These are the user level configurations. They only apply to your user account and are persisted to ~/.gitconfig. --local These are the repository level configurations. They only apply to the specific repository where they are set. Local git configs are found within the git repo under .git/config. The default value for git config is --local. "},{"title":"Viewing Your Configurations","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#viewing-your-configurations","content":"If you would like to see which config settings have been added automatically, you can type git config --list. This will automatically read from each of the three config files and list the setting they contain. $ git config --list Copy to clipboard Error Copied You can also narrow the list to a specific configuration level by including it before the list option. $ git config --global --list Copy to clipboard Error Copied Configuring Your User Name and Email Git uses the config settings for your user name and email address to generate a unique fingerprint for each of the commits you create. You can't create commits without these settings: $ git config --global user.name \"Last, First\" $ git config --global user.email \"you@optum.com\" Copy to clipboard Error Copied Tip: Your user name and email should match the default account set up in Outlook. A good trick is to start an email and then copy and paste your email address. It will look something like Last, First 'you@optum.com'. "},{"title":"Git Config Privacy for Public Repos","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#git-config-privacy-for-public-repos","content":"The instructions for this exercise use the --global flag when identifying your user.name and user.emailconfiguration settings. In some cases, you may need to contribute to open source projects on public github. By omitting the --global flag in configuration commands, you can set configurations specific to a particular repo. To obscure your real name and work email, github users often supply their Github handle as user.name and their spam email accounts as user.email. You can configure the email field for all future commits in a specific repo by running the following command in the directory where the repository is located: For example: git config user.email \"personal_correspondence@example.com\" Copy to clipboard Error Copied Your name and email address will automatically be stored in the commits you make with Git. Configuring autocrlf $ //for Windows users $ git config --global core.autocrlf true $ //for Mac or Linux users $ git config --global core.autocrlf input Copy to clipboard Error Copied Different systems handle line endings and line breaks differently. If you open a file created on another system and do not have this config option set, git will think you made changes to the file based on the way your system handles this type of file. Memory Tip: autocrlf stands for auto carriage return line feed. "},{"title":"Viewing Local History","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#viewing-local-history","content":""},{"title":"Using Git Log","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#using-git-log","content":"When you clone a repository, you receive the history of all the commits made in that repository. The log command allows us to view that history on our local machine. Aliases An alias allows for a shorthand command to represent a long string on the command line. Example: Original Command git log --oneline --graph --decorate --all Creating the Alias git config --global alias.lol \"log --oneline --graph --decorate --all\" Using the Alias git lol "},{"title":"Leveraging Bash Aliases","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#leveraging-bash-aliases","content":"You can shorten commands like git and checkout or any other commands to save typing. Adding to ~/.profile alias g=\"git\" alias c=\"git checkout\" alias d=\"git diff\" alias glg=\"git log --branches --tags --graph --oneline --decorate --remotes\" Note that those aliases must be put into a startup file. Typically the ~/.profile or ~/.bashrc will work well for defining these aliases automatically whenever your terminal boots up. Helpful alias links https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases http://haacked.com/archive/2014/07/28/github-flow-aliases/ http://haacked.com/archive/2015/06/29/git-migrate/ http://haacked.com/archive/2017/01/04/git-alias-open-url/ "},{"title":"Merge Conflicts","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#merge-conflicts","content":"What is a merge conflict? When you work with a team (and even sometimes when you are working alone) you will occasionally create merge conflicts. At first, merge conflicts can be intimidating, but resolving them is actually quite easy. In real merge conflicts, it's important to know who to ask in case you aren't sure how to resolve the conflict on your own. Usually it's a good idea to ask the person who made the conflicting changes, or someone who is a code owner on the file. Local merge conflicts Merge conflicts are a natural and minor side effect of distributed version control. They only happen under very specific circumstances. Changes to the same \"hunk\" of the same fileTwo different branchesChanges on both branches happened since the branches have diverged "},{"title":"Merge Strategies","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#merge-strategies","content":"RebaseFast ForwardRecursiveOctopus "},{"title":".gitconfig","type":1,"pageTitle":"Git Overview","url":"docs/dev/git/git-overview#gitconfig","content":"[credential] helper = wincred [core] autocrlf = input [user] name=\"Greg Schullo\" email=greg.schullo@gmail.com [push] default = simple "},{"title":"Google Site Reliability Engineering","type":0,"sectionRef":"#","url":"docs/books/tech-books/google-site-reliability-engineering","content":"","keywords":""},{"title":"Preface","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#preface","content":"For the systems you look after, for the groups you work in, or for the organizations you’re building, please bear the SRE Way in mind: thoroughness and dedication, belief in the value of preparation and documentation, and an awareness of what could go wrong, coupled with a strong desire to prevent it. Welcome to our emerging profession "},{"title":"Chapter 1 - Introduction","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-1---introduction","content":"https://landing.google.com/sre/sre-book/chapters/introduction/ \"Hope is not a strategy\" The Sysadmin Approach to Service Management Development/Ops split has a number of disadvantages that fall broadly under two categories. Direct Costs and Indirect CostsDirect Costs are not subtle or ambiguousIndirect Costs are often subtle and often more costly to the organization than direct costs Google's Approach to Service Management: Site Reliability Engineering Site Reliability Engineering is what you get when you ask a software engineer to design an operations teamBy design, it's crucial SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload and doing the same tasks over and over. Pursuing Maximum Change Velocity Without Violating a Service’s SLO Error Budget - Shooting for 100% reliability is wrong for basically everything. Users receive little benefit between 99.999% and 100%The correct reliability to shoot for is not a technical question, but a production one. Some questions to take into consideration should be: What level of availability will the users be happy with, given how they use the product?What alternatives are available to users who are dissatisfied with the product’s availability?What happens to users’ usage of the product at different availability levels? The business must determine the availability target. Once established, the error budget is one minus the availability target. A service that is 99.99% available is 0.01% unavailable. The permitted 0.01% unavailability is the error budget, which can be spent on anything you want as long as you don't overspend. Monitoring Monitoring is one of the primary means by which service owners keep track of a system’s health and availability.There are 3 kinds of valid monitoring output Alerts Need to be handled by a human immediately Tickets Signify that humans need to take action, but not immediately Logging No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so. "},{"title":"Chapter 2 - The Production Environment at Google, from the Viewpoint of an SRE","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-2---the-production-environment-at-google-from-the-viewpoint-of-an-sre","content":"https://landing.google.com/sre/sre-book/chapters/production-environment/ Goes into detail about hardware and how data centers are structured at Google Whether it is at Google or elsewhere, monitoring is an absolutely essential component of doing the right thing in production. "},{"title":"Chapter 3 - Embracing Risk","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-3---embracing-risk","content":"https://landing.google.com/sre/sre-book/chapters/embracing-risk/ Again, shooting for 100% reliability is often not the best case scenario. A user cannot tell the difference between 99.99 and 99.999% uptime. With this in mind, rather than maximizing uptime, Site Reliability Engineering seeks to balance the risk of unavailability with the goals of rapid innovation and efficient service operations, so users' overall happiness with features, service, and performance is optimized. Measuring Service Risk Time Based Availability Availability = uptime/(uptime+downtime)Instead of using metrics around uptime, define availability in terms of request success rate (Aggregate Availability) Aggregate Availability Availability = successful requests/total requestsIn a typical application, not all requests are equalIt is best practice to test a new release on a small subset of a typical workload. This is known as canarying.Managing service reliability is largely about managing risk, and managing risk can be costly. "},{"title":"Chapter 4 - Service Level Objects","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-4---service-level-objects","content":"https://landing.google.com/sre/sre-book/chapters/service-level-objectives/ It is important to understand which behaviors really matter for a service and how to measure and evaluate those behaviors. We need to define and deliver a certain level of service to our users. Indicators Service Level Indicator (SLI) - a quantitative measure of some aspect of the level of service that is provided.Most services consider request latency and error rate as key SLIs. Objectives Service Level Objective (SLO) - a target value or range of values for a service level that is measured by an SLI.AgreementsService Level Agreements (SLA) - an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain. Choosing Targets Don't pick targets based on current performance. While understanding the merits and limits of a system is essential, adopting values without reflection may lock you into supporting a system that requires heroic efforts to meet its targets, and that cannot be improved without significant redesign.Keep it simpleAvoid absolutesHave as few SLOs as possibleControl MeasuresSLIs and SLOs are crucial elements in the control loops used to manage systems: Monitor and measure the system’s SLIs.Compare the SLIs to the SLOs, and decide whether or not action is needed.If action is needed, figure out what needs to happen in order to meet the target.Take that action. "},{"title":"Chapter 5 - Eliminating Toil","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-5---eliminating-toil","content":"https://landing.google.com/sre/sre-book/chapters/eliminating-toil/ Invent more, toil lessToil - Operational work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows. Toil leads to career stagnation, low morale, creates confusion, slows progress, sets precedent, promotes attrition, and causes breaches in faith (specifically for those who join SRE roles from elsewhere) "},{"title":"Chapter 6 - Monitoring Distributed Systems","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-6---monitoring-distributed-systems","content":"https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/ This section offers guidelines for what issues should interrupt a human via a page, and how to deal with issues that aren’t serious enough to trigger a page. Monitoring - Collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes. White Box Monitoring - Monitoring based on metrics exposed by the internals of the system, including logs, interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics. Black Box Monitoring - Testing externally visible behavior as a user would see it. Dashboard - An application (usually web-based) that provides a summary view of a service’s core metrics. A dashboard may have filters, selectors, and so on, but is prebuilt to expose the metrics most important to its users. The dashboard might also display team information such as ticket queue length, a list of high-priority bugs, the current on-call engineer for a given area of responsibility, or recent pushes. Alert - A notification intended to be read by a human and that is pushed to a system such as a bug or ticket queue, an email alias, or a pager. Respectively, these alerts are classified as tickets, email alerts, and pages. Root Cause - A defect in a software or human system that, if repaired, instills confidence that this event won’t happen again in the same way. A given incident might have multiple root causes: for example, perhaps it was caused by a combination of insufficient process automation, software that crashed on bogus input, and insufficient testing of the script used to generate the configuration. Each of these factors might stand alone as a root cause, and each should be repaired. Node and Machine - Used interchangeably to indicate a single instance of a running kernel in either a physical server, virtual machine, or container. There might be multiple services worth monitoring on a single machine. The services may either be: a. Related to each other: for example, a caching server and a web serverb. Unrelated services sharing hardware: for example, a code repository and a master for a configuration system like Puppet or Chef Push - Any change to a service’s running software or its configuration. WHY MONITOR? Analyze long term trendsComparing over time or comparing experiment groupsAlertingBuilding dashboardsDebugging The 4 Golden Signals The four golden signals of monitoring are latency, traffic, errors, and saturation. If you can only measure four metrics of your user-facing system, focus on these four. Latency - The time it takes to service a requestTraffic - A measure of how much demand is being placed on your systemErrors - The rate of requests that failSaturation - How \"full\" your system is. When creating rules for monitoring and alerting, asking the following questions can help you avoid false positives and pager burnout Does this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible? Will I ever be able to ignore this alert, knowing it’s benign? When and why will I be able to ignore this alert, and how can I avoid this scenario? Does the alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren’t being negatively impacted, such as test deployments that should be filtered out? Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be a long-term fix, or just a short-term workaround? The alert system should be designed to alert on imminent real problems, alert on potential long term problems, and support rapid diagnosis. These questions reflect a fundamental philosophy on pages and pagers: Every time the pager goes off, I should be able to react with a sense of urgency. I can only react with a sense of urgency a few times a day before I become fatigued. Every page should be actionable. Every page response should require intelligence. If a page merely merits a robotic response, it shouldn’t be a page. Pages should be about a novel problem or an event that hasn’t been seen before. "},{"title":"Chapter 7 - The Evolution of Automation at Google","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-7---the-evolution-of-automation-at-google","content":"https://landing.google.com/sre/sre-book/chapters/automation-at-google/ The value of automation Consistency: any action performed by a human or humans hundreds of times won’t be performed the same way each time: very few of us will ever be as consistent as a machine. This inevitable lack of consistency leads to mistakes, oversights, issues with data quality, and reliability problems. Consistency is, in many ways, the primary value of automation. A Platform: automatic systems can be extended and applied to more systems. A platform also centralizes mistakes. Faster Repairs Faster Action Time Saving The evolution of automation follows a path: No automation Externally maintained system specific automation Externally maintained generic automation Internally maintained system specific automation Systems that don't need automation (automation is automatic) Automate yourself out of a job: Automate ALL The Things "},{"title":"Chapter 8 - Release Engineering","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-8---release-engineering","content":"https://landing.google.com/sre/sre-book/chapters/release-engineering/ Release engineering is a relatively new and fast-growing discipline of software engineering that can be described as building and delivering software. Philosophy - Release engineering is guided by an engineering and service philosophy that’s expressed through four major principles. Self Service Model - Release engineering has developed best practices and tools that allow product development teams to run their own release processes. High Velocity - Aim to roll out new features as quickly as possible. Frequent releases results in fewer changes between versions. Hermetic Builds - Build tools must allow us to ensure consistency and repeatability. Enforcement of Policies and Procedures Continuous Build and Deployment Rapid - A Google developed automated release system that leverages a number of Google technologies to provide a framework that delivers scalable, hermetic, and reliable releases Building, Branching, Testing, Packaging, Rapid, Deployment When equipped with the right tools, proper automation, and well-defined policies, developers and SREs shouldn’t have to worry about releasing software. Releases can be as painless as simply pressing a button. "},{"title":"Chapter 9 - Simplicity","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-9---simplicity","content":"https://landing.google.com/sre/sre-book/chapters/simplicity/ Boring is a good thing in software Remove code if it is not being run. Don't flag it. Don't comment it out. Remove it. Software simplicity is a prerequisite to reliability. "},{"title":"Chapter 10 - Practical Alerting from Time-Series Data","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-10---practical-alerting-from-time-series-data","content":"https://landing.google.com/sre/sre-book/chapters/practical-alerting/ \"May the queries flow and the pager stay silent.\" - Traditional SRE Blessing Monitoring, the bottom layer of the Hierarchy of Production Needs, is fundamental to running a stable service. Monitoring enables service owners to make rational decisions about the impact of changes to the service, apply the scientific method to incident response, and to ensure their reason for existence: to measure the service’s alignment with business goals. A service should be run in a symbiotic relationship with its monitoring. Monitoring a large system is challenging for a few reasons: The number of components to analyze.The need to maintain a low maintenance burden on engineers responsible for the system Mass data collection allows for cheap and efficient solutions where data can be used to generate charts and to create alerts. Because collection is no longer a short lived process, the history of collected data can be used for alert computation. These features help to meet the goal of simplicity by allowing the system overhead to be kept low so that the people running the services can remain agile and respond to continuous change in the system as it grows.Time series data - Chronological lists of data pointsArena - A fixed-sized block of memory structureVectors - slices and cross sections of a multidimensional matrix of data points in an arenaLabelset - the name of a time-series that's implemented as a set of labels expressed as key value pairsA few of these label names are declared important, as the time-series must, at a minimum, have the following labels defined to be identifiable in the time-series database:var - the name of the variablejob - the name given to the type of server being monitoredservice - a loosely defined collection of jobs that provide a service to the users, either internal or external zone - A Google convention that refers to the location (typically the datacenter) of the Borgmon that performed the collection of this variable.  "},{"title":"Chapter 11 - Being On Call","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-11---being-on-call","content":"https://landing.google.com/sre/sre-book/chapters/being-on-call/ SREs are typically have a diverse background in systems and software engineering as SREs place heavy emphasis on the use of engineering to approach problems previously experienced by operations teams that exist at a scale that would be intractable without software engineering solutions.50% of an SREs time should be allocated to engineering projects that further scale the impact of the team through automation, in addition to improving the service Balanced On Call Balance in QuantityIf a service entails enough work to justify growing a single-site team, we (Google) prefer to create a multi-site team. A multi-site team is advantageous for two reasons.Night shifts have detrimental effects on people's health, and a multi-site \"follow the sun\" rotation allows teams to avoid night shifts all together.Limiting the number of engineers in the on-call rotation ensures that engineers do not lose touch with the production system(s).However, multi-site teams input communication and coordination overhead. Therefore, the decision to go multi-site or single-site should be based upon the trade-offs each option entails, the importance of the system, and the workload each system generates. Balance in Quality For each on call shift, an engineer should have sufficient time to deal with any incidents and follow-up activities such as writing postmortems. Google has found that on average, dealing with the tasks involved in an on-call incident - root-cause analysis, remediation, and follow-up activities (like writing a postmortem and fixing bugs) takes about 6 hours. Compensation Adequate compensation needs to be considered for out-of-hours support. Different organizations handle on-call compensation in different ways; Google offers time-off-in-lieu or straight cash compensation, capped at some proportion of overall salary. The compensation cap represents, in practice, a limit on the amount of on-call work that will be taken on by any individual. This compensation structure ensures incentivization to be involved in on-call duties as required by the team, but also promotes a balanced on-call work distribution and limits potential drawbacks of excessive on-call work, such as burnout or inadequate time for project work. Feeling Safe Being an SRE on-call typically means assuming responsibility for user-facing, revenue-critical systems or for the infrastructure required to keep these systems up and running. Modern research identifies two distinct ways of thinking that an individual may (consciously or subconsciously) choose when faced with a challenge: Intuitive, automatic, and rapid action Rational, focused, and deliberate cognitive functions When one is dealing with outages related to complex systems, the second option is more likely to produce better results and lead to well planned incident handling. To ensure that engineers are in the appropriate frame of mind to leverage the mindset of rational, focused, and deliberate cognitive functions, it is important to reduce the stress related to being on-call. The importance and the impact of the services and the consequences of potential outages can create significant pressure on the on-call engineers, damaging the well-being of individual team members and possibly prompting SREs to make incorrect choices that can endanger the availability of the service. Stress hormones like cortisol and corticotropin-releasing hormone (CRH) are known to cause behavioral consequences—including fear—that can impair cognitive functions and cause suboptimal decision making Under the influence of these stress hormones, the more deliberate cognitive approach is typically subsumed by unreflective and unconsidered (but immediate) action, leading to potential abuse of heuristics. Heuristics are very tempting behaviors when one is on-call. For example, when the same alert pages for the fourth time in the week, and the previous three pages were initiated by an external infrastructure system, it is extremely tempting to exercise confirmation bias by automatically associating this fourth occurrence of the problem with the previous cause. While intuition and quick reactions can seem like desirable traits in the middle of incident management, they have downsides. Intuition can be wrong and is often less supportable by obvious data. Thus, following intuition can lead an engineer to waste time pursuing a line of reasoning that is incorrect from the start. Quick reactions are deep-rooted in habit, and habitual responses are unconsidered, which means they can be disastrous. The ideal methodology in incident management strikes the perfect balance of taking steps at the desired pace when enough data is available to make a reasonable decision while simultaneously critically examining your assumptions. It’s important that on-call SREs understand that they can rely on several resources that make the experience of being on-call less daunting than it may seem. The most important on-call resources are: Clear escalation pathsWell-defined incident-management proceduresA blameless postmortem culture It should always be possible to escalate to a partner team when necessary. Appropriate escalation of outages is generally a principled way to react to serious outages with significant unknown dimensions (what does that mean?)When an incident occurs, it's important to evaluate what went wrong, recognize what went well, and take action to prevent the same errors from happening again in the future. Focus on events rather than people. Mistakes happen, and software should make sure that we make as few mistakes as possible.Recognize automation opportunities. It's one of the best ways to prevent human error. Avoiding Inappropriate Operational Load Misconfigured monitoring is a common cause of operational overload.Paging alerts should be aligned with the symptoms that threaten a service's SLOsAll paging alerts should be actionableLow priority, frequent alerts disrupt productivity and can cause serious alerts to be treated less seriously It is important to control the number of alerts for a single incidentSilence duplicate or uninformative alerts generated by a single incidentSRE teams should be sized to allow every engineer to be on call at least once or twice a quarter. This allows for familiarity with the production system(s) and addresses an appropriate confidence level. "},{"title":"Chapter 12 - Effective Troubleshooting","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-12---effective-troubleshooting","content":"https://landing.google.com/sre/sre-book/chapters/effective-troubleshooting/ We can think of the troubleshooting process as an application of the hypothetico-deductive method: given a set of observations about a system and a theoretical basis for understanding system behavior, we iteratively hypothesize potential causes for failure and try to test those hypotheses.A process for troubleshooting a. Problem Report b. Triage c. Examine d. Diagnose e. Test/Treat f. Cure Test Hypotheses in two ways. Compare the observed state of the system against our theories to confirm or disconfirm or we could \"treat\" the system by changing the system in a controlled way and observe the results. Ineffective troubleshooting sessions are caused by problems at the triage, examine, and diagnose steps usually due to lack of deep system understanding. Common issues include looking at symptoms that aren't relevant or misunderstanding the meaning of system metrics, misunderstanding how to change the system, its inputs or environment to safely and effectively test a hypotheses, thinking up improbable theories or latching on to causes of past issues, and hunting down correlations that are coincidences or are correlated with shared causes Not all failures are equally probable Correlation does not equal causation Problem Report An effective problem reports should tell you the expected behavior, actual behavior, and how to reduce the unwanted behavior if possible. Triage Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. Ignore this instinctYour first response should be to make the system work as well as it can under the circumstances. Examine We need to be able to examine what each component in the system is doing in order to understand whether or not it's behaving correctlyMonitoring systems can record metrics and metrics are a good place to start figuring out what's wrong.Logging is a valuable tool as it allows for understanding exactly what a process is doing at a given point in time. Analyzing system logs across one or many processes may be necessary. Tracing requests through the WHOLE STACK provides a VERY POWERFUL way to understand how a distributed system is working. More on Logging Text logs are helpful for reactive debugging in real timeStored logs in a structured format can make it possible to build tools to conduct analysis with more information.Having multiple verbosity levels available along with a way to increase these levels on the fly is useful. This functionality enables you to examine operations in detail without having to restart your process when there are issues, while still allowing you to dial back the verbosity levels when your service is operating normally. Diagnose Simplify and reduceInject known test data as input to check the output is what is expected. Having reproducible test cases makes debugging much fasterIn a multi-layer system, it is useful to start systematically from one end of the stack and to the other end, examining each component in turn.Ask \"what,\" where,\" and \"why.\" A malfunctioning system is often still attempting to do something, just not what you want it to be doing. Finding out what it's doing instead of what you want and why it's doing it, and where resources are being used can help to gain understanding how things have gone wrong. Test and Treat Test hypotheses using the experimental method to rule out hypotheses. Negative Results are Magic A negative result is an experiment outcome where the expected effect is absent. Any experiment that doesn't work out as planned.Negative results should not be ignored or discounted as realizing you are wrong has valueExperiments with negative results are conclusiveTools and methods can outlive an experiment and inform future workPublishing negative results improves our industry's data-driven culturePublish results of your experiments Cure Proving a cause is often difficult in production systems. Most of the time we can only find a probable cause because systems are complex and reproducing an issue may not be an option in a production system. Making Troubleshooting Easier The most fundamental ways to simplify and speed up troubleshooting are building observability and by designing systems with well understood and observable interfaces between components. "},{"title":"Chapter 13 - Emergency Response","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-13---emergency-response","content":"https://landing.google.com/sre/sre-book/chapters/emergency-response/ Things break; that's life. What to Do When Systems Break Don't panic... Typically, no one is in physical danger... Things could be worse.Pull in more people where necessary, but be familiar with and follow your company's incident response process Test-Induced Emergency Break systems, watch how they fail, and make changes to improve reliability and prevent future failures from occurring. Change-Induced Emergency Perform tests on configuration changes to ensure they don’t result in unexpected and undesired behavior. Scale and complexity of infrastructure make it hard to anticipate every dependency or interaction. These tests can help uncover unknown Process-Induced Emergency Sometimes, automation efficiency can be scary to think about, especially when automation does not go as planned. All Problems Have Solutions Systems will not only break, but will break in ways that could never previously be imagined.Once an emergency has been mitigated, don't forget to set aside time to clean up and write up the incident Learn from the Past. Don't Repeat it. Keep a history of outages. History is about learning from everyones' mistakes.Ask the big, even improbable questions. What if...? "},{"title":"Chapter 14 - Managing Incidents","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-14---managing-incidents","content":"https://landing.google.com/sre/sre-book/chapters/managing-incidents/ Effective incident management is key to limiting the disruption caused by an issue and restoring normal business operations as quickly as possible. Anatomy of Unmanaged Incidents Sharp Focus on the Technical Problem Many times we think about trying to resolve an issue rather than the big picture and how to mitigate an issue if the technical task at hand is overwhelming Poor CommunicationFreelancing Work with others in solving problems. Sometimes troubleshooting efforts can make a situation worse. Elements of Incident Management Process Incident management skills and practices exist to channel the energies of enthusiastic individuals. A well designed incident management process has the following features Recursive Separation of Responsibilities Know your role and don't do someone elses. A clear separation of responsibilities allows individuals more autonomy. Incident CommandOperational WorkCommunicationPlanningA Recognized Command Post It is important for interested parties to know where they can interact with the incident commander. This may include a War Room or incident updates via email and INC tickets. Live Incident State Document The incident commander's most important responsibility is to keep a living incident document. Clear, Live Handoff When to Declare an Incident It is better to declare an incident early and then find a simple fix than to have to spin up the same incident management framework hours into an ongoing problem.Set clear conditions for declaring an incident Example:Do you need to involve a second team to resolve this problem?Is the outage visible to customers?Is the issue involved even after an hour's concentrated analysis? Best Practices for Incident Management Prioritize - Stop the bleeding, restore service, and preserve the evidence for root-causing. Prepare - Develop and document your incident management procedures in advance, in consultation with incident participants. Trust - Give full autonomy within the assigned role to all incident participants. Introspect - Pay attention to your emotional state while responding to an incident. If you start to feel panicky or overwhelmed, solicit more support. Consider alternatives - Periodically consider your options and re-evaluate whether it still makes sense to continue what you’re doing or whether you should be taking another tack in incident response. Practice - Use the process routinely so it becomes second nature. Change it around - Were you incident commander last time? Take on a different role this time. Encourage every team member to acquire familiarity with each role. "},{"title":"Chapter 15 - Postmortem Culture: Learning from Failure","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-15---postmortem-culture-learning-from-failure","content":"https://landing.google.com/sre/sre-book/chapters/postmortem-culture/ \"The cost of failure is education.\" A postmortem is a written record of an incident, its impact, the actions taken to mitigate or resolve it, the root cause(s), and the follow-up actions to prevent the incident from recurring. Google's Postmortem Philosophy The primary goals of writing a postmortem are to: a. Ensure that the incident is documented b. all contributing root cause(s) are well understood c. effective preventative actions are put in place to reduce the likelihood and/or impact of recurrence Common postmortem triggers include: User-visible downtime or degradation beyond a certain thresholdData loss of any kindOn-call engineer intervention (release rollback, rerouting of traffic, etc.)A resolution time about some thresholdA monitoring failure (which usually implies manual incident discovery) Blameless postmortems are essential. They must focus on identifying the contributing cause(s) of the incident without indicating any individual or team for bad or inappropriate behavior. People are much more willing to bring issues to light if a blameless culture is in place and practiced. Best Practice: Avoid blame and keep things constructive. Collaborate and Share Knowledge The postmortem workflow includes collaboration and knowledge-sharing at every stage Postmortem tools should include the following key features: Real-time collaborationAn open commenting/annotation systemEmail notifications Writing postmortems involves a formal review and publication. Review criteria may include: What key incident data collected for posterity?Are the impact assessments complete?Was the root cause sufficiently deep?Is the action plan appropriate and are resulting bug fixes appropriate priority?Did we share the outcome with relevant stakeholders? Best Practice: No Postmortem Left Unreviewed. Introducing a Postmortem Culture Introducing a blameless postmortem culture into an organization can be difficult. One of the biggest challenges is that some may question their value given the cost of their preparation. The following strategies can help in facing this challenge: Ease postmortems into the workflow. A trial period with several complete and successful postmortems may help prove their value, in addition to helping identify what criteria should initiate a postmortem.Make sure that writing effective postmortems is a rewarded and celebrated practice, both publicly and through individual and team performance management.Encourage senior leadership's acknowledgement and participation. Best Practice: Visibly Reward People for Doing the Right Thing. Best Practice: Ask for Feedback on Postmortem Effectiveness "},{"title":"Chapter 16 - Tracking Outages","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-16---tracking-outages","content":"https://landing.google.com/sre/sre-book/chapters/tracking-outages/ Escalator - all alert notifications require acknowledgement from a human. If no acknowledgment is received from the primary on call, the alert gets escalated to the secondary on-call. Outalator - Google's homegrown outage management solution Many organizations use messaging systems like Slack, Hipchat, or even IRC for internal communication and/or updating status dashboards. These systems are a great place to hick into with an outage tracking system. Aggregation A single even will often trigger multiple alerts.While it may be worthwhile to attempt to minimize the number of alerts triggered by a single event, triggering multiple alerts is unavoidable in most trade offs between false positives and false negatives.The ability to group multiple alerts together into a single incident is critical in dealing with duplication. Tagging Tagging can be used to add meta data to notifications. An example may look something like cause:network:switchTags can be parsed and turned into a convenient link. Analysis Historical data is useful when one is responding to an incident. What was done last time, is often a good starting point when diagnosing issues. Historical data is far more useful when it concerns systematic, periodic, or other wider problems that may exist. Enabling such analysis is one of the most important functions of an outage tracking tool. Reporting and Communication It is important to have the ability to select zero or more outages including their subjects, tags, and \"important\" annotation in an email to the next on-call engineer. Unexpected Benefits Benefits of being able to identify an alert coincides with a given other outage has obvious benefits that include increasing the speed of diagnosis and reducing lead on other teams by acknowledging there is a real incident. Non-obvious benefits include improving cross team visibility, which makes a difference in incident resolution and incident mitigation. "},{"title":"Chapter 17 - Testing for Reliability","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-17---testing-for-reliability","content":"https://landing.google.com/sre/sre-book/chapters/testing-reliability/ \"If you haven't tried it, assume it's broken.\" A key responsibility of SREs is to quantify confidence in the systems they maintain by adapting classical software testing techniques to systems at scale. Confidence can be measured both by past reliability and future reliability by analyzing data provided from monitoring system behavior. In order for these predictions to be strong and useful, one of the following must be true: The site remains completely unchanged over time with no software releases or changes in the server fleet, which means that future behavior will be similar to past behavior.You can confidently describe all change to the sire, in order for analysis to allow for the uncertainty incurred by each of these changes. Relationships Between Testing and Mean Time to Repair Passing a test or series of tests doesn't prove reliability, but tests that are failing generally prove the absence of reliability.A monitoring system can uncover bugs, but only as quickly as as the reporting pipeline can react. The Mean Time to Repair (MTTR) measures how long it takes the operations team to fix the bug, either through a rollback or another action.It's possible for a testing system to identify a bug with zero MTTR. Zero MTTR occurs when a system-level test is applied to a subsystem and that test detects the exact same problem that monitoring would detect. The more bugs you can find with zero MTTR, the higher the Mean Time Between Failures (MTBF) experienced by your users. Types of Software Testing Software testing can be broken out into two main groups. Traditional and Production. Traditional tests are common in software development where production tests are performed on live web services to evaluate whether a deployed software system is working correctly. Traditional Tests Hierarchy of Traditional TestsUnit Tests - A unit test is the smallest and simplest form of software testing employed to assess a separable unit of software, such as a class or function for correctness independent of the larger software system containing the unit. Unit tests are also employed as a form of specification to ensure that a function or module exactly performs the behavior required by the system and are commonly used to introduce test-driven development concepts.Integration Tests - Software components that pass individual unit tests are assembled into larger components. Integration tests are then run on an assembled component to verify that if functions correctly. Dependency injection is an extremely powerful technique for creating mocks of complex dependencies to an engineer can cleanly test a component. Example: Replace a stateful database with a lightweight mock that has precisely specified behavior.System Tests - The largest scale test that engineers run for an undeployed system. All modules belonging to a specific component, such as a server that passed integration tests, are assembled into the system. Then the end to end functionality of the system is tested in many different flavors: Smoke Tests - tests where simple but critical behavior is evaluated. Also known as sanity testing.Performance Tests - test to ensure performance of the system stays acceptable over the duration of its lifecycle. A performance test also ensures that over time, a system doesn't degrade or become too expensive.Regression Tests - test to prevent bugs from entering the codebase. Document bugs and don't accidentally introduce bugs that have already invested time and effort to eliminate back into the codebase. Production Tests - tests that interact with a live, production system, as opposed to a system in a hermetic testing environment. These tests are in many ways similar to black-box monitoring and are sometimes called black-box tests. Production tests are essential to running a reliable production service. Rollout Entangle Test - to manage uncertainty and hide risk from users, changes may be pushed in a different order than they were added to source control. Rollouts often happen in stages, using mechanisms that gradually shuffle users around, in addition to monitoring at each stage to ensure that the new environment isn't hitting anticipated, yet unexpected problems.Configuration Test - tests that examine production to see how a particular binary is actually configured and reports discrepancies against that file. Configuration tests are built and tested for a specific version of the checked-in configuration file.Stress Test - tests used to understand the limits of the system and its components. Stress tests answer questions such as, how full can a database get before writes start to fail or how many queries a second can be sent to an application server before it comes overloaded, causing requests to fail?Canary Test - test where a subset of components (servers is a good example) are upgraded to a new version or configuration and then left in an incubation period, leaving the rest of the components in a known good state. Provided things don't go awry, the rest of the components are upgraded as well. If things do go awry, those upgraded components may be quickly restored to a known good state. Creating a Test and Build Environment Start with testing that delivers the most impact with the least effort. Prioritize the codebase and focus on functions or classes that are mission and business critical. A few ways to establish a strong testing culture is to start documenting all reported bugs as test cases and to set up testing infrastructure with versioned source control that tracks every change to a codebase. Once source control is in place, the addition of a continuous build system that builds software and runs tests every time code is submitted is very helpful. When the build system notifies engineers of an issue, it is crucial that fixing it becomes top priority for a few reasons: It's harder to fix what is broken if there are changes to the codebase after the defect is introducedBroken software slows down the team because they must work around the breakageRelease cadences, such as nightly and weekly builds, lose their valueThe ability of the team to respond to a request for an emergency release (for example, in response to a security vulnerability disclosure) becomes much more complex and difficult.  Testing Disaster Many disaster recovery tools can be carefully designed to operate offline. Such tools do the following: Compute a checkpoint state that is equivalent to cleanly stopping the servicePush the checkpoint state to be loadable by existing non-disaster validation toolsSupport the usual release barrier tools, which trigger the clean start procedure Pushing to Production Software testing infrastructure often can't see production configuration Testing is one of the most profitable investments engineers can make to improve the reliability of their product. Testing is a continuous activity. The amount of effort required to write good tests is substantial, as is the effort to build and maintain infrastructure that promotes a strong testing culture. You can't fix a problem until you understand it, and in engineering, you can only understand a problem by measuring it. "},{"title":"Chapter 18 - Software Engineering in SRE","type":1,"pageTitle":"Google Site Reliability Engineering","url":"docs/books/tech-books/google-site-reliability-engineering#chapter-18---software-engineering-in-sre","content":"https://landing.google.com/sre/sre-book/chapters/software-engineering-in-sre/ "},{"title":"Javascript Datatypes","type":0,"sectionRef":"#","url":"docs/dev/javascript/javascript-datatypes","content":"Placeholder for Javascript Datatype documentation.","keywords":""},{"title":"Javascript Control Structures","type":0,"sectionRef":"#","url":"docs/dev/javascript/javascript-control-structures","content":"","keywords":""},{"title":"Javascript Statements","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#javascript-statements","content":"Example: Copy var x, y, z;//Statement 1 x =5;//Statement 2 y =6;//Statement 3 z =x+y;//Statement 4 A Javascript program is a list of statements to be executed by a computer Javascript statements are composed of values, operators, expressions, keywords, and comments. The statements are executed, one by one, in the order as they are written. Javascript programs (and Javascript statements) are often called Javascipt code "},{"title":"Javascript Programs","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#javascript-programs","content":"A computer program is a list of \"instructions\" to be \"executed\" by a computer. In a programming language, these programming instructions are called statements. A Javascript program is a list of programming statements. In HTML, Javascript programs are executed by the web browser. "},{"title":"Javascript Functions and Events","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#javascript-functions-and-events","content":"A Javascript function is a block of Javascript code that can be executed when \"called\" for. For example, a function can be called when an event occurs. You can use any number of scripts in an HTML document. Scripts can be placed in the <head>, <body>, or in both sections of an HTML page between <script> tags. Placing scripts at the bottom of the <body> element improves the display speed because script compilation slows down the display. "},{"title":"External Javascript","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#external-javascript","content":"Scripts can be placed in external filesExternal scripts are practical when the same code is used in many different web pagesJavascript files have the file extension .jsTo use an external script, put the name of the script file in the src (source) attribute of a <script> tag Example: <script src =\"myScript.js\"></script> Placing scripts in external files has some advantages It separates HTML and codeIt makes HTML and Javascript easier to read and maintainCached Javascript files can speed up page loads "},{"title":"External References","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#external-references","content":"External scripts can be referenced with a full URL or with a path relative to the current web page. "},{"title":"Javascript Output","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#javascript-output","content":"Javascript can display data in different ways writing into an HTML Element using innerHTMLwriting into the HTML output using document.write()Writing into an alert box, using window.alert()Writing into the console, using console.log() Using innerHTML To access an HTML Element, Javascript can use the document.getElementById(id) method. The id attribute defines the HTML element. The innerHTML property defines the HTML content. Using document.write() For testing purposes, it is convenient to use document.write() : Using document.write() after an HTML doc is loaded will delete all existing HTML. Using window.alert() You can se an alert to display information. Using console.log() For debugging purposes, you can use the console.log() method to display data. "},{"title":"Javascript Codeblocks","type":1,"pageTitle":"Javascript Control Structures","url":"docs/dev/javascript/javascript-control-structures#javascript-codeblocks","content":"Javascript statements can be grouped together in code blocks, inside curly braces {...}. The purpose of code blocks is to define statements to be executed together. One place you will find statements grouped together in blocks, is in JavaScript "},{"title":"Java Syntax","type":0,"sectionRef":"#","url":"docs/dev/java/java-syntax","content":"","keywords":""},{"title":"Basic Syntax","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#basic-syntax","content":""},{"title":"Print Line","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#print-line","content":"In Java, System.out.pringln() can print to the console: System is a class from the core library provided by Javaout is an object that controls outputprintln() is a method associated with that object that receives a single argument "},{"title":"main() Method","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#main-method","content":"In Java, every application must contain a main() method, which is the entry point for the application. All other methods are invoked from the main() method. The signature of the method is public static void main(String[] args) { }. It accepts a single argument: an array of elements of type String. "},{"title":"Classes","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#classes","content":"In Java, a class represents a single concept. A Java program must have one class whose name is the same as the program filename. In the example, the Person class must be declared in a program file named Person.java. "},{"title":"Statements","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#statements","content":"In Java, a statement is a line of code that executes a task and is terminated with a ;. "},{"title":"Comments","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#comments","content":"In Java, comments are bits of text that are ignored by the compiler. They are used to increase the readability of a program. Single line comments are made by using // and multi-line comments are made by starting with / and ending with /. "},{"title":"Whitespace","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#whitespace","content":"Whitespace, including spaces and newlines, between statements ignored. "},{"title":"Compiling","type":1,"pageTitle":"Java Syntax","url":"docs/dev/java/java-syntax#compiling","content":"In Java, when we compile a program, each individual class is converted into a .class file, which is known as byte code. The JVM (Java Virtual Machine) is used to run the byte code. "},{"title":"Javascript Overview","type":0,"sectionRef":"#","url":"docs/dev/javascript/javascript-overview","content":"Javascript is the programming language of HTML and the web. Javascript can change HTML format, HTML styles (CSS), and Javascript can show and hide HTML elements. Javascript is the default scripting language in HTML.","keywords":""},{"title":"changing-python-versions","type":0,"sectionRef":"#","url":"docs/dev/python/changing-python-versions","content":"Under no circumstances should you touch or delete anything in the /System or /usr/bin/python","keywords":""},{"title":"Javascript Syntax","type":0,"sectionRef":"#","url":"docs/dev/javascript/javascript-syntax","content":"","keywords":""},{"title":"Javascript Values","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#javascript-values","content":"The javascript syntax defines two types of values: Fixed values and variable values. Fixed values are called literals. Variable values are called variables. "},{"title":"Javascript Literals","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#javascript-literals","content":"The most important rules for writing fixed values are: Numbers are written with or without decimals. Strings are text, written within double or single quotes. "},{"title":"Javascript Variables","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#javascript-variables","content":"In a programming language, variables are used to store data values. Javascript uses the keyword var to declare variables. "},{"title":"Javascript Operators","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#javascript-operators","content":"Placeholder for operators "},{"title":"Semicolons","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#semicolons","content":"Semicolons separate Javascript statements. Add a semicolon at the end of each executable statement. When separated by semicolons, multiple statements are allowed on a single line, although that is not encouraged from a readability or best practice standpoint. "},{"title":"Whitespace","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#whitespace","content":"Javascript ignores multiple spaces. Whitespace can be added to make your script more readable. A good practice is to put spaces around operators ( = + - * / ) "},{"title":"Line Length and Line Breaks","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#line-length-and-line-breaks","content":"Placeholder for line length and breaks content. "},{"title":"Javascript Placement","type":1,"pageTitle":"Javascript Syntax","url":"docs/dev/javascript/javascript-syntax#javascript-placement","content":"In HTML, Javascript code must be inserted between tags. Old Javascript may use a type attribute: <script type = \"text/javascript\">. The type attribute is not required. "},{"title":"Javascript Keywords","type":0,"sectionRef":"#","url":"docs/dev/javascript/javascript-keywords","content":"Javascript statements often start with a keyword to identify the Javascript action to be performed. Javascript keywords are reserved words. Reserved words cannot be used as variable names. break - terminates a switch or loop continue - jumps out of a loop and starts at the top debugger - stops the execution of JavaScript and calls (if available) the debugging function do...while - executes a block of statements and repeats the block while the given condition(s) are true for - marks a block of statements to be executed, as long as the condition(s) given hold true function - declares a function if...else - marks a block of statements to be executed, depending on a condition return - exits a function switch - marks a block of statements to be executed, depending on a condition try...catch - implements error handling to a block of statements var - declares a variable","keywords":""},{"title":"Python Debugging","type":0,"sectionRef":"#","url":"docs/dev/python/python-debugging","content":"","keywords":""},{"title":"Traceback","type":1,"pageTitle":"Python Debugging","url":"docs/dev/python/python-debugging#traceback","content":"When a program contains a significant error, Python displays a traceback, which is an error report. Python looks through the file and tries to identify the problem. Check the traceback; it might give you a clue as to what issue is preventing the program from running. "},{"title":"Node.js Overview","type":0,"sectionRef":"#","url":"docs/dev/node.js/node.js-overview","content":"","keywords":""},{"title":"Node.js Architecture","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#nodejs-architecture","content":"JS Code -> JS Engine -> Machine Code Node.js is NOT a programming language. Node.js is NOT a framework. "},{"title":"How Node.js Works","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#how-nodejs-works","content":"Node.js is great for scalability because of its non-blocking, asynchronous behavior by default. Node.js is ideal for I/O intensive applications due to its asynchronous, threading architecture. Do not use Node.js for CPU intensive applications, such as video encoding or image manipulation service. "},{"title":"Node.js Module System","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#nodejs-module-system","content":"Global Objects An object that always exits in the global scope. In Javascript, there's always a global object defined. Examples: console.log(); setTimeout(); setInterval(); clearInterval(); window.setTimeout(); Variables defined locally are not transferred to the global objects. Local variables defined the same as globals take precedence over global definitions. As a rule of thumb, avoid assigning variables as global variables.  Every file in Node.js is referred to as a module. each Node.js application has at least one file (module) referred to as the \"main\" module.  ./ indicates the current folder Require function is used to call and use a module. Best practice is to store values retrieved from other modules in constant variables. Node.js does not execute code directly. It wraps it and executes each module inside of a function. This is called the Modular Wrapper Function. "},{"title":"Built in Node.js Modules","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#built-in-nodejs-modules","content":"Modules are similar to Javascript libraries. A module is a set of functions you want to use. "},{"title":"Path","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#path","content":"Using Path -> const path = require('path'); parse method -> path.parse Example: var pathObj = path.parse(_filename); console.log(pathObj); Only use asynchronous methods. "},{"title":"Events","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#events","content":"A signal that something has happened. "},{"title":"Classes","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#classes","content":"The first word of each string is capitalized to indicate a class. Example: const EventEmitter  A class is a container for many related methods. "},{"title":"Listener","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#listener","content":"A function that is called when an event is raised. "},{"title":"Event Arguments","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#event-arguments","content":"Functions defined withing a class do not need the function keyword. When a function is in a class, it is then referred to as a \"method\" of that class. "},{"title":"HTTP Module","type":1,"pageTitle":"Node.js Overview","url":"docs/dev/node.js/node.js-overview#http-module","content":"Build a simple web server with this module. "},{"title":"Python Datatypes","type":0,"sectionRef":"#","url":"docs/dev/python/python-datatypes","content":"","keywords":""},{"title":"Integers","type":1,"pageTitle":"Python Datatypes","url":"docs/dev/python/python-datatypes#integers","content":"integers are used in Python to represent positive or negative whole numbers with no decimal point. "},{"title":"Floats","type":1,"pageTitle":"Python Datatypes","url":"docs/dev/python/python-datatypes#floats","content":"floats are used in Python to represent numbers that aren't integers. They can be created directly by entering a number with a decimal point or by using operations such as division on integers. Extra zeros at the number's end are ignored. floats can be used to indicate scientific notation float = 1.5e2 == 150 "},{"title":"Strings","type":1,"pageTitle":"Python Datatypes","url":"docs/dev/python/python-datatypes#strings","content":"Text is considered a string in Python. A string is created by entering text between two single or double quotes. Anything inside quotes is considered a string. A string must be opened and closed by the same type of quote Multiple strings can be combined using a + sign (concatenation) Multi line strings use triple quotes to span strings down lines Example: Copy multi_line =(\"\"\" this is a test \"\"\") Some characters can't be directly included in a string. For example, double quotes can't be in a string as it would cause the string to end prematurely . Characters like this must be escaped by placing a backslash ( \\ ) before them. Concatenation - the process of adding two strings together. Even if your strings contain numbers, they are still added as strings rather than integers. Adding a string to a number produces an error. String Operations - Strings can also be multiplied by integers. This produces a repeated version of the original string. The order of the string and the integer doesn't matter, but the string typically comes first. Strings can't be multiplied by other strings or floats (even if the float is a whole number). Popular string methods Copy len()- gets length of a string lower()- make entire string lowercase upper()- make entire string uppercase srt()- turn a non string into a string docstring - "},{"title":"Booleans","type":1,"pageTitle":"Python Datatypes","url":"docs/dev/python/python-datatypes#booleans","content":"Variables that are set as either True or False They can be created by comparing values, for instance by using the equal operator == Not equals ( != ) is another method of comparison Greater Than ( > ) and Less Than ( < ) are other methods of comparison Greater Than or Equal To ( >= ) and Less Than or Equal To ( <= ) are other methods of comparison True corresponds to integer value 1 False corresponds to integer value 0 "},{"title":"Python Overview","type":0,"sectionRef":"#","url":"docs/dev/python/python-overview","content":"Python is a high level programming language with applications in numerous areas including web programming, scripting, scientific computing, and artificial intelligence.","keywords":""},{"title":"Python Keywords","type":0,"sectionRef":"#","url":"docs/dev/python/python-keywords","content":"print - prints a message onto the screen","keywords":""},{"title":"Useful Python Methods","type":0,"sectionRef":"#","url":"docs/dev/python/python-useful-methods","content":"","keywords":""},{"title":"Strip Methods","type":1,"pageTitle":"Useful Python Methods","url":"docs/dev/python/python-useful-methods#strip-methods","content":"Copy .rstrip() .lstrip() .strip() Removes whitespace from the right end, left end, or both ends of a string "},{"title":"Ruby Datatypes","type":0,"sectionRef":"#","url":"docs/dev/ruby/ruby-datatypes","content":"","keywords":""},{"title":"Ruby Strings","type":1,"pageTitle":"Ruby Datatypes","url":"docs/dev/ruby/ruby-datatypes#ruby-strings","content":"Strings can use single or double quotes "},{"title":"Python Syntax","type":0,"sectionRef":"#","url":"docs/dev/python/python-syntax","content":"","keywords":""},{"title":"Basic Syntax","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#basic-syntax","content":""},{"title":"Print Function - print()","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#print-function---print","content":"The print function is used to output text, numbers, or other printable information to the console. It takes one or more argument and will output each of the arguments to the console separated by a space. If no arguments are provided, the print function will output a blank line. "},{"title":"Comments","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#comments","content":"A comment is a piece of text within a program that is not executed as part of the program. It can be used to provide additional information to aid in understanding the code. The # character is used to start a comment and it continues until the end of the line. "},{"title":"Variables","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#variables","content":"A variable is used to store data that will be used by the program. This data can be a number, a string, a Boolean, a list, or some other data type. Every variable has a name with can consist of letters, numbers, and the underscore character _ . The equal sign = is used to assign a value to a variable. After the initial assignment is made, the value of a variable can be updated to new values as needed. "},{"title":"Integers","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#integers","content":"Python variables can be assigned different types of data. One supported data type is the integer. An integer is a number which can be written without a fractional part (no decimal). An integer can be a positive number, a negative number, or the number zero so long as there is no decimal portion. The number zero represents an integer value, but the same number written as 0.0 would represent a floating point number. "},{"title":"Floating Point Numbers","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#floating-point-numbers","content":"Python variables can be assigned different types of data. One supported data type is the floating point number. A floating point number is a value which contains a decimal portion. Floating point numbers are used to represent numbers which have fractional quantities. For example, a =3/5 cannot be represented as an integer so the variable a is assigned the floating point value 0.6 "},{"title":"Arithmetic Operations","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#arithmetic-operations","content":"Python supports different types of arithmetic operations. These operations can be performed on literal numbers, variables, or some combination. The primary arithmetic operations are: '+' for addition'-' for subtraction'/' for division'*' for multiplication Modulo Operator - % Python supports an operator to perform the modulo calculation. A modulo calculation returns the remainder of a division between the first and second number. For example: The result of the expression 4 % 2 would result in the value 0, because 4 is evenly divisible by 2 leaving no remainder. The result of the expression 7 % 3 would return 1, because 7 is not evenly divisible by 3, leaving a remainder of 1. "},{"title":"Exponentiation","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#exponentiation","content":"In addition to the basic operations of addition, subtraction, multiplication, and division, Python supports an operator for exponentiation. That operator is written with two asterisks like so . The format for exponentiation in Python is a number or variable followed by the operator followed by a number or variable which represents the power to raise the number. Both the number and the power can be integer or floating point values. "},{"title":"Integer Division","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#integer-division","content":"Python 3 will automatically convert integer numbers to floating-point before performing division. This behavior is changed from Python 2 where integer numbers were not automatically converted. "},{"title":"Plus-Equals Operator","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#plus-equals-operator","content":"The plus-equals operator += provides a convenient way to add a value to an existing variable and assign the new value back to the same variable. In the case where the variable and the value are strings, this operator performs string concatenation instead of addition. The operation is performed in-place, meaning that any other variable which points to the variable being updated will also be updated. "},{"title":"Strings","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#strings","content":"A string is a sequence of characters (letters, numbers, whitespace, or punctuation) enclosed by quotation marks. It can be enclosed using either the double quotation mark \" or the single quotation mark ' . If a string has to be broken into multiple lines, the backslash character \\ can be used to indicate that the string continues on the next line. "},{"title":"String Concatenation","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#string-concatenation","content":"Python supports the joining (concatenation) of strings together using the + operator. The + operator is also used for mathematical addition operations. If the parameters passed to the + operator are strings, then concatenation will be performed. If the parameter passed to + have different types, then Python will report an error condition. Multiple variables or literal strings can be joined together using the + operator. The concatenation process does not add any whitespace between the strings that are joined. "},{"title":"Error Notification","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#error-notification","content":"The Python interpreter will report errors present in your code. For most error cases, the interpreter will display the line of code where the error was detected and place a caret character ^ under the portion of the code where the error was detected. "},{"title":"SyntaxError","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#syntaxerror","content":"A SyntaxError is reported by the Python interpreter when some portion of the code is incorrect. This can include misspelled keywords, missing or too many brackets or parenthesis, incorrect operators, missing or too many quotation marks, or other conditions. "},{"title":"NameError","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#nameerror","content":"A NameError is reported by the Python interpreter when it detects a variable that is unknown. This can occur when a variable that is unknown. This can occur when a variable is used before it has been assigned a value or if a variable name is spelled differently than the point at which it was defined. The Python interpreter will display the line of code where the NameError was detected and indicate which name it found that was not defined. "},{"title":"ZeroDivisionError","type":1,"pageTitle":"Python Syntax","url":"docs/dev/python/python-syntax#zerodivisionerror","content":"A ZeroDivisionError is reported by the Python interpreter when it detects a division operation is being performed at the denominator (bottom number) is 0. In mathematics, dividing a number by zero has no defined value, so Python treats this as an error condition and will report a ZeroDivisionError and display the line of code where the division occurred. This can also happen if a variable is used as the denominator and its value has been set to or changed to zero. Python Basic Syntax Variables Variables are used to deal with data that changes over time. A variable allows you to store a value by assigning it to a name, which can be used to refer to the value later in the program. Variable names can contain only letters, numbers, and underscores. They can start with a letter or an underscore, but not with a number. For instance, you can call a variable message_1 but not 1_message. Spaces are not allowed in variable names, but underscores can be used to separate words in variable names. For example, greeting_message works, but greeting message will cause errors. Avoid using Python keywords and function names as variable names; that is, do not use words that Python has reserved for a particular programmatic purpose, such as the word print. Variable names should be short but descriptive. For example, name is better than n, student_name is better than s_n, and name_length is better than length_of_persons_name. Note: Be careful when using the lowercase letter l and the uppercase letter O because they could be confused with the numbers 1 and 0. To assign a variable, use one equals sign ( = ). Attempting to reference a variable you haven't assigned anything to causes an error. Example: x = 7 Variables can hold integers (whole numbers without decimals), numbers with decimals, boolean values, and strings. Variables can be used to do arithmetic. Variables can be reassigned as many times as you want to change their value. In Python, variables don't have specific types, so you can assign a string to a variable and later assign an integer (or other type) to the same variable. You can use the del statement to remove a variable, which means the reference from the name to the value is deleted, and trying to use the variable causes an error. Variable Names Copy • Only characters that are allowed are letters, numbers, and underscores • Variables cannot start with numbers. • Variables cannot contain spaces. valueError - Python automatically assigns a variable the appropriate datatype Comments Comments in python are expressed using the pound/hashtag symbol ( # ) Simple Operations Python has the capability of carrying out calculations. Addition Subtraction Multiplication / Division // Floor Division (Quotient) % Modulo (Remainder) ( ) Parentheses - Order of Operations In place operators allow you to write code like 'x = x + 3' more concisely, as 'x += 3'. Copy • It's possible with other operators such as -, *, / and % as well. • In place operators are possible with strings as well. New Line \\n You can also use three sets of quotes and new lines created by enter to create new lines. Input and Output Usually, programs take input and process it to produce output. In Python, you can use the print function to produce output. This displays a textual representation of something to the screen. To get input from the user in Python, you can use the intuitively named input function. The input function prompts the user for input and returns (outputs) what they enter as a string. Type Conversion In Python, it's impossible to complete certain operations due to the types involved. For instance, you can't add two strings containing the numbers 2 and 3 together to produce the integer 5, as the operation will be performed on strings, resulting in '23' The solution to this is type conversion. Example: '2' + '3' '23' int('2')+int('3') 5 You can also turn user input (which is a string) to numbers (integers or floats), to allow for the performance of calculations. Example: float(input('Enter a number.'))+float(input('Enter another number.')) In-Place Operators datetime datetime.now() now function contains .year, .month, .day, .hour, .minute, and .second "},{"title":"Ruby Keywords","type":0,"sectionRef":"#","url":"docs/dev/ruby/ruby-keywords","content":"puts - prints content to the terminal and creates a new line after content print - prints content to the terminal chomp - removes the extra line automatically added in Ruby after receiving input from the user","keywords":""},{"title":"Ruby Overview","type":0,"sectionRef":"#","url":"docs/dev/ruby/ruby-overview","content":"Ruby is a powerful, flexible programming language you can use in web/Internet development, to process text, to create games, and as part of the popular Ruby on Rails web framework. Ruby is: High-level, meaning reading and writing Ruby is really easy—it looks a lot like regular EnglishInterpreted, meaning you don’t need a compiler to write and run Ruby. You can write it on your own computer (many are shipped with the Ruby interpreter built in).Object-oriented, meaning it allows users to manipulate data structures called objects in order to build and execute programs. We’ll learn more about objects later, but for now, all you need to know is everything in Ruby is an object.Easy to use. Ruby was designed by Yukihiro Matsumoto (often just called “Matz”) in 1995. Matz set out to design a language that emphasized human needs over those of the computer, which is why Ruby is so easy to pick up.","keywords":""},{"title":"Ruby Useful Methods","type":0,"sectionRef":"#","url":"docs/dev/ruby/ruby-useful-methods","content":"Popular methods that can be used on strings include .length, .reverse, .upcase, and .downcase","keywords":""},{"title":"Ruby Control Structures","type":0,"sectionRef":"#","url":"docs/dev/ruby/ruby-control-structures","content":"","keywords":""},{"title":"Ruby Arrays","type":1,"pageTitle":"Ruby Control Structures","url":"docs/dev/ruby/ruby-control-structures#ruby-arrays","content":"Empty Array Copy myArray = [ ] Array with contents Copy myArray = [1, 2, 3] Defining locations in array with values Copy myArray = [ ] myArray [0] = 1 myArray [1] = 2 myArray [2] = 3 "},{"title":"Ruby Hashes and Symbols","type":1,"pageTitle":"Ruby Control Structures","url":"docs/dev/ruby/ruby-control-structures#ruby-hashes-and-symbols","content":"Hashes are similar to arrays. Basically, the difference is how you get data. Hashes are defined using Hash.new or myHash={ }. Example: Copy myHash={ \"Key\" => \"value\", \"Key1\" => \"value1\", } puts myHash[\"Key\"] Commas in this example separate input Another way to create a Hash is: Copy myHash = Hash.new() myHash[\"Key\"] = \"value\" myHash[\"Key1\"] = \"value1\" puts myHash[\"Key\"] Instead of using a string as the key, you can use symbols. Copy myHash=Hash.new() myHash[:Key]=\"value\" myHash[:Key2]=\"value2\" puts myHash[:Key] When using myHash={} with symbols, symbols are used differently, like this Copy myHash={ Key: \"value\", Key2: \"value2\", } puts myHash[:Key] "},{"title":"Ruby Conditional Statements","type":1,"pageTitle":"Ruby Control Structures","url":"docs/dev/ruby/ruby-control-structures#ruby-conditional-statements","content":"Copy if condition do something end if condition do something else do something else end if condition do something elsif different condition do something else another different thing to be done end "},{"title":"Ruby Unless, Until, and While","type":1,"pageTitle":"Ruby Control Structures","url":"docs/dev/ruby/ruby-control-structures#ruby-unless-until-and-while","content":"The unless statement syntax is almost identical to the if statement. Copy unless condition thing to be done if the condition is false end While statements will keep doing what's in the loop until the condition is false. Copy while condition something to do end Until statements are the opposite of while statements. They will keep doing what is in the loop until the condition is true Copy until condition something to do end "},{"title":"Ruby Methods","type":1,"pageTitle":"Ruby Control Structures","url":"docs/dev/ruby/ruby-control-structures#ruby-methods","content":"Methods are reusable sections of code that perform specific tasks in our program. It makes our code much easier to fix if there are bugs and it helps separate our concerns. Copy def methodName #method code here end The def keyword is the header of the method. This will include the name of the method along with any parameter the method will take (if we want it to have any). The body of the method will contain the code that we want the method to carry out. The method ends with the end keyword. To call a method in Ruby, we just type the method name. Methods with Parameters "},{"title":"docker","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/docker/docker","content":"","keywords":""},{"title":"Docker Concepts","type":1,"pageTitle":"docker","url":"docs/infrastructure/containerization/docker/docker#docker-concepts","content":"Docker is a platform to build, run, and share applications within containers. The use of containers to deploy applications is known as containerization. Containerization is increasingly popular because containers are: Flexible: Even the most complex applications can be containerized.Lightweight: Containers leverage and share the host kernel, making them much more efficient in terms of system resources than virtual machines.Portable: You can build locally, deploy to the cloud, and run anywhere.Loosely coupled: Containers are highly self sufficient and encapsulated, allowing you to replace or upgrade one without disrupting others.Scalable: You can increase and automatically distribute container replicas across a datacenter.Secure: Containers apply aggressive constraints and isolation to processes without any configuration required by the user. "},{"title":"Dockerfile","type":1,"pageTitle":"docker","url":"docs/infrastructure/containerization/docker/docker#dockerfile","content":"Common Dockerfile Commands: FROM - a pre-existing base image. WORKDIR - specifies all subsequent actions should be taken from the directory specified. This should be a directory in your image filesystem. COPY - copy a file from your host to another location. ( . refers to the present location). RUN - runs a command inside your image file filesystem. Common Dockerfile Run Flags: Build Command: docker build --tag <imagename> . Example: docker build --tag bulletinboard:1.0 . docker run --publish <port>:<port> --detach --name <appname> <image> Example: docker run --publish 8000:8080 --detach --name bb bulletinboard:1.0 --publish asks Docker to forward traffic incoming on the host’s port 8000, to the container’s port 8080. Containers have their own private set of ports, so if you want to reach one from the network, you have to forward traffic to it in this way. Otherwise, firewall rules will prevent all network traffic from reaching your container, as a default security posture.--detach asks Docker to run this container in the background.--name specifies a name with which you can refer to your container in subsequent commands "},{"title":"Docker Compose","type":1,"pageTitle":"docker","url":"docs/infrastructure/containerization/docker/docker#docker-compose","content":"Docker Compose is a tool provided by Docker to build and run multi container applications. "},{"title":"Services","type":1,"pageTitle":"docker","url":"docs/infrastructure/containerization/docker/docker#services","content":"Services are the components of your architecture or application. A service is something that would usually have an independent Dockerfile created for. "},{"title":"Docker Commands","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/docker/docker-commands","content":"docker ps - lists containers (default shows only running containers) docker exec -it - container name /bin/bash","keywords":""},{"title":"Ruby Syntax","type":0,"sectionRef":"#","url":"docs/dev/ruby/ruby-syntax","content":"","keywords":""},{"title":"Math","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#math","content":"1 + 2 # => 3 2 7 # => 14 5 / 2 # => 2 (both arguments are whole numbers) 5 / 2.0 # => 2.5 (one argument has a decimal) 1 + (2 3) # => 7 (order of operations) "},{"title":"Strings","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#strings","content":"'single quoted' # => 'single quoted' \"double quoted\" # => \"double quoted\" 'It\\'s alive!' # => It's alive! (\\ is an escape character) '1 + 2 = 5' # => 1 + 2 = 5 x = 'Bob' \"Hi, #{x}\" # => \"Hi, Bob\" 'Hello, #{x}' # => \"Hello, #{x}\" (single quotes don't work for variables) "},{"title":"Interpolation","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#interpolation","content":"When strings have quotes withing quotes, use double quotes \" \" in the outer quotes, and then single quotes ' ' for the inner quotes. "},{"title":"Truths","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#truths","content":"true # => true false # => false nil # => nil 0 # => true (only false and nil are false in Ruby. If it exists in Ruby, even as a 0, it's true) 1 == 1 # => true (== checks for equality) 1 == true # => false (== checks for equality) "},{"title":"Untruths","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#untruths","content":"!true # => false !false # => true !nil # => true 1! = 2 # => true (1 is not equal to 2) 1! = 1 => false (1 is not, not equal to itself) "},{"title":"Convert Truths","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#convert-truths","content":"!!true # => true !!false # => false !!nil # => false !!0 # => true (zero is not false) "},{"title":"Arrays","type":1,"pageTitle":"Ruby Syntax","url":"docs/dev/ruby/ruby-syntax#arrays","content":"x = ['a','b','c'] # => [\"a\",\"b\",\"c\"] x[0] # => \"a\" (zero is first index) x.first # => \"a\" x.last # => \"c\" x + ['d'] # => [\"a\",\"b\",\"c\",\"d\"] x # => [\"a\",\"b\",\"c\"] x = x + ['d'] # => [\"a\",\"b\",\"c\",\"d\"] x = # => [\"a\",\"b\",\"c\",\"d\"] "},{"title":"Resume","type":0,"sectionRef":"#","url":"docs/greg-schullo-resume","content":"","keywords":""},{"title":"Education","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#education","content":"B.B.A - Management of Information Systems - University of Minnesota Duluth - Summer 2014. "},{"title":"Experience","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#experience","content":""},{"title":"UnitedHealth Group","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#unitedhealth-group","content":"Senior Software Engineer | September 2019 to Present# Support the MyUHC Application and drive resiliency best practicesImplement Telemetry Monitoring for various enterprise applicationsSupport Change Data Capture services and drive performance and resiliency efforts Application Performance Management Administrator | August 2014 to September 2019# Design, apply, and support Application Performance Monitoring as a serviceSupport and manage the infrastructure behind performance monitoring applicationsTroubleshoot and diagnose performance issues of infrastructure and various applicationsApply Agile and DevOps practices to our internal working processes and to APM servicesExperience with Ansible, Openshift, Kubernetes, and Docker to deliver a more streamlined Application Performance Management experienceExperience with bash, powershell, and Python scripting languages in automation efforts to better support APM infrastructureNew Relic Service Level Owner "},{"title":"Essentia Health","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#essentia-health","content":"Business Intelligence Analyst Intern | December 2013 to August 2014# Write SQL code to extract specific information from databasesGenerate reports for employees in the Essentia Health systemMaintain and update reports in Clarity Information Services Intern | June 2013 to December 2013# Assist clients with basic technology troubleshooting Assist employees with basic software usageReset passwords and other account settingsDelegated issues to appropriate support teamsAssist customers with Essentia MyHealth "},{"title":"University of Minnesota Duluth","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#university-of-minnesota-duluth","content":"Information Technology Systems and Services Lab Consultant| May 2012 to May 2014# Troubleshoot a wide variety of basic technology related issuesAssist customers with printing and scanning needs Assist customers with installing software and removing virusesSet up email on mobile devices for students and staffCreate tickets to document issues and repairs "},{"title":"Duluth Entertainment Convention Center","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#duluth-entertainment-convention-center","content":"Technology Services| May 2012 to June 2013# Assist customers with wireless internet and technology troubleshooting Set up sound, video, and lighting for events/showsProvided power for customers and companies for events "},{"title":"Awards and Acknowledgements","type":1,"pageTitle":"Resume","url":"docs/greg-schullo-resume#awards-and-acknowledgements","content":"Boy Scouts of America Eagle Scout AwardNational Agency ClearanceOptum Site Reliability Engineering Team Embedded Program "},{"title":"Installing Kubectl","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/installing-kubectl","content":"","keywords":""},{"title":"Prerequisites","type":1,"pageTitle":"Installing Kubectl","url":"docs/infrastructure/containerization/kubernetes/installing-kubectl#prerequisites","content":"Ensure virtualization is supported on your macOS, open terminal and run: sysctl -a | grep -E --color 'machdep.cpu.features|VMX' If you see VMX in the output (should be colored), the VT-x feature is enabled in your machine. "},{"title":"Installing Kubectl","type":1,"pageTitle":"Installing Kubectl","url":"docs/infrastructure/containerization/kubernetes/installing-kubectl#installing-kubectl","content":"Install the latest version of kubectlcurl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl\"Make the kubectl binary executablechmod +x ./kubectlMove the binary in to your PATHsudo mv ./kubectl /usr/local/bin/kubectlTest to ensure the version you installed is up to date.kubectl version --client Alternatively, install kubectl with Homebrewbrew install kubectl "},{"title":"Docker","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/docker/docker-index","content":"","keywords":""},{"title":"Docker","type":1,"pageTitle":"Docker","url":"docs/infrastructure/containerization/docker/docker-index#docker","content":"Docker for the Virtualization Admin "},{"title":"Installing Minikube","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/installing-minikube","content":"","keywords":""},{"title":"Prerequisites","type":1,"pageTitle":"Installing Minikube","url":"docs/infrastructure/containerization/kubernetes/installing-minikube#prerequisites","content":"kubectl A Hypervisor such as Hyperkit, VirtualBox, or VMware fusion. "},{"title":"Install Minikube on MacOS","type":1,"pageTitle":"Installing Minikube","url":"docs/infrastructure/containerization/kubernetes/installing-minikube#install-minikube-on-macos","content":"Install the latest version of minikube Copy curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64 \\ && chmod +x minikube Add the minikube executable to your path sudo mv minikube /usr/local/bin Alternatively, install minikube with Homebrew brew install minikube Confirm successful installation of the Hypervisor and minikube.minikube start --driver=virtualbox Reference the Kubernetes documentation for a full list of VM drivers if not using VirtualBox. "},{"title":"Successful Output","type":1,"pageTitle":"Installing Minikube","url":"docs/infrastructure/containerization/kubernetes/installing-minikube#successful-output","content":"Copy minikube start --driver=virtualbox 😄 minikube v1.9.0 on Darwin 10.15.3 ✨ Using the virtualbox driver based on user configuration 💿 Downloading VM boot image ... > minikube-v1.9.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s > minikube-v1.9.0.iso: 174.93 MiB / 174.93 MiB [-] 100.00% 9.84 MiB p/s 18s 💾 Downloading Kubernetes v1.18.0 preload ... > preloaded-images-k8s-v2-v1.18.0-docker-overlay2-amd64.tar.lz4: 542.91 MiB 🔥 Creating virtualbox VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... 🐳 Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... 🌟 Enabling addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \"minikube\" To check the status of the cluster, run: minikube status Output should be similar to the following: m01 host: Running kubelet: Running apiserver: Running kubeconfig: Configured minikube delete minikube start --cpus 4 --memory 6144 "},{"title":"Python Control Structures","type":0,"sectionRef":"#","url":"docs/dev/python/python-control-structures","content":"","keywords":""},{"title":"if Statements","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#if-statements","content":"In Python, if statements are used to run code if a certain condition holds. If an expression evaluates to True, some statements are carried out. If an expression evaluates to False, they aren't carried out. An if statement looks like: Copy if<expression>: statements Python uses indentation (white space at the beginning of a line) to delimit blocks of code. Other languages, such as C, use curly braces to accomplish this, but in Python indentation is mandatory; programs won't work without it. To perform more complex checks, if statements can be nested, one inside of the other. This means that the inner if statement is part of the outer one. This is one way to see whether multiple conditions are satisfied. "},{"title":"else Statements","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#else-statements","content":"In Python, an else statement follows an if statement and contains code that is called when the if statement evaluates to False. As with if statements, the code inside the block should be indented. An example of an else statement would be: Copy if<expression>: statement(s) else: statement(s) You can chain if and else statements to determine which option in a series of possibilities is True. "},{"title":"elif Statements","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#elif-statements","content":"elif (short for else if) statement is a shortcut to use when chaining if and else statements. A series of if elif statements can have a final else block, which is called if none of the if or elif expressions is True. Example: Copy num =7 if num ==5: print(\"Number is 5\") elif num ==11: print(\"Number is 11\") elif num ==7: print(\"Number is 7\") else: print(\"Number isn't 5, 11 or 7\") "},{"title":"Boolean Logic","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#boolean-logic","content":"Boolean logic is used to make more complicated conditions for if statements that rely on more than one condition. Python's boolean operators are and, or, and not. The and operator takes two arguments and evaluates as True only of both arguments are True. Otherwise it evaluates to False. The or operator also takes two arguments and evaluates as True if either (or both) of its arguments are True. Otherwise it evaluates to False. The not operator only takes one argument and inverts it. The result of not True is False and the result of not False is true. "},{"title":"Operator Precedence","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#operator-precedence","content":"Operator precedence is an important concept in programming. It is an extension of the mathematical idea of order of operations (multiplication being performed before addition, etc.) to include other operators, such as those in Boolean logic. Example: Copy False==FalseorTrue True False==(FalseorTrue) False (False==False)orTrue True "},{"title":"Lists","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#lists","content":"Lists are another type of object in Python. They are used to store an indexed list of items. A list is created using square brackets with commas separating items. The certain item in the list can be accessed by using its index in square brackets. Example: Copy words =[\"Hello\",\"world\",\"!\"] print(words[0]) print(words[1]) print(words[2]) The first list item's index is 0, rather than 1, as might be expected. Zeros matter in programming too! An empty list is created with an empty pair of square brackets. Example: Copy empty_list =[] print(empty_list) Most of the time, a comma won't follow the last item in a list. However, it is perfectly valid to place one there, and it is encouraged in some cases. Typically, a list will contain items of a single item type, but it is also possible to include several different types. Lists can also be nested within other lists. Example: Copy number =3 things =[\"string\",0,[1,2, number],4.56] print(things[1]) print(things[2]) print(things[2][2]) Lists of lists are often used to represent 2D grids, as Python lacks the multidimensional arrays that would be used for this in other languages. Indexing out of the bounds of possible list values causes an IndexError. Some types, such as strings, can be indexed like lists. Indexing strings behaves as though you are indexing a list containing each character in the string. For other types, such as integers, indexing them isn't possible, and it causes a TypeError. "},{"title":"List Operations","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#list-operations","content":"The item at a certain index in a list can be reassigned. Example: Copy nums =[7,7,7,7,7] nums[2]=5 print(nums) Lists can be added and multiplied in the same way as strings. Example: Copy nums =[1,2,3] print(nums +[4,5,6]) print(nums *3) Result: Copy [1,2,3,4,5,6] [1,2,3,1,2,3,1,2,3] Lists and strings are similar in many ways - strings can be thought of as lists of characters that can't be changed. To check if an item is in a list, the in operator can be used. It returns True if the item occurs one or more times in the list, and False if it doesn't. Example: Copy words =[\"spam\",\"egg\",\"spam\",\"sausage\"] print(\"spam\"in words) print(\"egg\"in words) print(\"tomato\"in words) Result: Copy True True False The in operator is also used to determine whether or not a string is a substring of another string. To check if an item is not in a list, you can use the not operator in one of the following ways: Example: Copy nums =[1,2,3] print(not4in nums) print(4notin nums) print(not3in nums) print(3notin nums) Result: Copy True True False False "},{"title":"List Functions","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#list-functions","content":"Another way of altering lists is using the append method. This adds an item to the end of an existing list. Example: Copy nums =[1,2,3] nums.append(4) print(nums) Result: [1, 2, 3, 4] The dot before append is there because it is a method of the list class. To get the number of items in a list (or the length of a list), you can use the len function. Example: Copy nums =[1,3,5,2,4] print(len(nums)) Result: Copy 5 Unlike append, len is a normal function, rather than a method. This means it is written before the list it is being called on, without a dot. The insert method is similar to append, except that it allows you to insert a new item at any position in the list, as opposed to just at the end. Example: Copy words =[\"Python\",\"fun\"] index =1 words.insert(index,\"is\") print(words) Result: Copy ['Python','is','fun'] The index method finds the first occurrence of a list item and returns its index. If the item isn't in the list, it raises a ValueError. Example: Copy letters =['p','q','r','s','p','u'] print(letters.index('r')) print(letters.index('p')) print(letters.index('z')) Result: 2 0 ValueError: 'z' is not in list There are a few more useful functions and methods for lists. max(list): Returns the list item with the maximum value min(list): Returns the list item with minimum value list.count(obj): Returns a count of how many times an item occurs in a list list.remove(obj): Removes an object from a list list.reverse(): Reverses objects in a list "},{"title":"Range","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#range","content":"The range function creates a sequential list of numbers. The code below generates a list containing all of the integers, up to 10. Example: Copy numbers =list(range(10)) print(numbers) Result: Copy [0,1,2,3,4,5,6,7,8,9] The call to list is necessary because range by itself creates a range object, and this must be converted to a list if you want to use it as one. If range is called with one argument, it produces an object with values from 0 to that argument (not including the argument). If it is called with two arguments, it produces values from the first to the second. Example: Copy numbers =list(range(3,8)) print(numbers) print(range(20)==range(0,20)) Result: Copy [3,4,5,6,7] True range can have a third argument, which determines the interval of the sequence produced. This third argument must be an integer. Example: Copy numbers =list(range(5,20,2)) print(numbers) Result: Copy [5,7,9,11,13,15,17,19] "},{"title":"For Loops","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#for-loops","content":"Sometimes, you need to perform code on each item in a list. This is called iteration, and it can be accomplished with a while loop and a counter variable. Example: Copy words =[\"hello\",\"world\",\"spam\",\"eggs\"] counter =0 max_index =len(words)-1 while counter <= max_index: word = words[counter] print(word +\"!\") counter = counter +1 Result: Copy hello! world! spam! eggs! The example above iterates through all items in the list, accesses them using their indices, and prints them with exclamation marks. Iterating through a list using a while loop requires quite a lot of code, so Python provides the for loop as a shortcut that accomplishes the same thing. A for loop. The same code from the previous example can be written with a for loop: Copy words =[\"hello\",\"world\",\"spam\",\"eggs\"] for word in words: print(word +\"!\") Result: Copy hello! world! spam! eggs! The for loop in Python is like the foreach loop in other languages. The for loop is commonly used to repeat some code a certain number of times. This is done by combining for loops with range objects. Example: Copy for i inrange(5): print(\"hello\") Result: Copy hello hello hello hello hello You don't need to call list on the range object when it is used in a for loop, because it isn't being indexed, so a list isn't required. "},{"title":"While Loops","type":1,"pageTitle":"Python Control Structures","url":"docs/dev/python/python-control-structures#while-loops","content":"A while statement is similar to an if statement, except it can be run more than once. The statements inside it are repeatedly executed as long as the condition holds. Once it evaluates to False, the next section of code is executed. Example: Copy i =1 while i <=5: print(i) i = i +1 print(\"Finished\") The code in the body of a while loop is executed repeatedly. This is called iteration. infinite loop - a special kind of while loop that never stops running. Its condition always remains True. Example of an infinite loop: Copy while1==1: print(\"In the loop\") You can stop the program's execution by using the Ctrl-C shortcut or by closing the program. break To end a while loop prematurely the break statement can be used. When encountered inside a loop, the break statement causes the loop to finish immediately. Example: Copy i =0 while1==1: print(i) i = i +1 if i >=5: print(\"Breaking\") break print(\"Finished\") Using the break statement outside of a loop causes an error. continue Another statement that can be used within loops is continue. Unlike break, continue jumps back to the top of the loop, rather than stopping it. Example: Copy i =0 whileTrue: i = i +1 if i ==2: print(\"Skipping 2\") continue if i ==5: print(\"Breaking\") break print(i) print(\"Finished\") the continue statement stops the current iteration and continues with the next one. Using the continue statement outside of a loop causes an error. "},{"title":"Kubernetes","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/kubernetes-index","content":"Placeholder for new Kubernetes documentation. https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/","keywords":""},{"title":"Kubernetes Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/kubernetes-overview","content":"Kubernetes notes Kubernetes (k8s) - containerization orchestration tool makes it easy to manage and automate containerized application infrastructure Container - wraps software in independent, portable packages, making it easy to quickly run software in a variety of environments kubeadm - tool that automates a lot of setting up a kubernetes cluster kubelet - essential component of kubernetes. Essentially an agent on each node that acts as a middleman to the Kubernetes API and the docker container runtime. kubectl - command line tool used to interact with the k8s cluster once the cluster is up and running. kubectl version kubectl get nodes kubectl get pods kubectl get deployments kubectl get events kubectl get services kubectl config view Create a Kubernetes cluster Deploy an application Explore your app Expose your app publicly Scale up your app Update your app Minikube minikube start minikube dashboard minikube stop minikube delete Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Kubernetes Documentation https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-curl Install the latest kubectl binary release using curl: curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl Default configuration settings are located at: ~/.kube/config Check the kubectl cluster state: kubectl cluster-info","keywords":""},{"title":"Kubectl Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/kubectl-overview","content":"Kubernetes Kubectl Overview Documentation","keywords":""},{"title":"Minikube Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/minikube-overview","content":"Minikube is a tool that runs a single-node Kubernetes cluster in a virtual machine on your personal computer. Minikube supports the following Kubernetes features: DNSNodePortsConfigMaps and SecretsDashboardsContainer Runtime: Docker, CRI-O, and containerdEnabling CNI (Container Network Interface)Ingress","keywords":""},{"title":"gRPC","type":0,"sectionRef":"#","url":"docs/infrastructure/grpc","content":"gRPC Documentation RPC - Remote Procedure Call An RPC is when a computer program causes a procedure to execute in a different address space which is coded as if it were a normal procedure call without the programmer explicitly coding the details for the remote interaction. gRPC is a RPC initially developed at Google. It uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication, bidirectional streaming and flow control, blocking or non-blocking bindings, and cancellation and timeouts. It generates cross-platform client and server bindings for many languages. Most common usage scenarios include connecting services in microservices style architecture and connect mobile devices, browser clients to back-end services.","keywords":""},{"title":"Docker Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/docker/docker-overview","content":"","keywords":""},{"title":"Docker Concepts","type":1,"pageTitle":"Docker Overview","url":"docs/infrastructure/containerization/docker/docker-overview#docker-concepts","content":"Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. The use of Linux containers to deploy applications is called containerization. Containers are not new, but their use for easily deploying applications is. Containerization is increasingly popular because containers are: Flexible - even the most complex applications can be containerized Lightweight - containers leverage and share the host kernel Interchangeable - deploy updates and upgrades on the fly Portable - build locally, deploy to the cloud, and run anywhere Scalable - increase and automatically distribute container replicas Stackable - stack services vertically and on the fly  "},{"title":"Images and Containers","type":1,"pageTitle":"Docker Overview","url":"docs/infrastructure/containerization/docker/docker-overview#images-and-containers","content":"A container is launched by a running image. An image is an executable package that includes everything needed to run an application. The code, a runtime, libraries, environment variables, and configuration files. A container is a runtime instance of an image - what the image becomes in memory when executed (that is, an image with state or a user process). You can see a list of running containers with the command, docker ps, just as you would in Linux. "},{"title":"Containers and Virtual Machines","type":1,"pageTitle":"Docker Overview","url":"docs/infrastructure/containerization/docker/docker-overview#containers-and-virtual-machines","content":"A container runs natively on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight. By contrast, a virtual machine (VM) runs a full blown \"guest\" operating system with virtual access to host resources through a hypervisor. In general, VMs provide an environment with more resources than most applications need. "},{"title":"Infrastructure Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/infrastructure-overview","content":"Infrastructure Documentation Landing Page","keywords":""},{"title":"Networking Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/networking/networking-overview","content":"Placeholder for future Networking Overview documentation.","keywords":""},{"title":"OSI Model","type":0,"sectionRef":"#","url":"docs/infrastructure/osi-model","content":"Application Layer - Responsibility of the host. Services layer. Presentation Layer - Responsibility of the host. Services layer. Session Layer - Responsibility of the host. Middleware layer. Transport Layer - Responsibility of the host. Responsibility of the network. Middleware layer. Network Layer - Responsibility of the network. The operating systems layer. Data Link Layer - Responsibility of the network. The operating systems layer. Physical Layer - Responsibility of the network. The hardware layer. Physical Layer - Transmits electrical signals to the hosts. The hardware (Network Interface Card or NIC, switch port, etc) is responsible for this. Data Link Layer - Defines the rules and procedures for accessing the physical layer. Defines how hosts are identified on the network and how the network mediums are accessed. It also specifies how to verify data received from the physical layer. Network Layer - This enables the routing of the data. It specifies how to recognize the address of neighboring nodes and routers, for example. It basically specifies how to determine the next network point a packet should be forwarded toward its destination. The internet protocol operates at this layer. Transport Layer - On the sending host, the transport layer receives information from the upper layers on the OSI model. It then divides that data into small, transmittable chunks called packets. On the receiving host, the transport layer resembles those packets from the lower layers of the OSI model. The transport layer also provides error checking mechanisms that ensure the data arrives at the destination host in tact. The transmission control protocol or TCP, and the User Datagram Protocol or UDP, all operate at layer 4. Session Layer - Responsible for establishing and maintaining connections between source and destination network hosts. These connections are referred to as sessions. Presentation Layer - Responsible for insuring the information passing through all of the OSI layers is formatted correctly for the application on the destination system. Application Layer - Responsible for providing applications with a way that actually accesses the network.","keywords":""},{"title":"Openshift","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/openshift-index","content":"Placeholder for new Openshift documentation.","keywords":""},{"title":"Networking Basics","type":0,"sectionRef":"#","url":"docs/infrastructure/networking/networking-basics","content":"","keywords":""},{"title":"Internet, Networks, Routers","type":1,"pageTitle":"Networking Basics","url":"docs/infrastructure/networking/networking-basics#internet-networks-routers","content":"You can configure a Linux System to fulfill a variety of roles. Domain ControllerDatabase ServerDHCP ServerWeb ServerEmail ServerFile and Print ServerPacket-filtering, stateful, or even application level firewallsProxy ServerContent Filtration ServerRouter The entire picture of the IP protocol and the transmission control protocol: TCP/IP TCP ensures data exchanged between two network hosts is exchanged reliably. Typical upper-layer applications that use TCP are: Web Servers, Email Servers, and FTP Servers. UCP - User Datagram Protocol UDP is a connectionless protocol UDP packets are sent unacknowledged Applications that can tolerate less reliability can use UDP Typical upper-layer applications that use UDP are: streaming audio and VoIP (Voice over IP)  ICMP - Internet Control Message Protocol ICMP is for testing and verifying network communications between hosts. Ports TCP and UDP both provide ports for upper layer protocols. Port numbers range from 0 up to 65,536 for each individual IP address. Well known Ports range from 0 - 1023. Registered Ports: An organization can program their own network service and then apply for a registered port number to be assigned to it. Range 1024 - 49,151. Dynamic Ports: Dynamic ports are available for use by any network service range 49,152 - 65,536. NAT - Network Address Translation We use a NAT router that creates a \"private\" IP address space on our LAN with multiple devices and computers but the outside, \"the router,\" has only 1 \"public\" IP address in which internal network gets \"translated\" through. Private IP Ranges: 10.0.0.0 - 10.255.255.255 (Class A) 172.16.0.0 - 172.31.255.255 (Class B) 192.168.0.0 - 192.168.255.255 (Class C) Subnet Mask Network vs Node with our IP Address Anything with a 255 in the octet in a subnet mask is used to identify the network portion of an IP address. Class A subnet mask First octet must be between 1 and 126First octet is the network addressLast three octets of the address is the node addressDefault subnet mask class A 255.0.0.0Class A allows 126 total possible networksClass A can offer 16.7 million possible nodes addresses. Class B subnet mask First octet must be between 128 and 192Frist two octets are the network addressLast two octets of the address is the node addressDefault subnet mask class B 255.255.0.0 Class B allows 16,384 total possible networks Class B can offer 65,534 possible node addresses  Class C subnet mask First octet must be between 192 and 223 First three octets are the network address Last octet is the node address Default subnet mask class C 255.255.255.0 Class C allows 2,097,152 total possible networks Class C can offer 254 possible node addresses (limited host)  Subnet mask can be typed in shorthand: 192.168.1.1/24 - indicates that 24 bits are used for the subnet mask. 192.168.1.1 255.255.255.0 Partial Subnet You do not have to use the default subnet mask Example: A class A address could use only part of an octet for the address such as 255.255.252.0 For PC3 to speak to PC2 and PC1, we need another router because it is a different subnet. Without this route, PC3 (using subnet mask of 255.255.252.0) can not communicate with the other host using 255.255.255.0 "},{"title":"Domain Name Service and the Default Gateway (Network Router)","type":1,"pageTitle":"Networking Basics","url":"docs/infrastructure/networking/networking-basics#domain-name-service-and-the-default-gateway-network-router","content":"When looking up a URL or domain names, we have to translate that domain name in to the IP address of a web server. www.linuxacademy.com to 54.165.61.14 and 54.164.230.15 DNS server translates domain names into IP addresses.  "},{"title":"Network Configuration","type":1,"pageTitle":"Networking Basics","url":"docs/infrastructure/networking/networking-basics#network-configuration","content":"ping - testing of connectivity of a remote network device dig - allows us to look up IP addresses for DNS names netstat - list network connections, routing info, NIC info route - current route / net settings traceroute - traces the route a packet takes ifconfig - current network settings ipaddr - current IP address and network settings /etc/resolv.conf this is where our DNS server information is stored on Linux. It is recommended that this file is not edited manually.  "},{"title":"Alerting Strategy","type":0,"sectionRef":"#","url":"docs/monitor-alert/alert/alerting-strategy","content":"","keywords":""},{"title":"Evaluate and Document Current Alerting Strategy","type":1,"pageTitle":"Alerting Strategy","url":"docs/monitor-alert/alert/alerting-strategy#evaluate-and-document-current-alerting-strategy","content":"Analyze alerts that are configured in production and have a conversation with the support team to determine the following: What tools are being used to generate alerts? (SiteScope, Alertmanager, Splunk, etc) What metrics are being leveraged to generate alerts? (CPU Utilization, Response Time, HTTP 5xx, RER, etc) How are alerts distributed? (OpsBridge, PagerDuty, Email, SMS, ChatOps, etc) Who do the alerts go to? (Distribution List, Individuals, On-Call Rotation, etc) How are alerts classified (P1/P2, Alert/Ticket, Sev1, etc) How are alert thresholds determined? Are alert thresholds static or dynamic? How many alerts fire per week? What percentage of alerts are actionable? If able to determine, what is the average Mean Time to Detect (MTTD)?  Completion Criteria: Produce a document with these findings articulated. "},{"title":"Review and Improve Alerting Strategy","type":1,"pageTitle":"Alerting Strategy","url":"docs/monitor-alert/alert/alerting-strategy#review-and-improve-alerting-strategy","content":"Produce an Alerting Strategy document that describes the strategy and approach the team will take when crafting alerts in each monitoring system. If necessary implement changes to the strategy that enable the following: All alerts that page a human should be actionableAlerts that are not actionable should be converted to either tickets or loggingThresholds are set for alerts, and should be revisitedDynamic thresholds for alerts should be utilized when appropriateAll alerts should be set with the SLA/SLE in mindThe team should construct alerts in service of the shortest reasonable MTTD (Strive for <10 min)Alerting strategy should minimize noise (alert storms / non-actionable alerts)Who will alerts go to? Does this change under certain circumstances (Priority, service, etc)?All members of the team have reviewed the alerting strategy document and have committed to following it in the future, or updating it as a team Completion Criteria: A document that describes the teams' Alerting Strategy is produced and agreed upon by the team. "},{"title":"Validate and Implement Advanced Alerting","type":1,"pageTitle":"Alerting Strategy","url":"docs/monitor-alert/alert/alerting-strategy#validate-and-implement-advanced-alerting","content":"Enable advanced alerting techniques. This is highly specific to the application, thus the SRE will need to generate and document specific recommendations for the team using the below items as guidelines: The team leverages a persistent chat tool like Flowdock or Slack or Cisco SparkThe team understands the core concepts of ChatOpsThe team is leveraging bots in their chat tool to pipe in alerts and is able to take action like acknowledging or escalating themAchieve <10 min MTTDThe team is using dynamic thresholds for most alertsAlerts configured on absence conditions (0 orders placed, etc)Minimal false positive alertsMajority of alerts are actionableAlerts are consistently tuned as the applications changes over timeConfidence in alerts is high enough to enable automated actions Completion Criteria: Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Kubernetes Introduction","type":0,"sectionRef":"#","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction","content":"","keywords":""},{"title":"What is Kubernetes?","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#what-is-kubernetes","content":"Open source platform Automation of application containers DeploymentScalingOperating Started by Google in 2014 Cloud Native Computing Foundation Abbreviated as K8S Kubernetes is of Greek origin and stands for Pilot or Governor Deploy applications using containers Virtualize operating system, not hardware More efficient use of resourcesSmall and fast (lightweight)One app per container Benefits of K8S Portable (private cloud, public cloud, hybrid, or multi-cloud) Extensible (modular and plugable) Self-healing (fault tolerant) auto placement, auto restart auto replication and auto Scaling Quickly and efficiently respond to demand Deploy applications quicklyRoll out new features easilyScale easilyUse only required resources The goal of K8S is to make running applications in the cloud simpler Container centric infrastructure. Move away from host centric infrastructure. This results in taking full advantage of benefits fundamental to containers. Deployments are simpler, more portable, more predictable, and easier to manage. k8S runs on top of linux but is platform agnostic. That means it can be run on top of bare metal, on servers, on desktops or laptops, on VMs, on cloud providers, or on OpenStack. "},{"title":"Why Use Kubernetes?","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#why-use-kubernetes","content":"Running apps in the cloud is simpler Container-centric infrastructure Away from host-centric infrastructure These items result in: Taking full advantage of benefits fundamental to containersDeployments SimplerPortablePredictable Easier Management "},{"title":"Introduction to Docker","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#introduction-to-docker","content":"Software container platform Virtualization at the operating system level Run and manage apps Isolated containersDeployed as single packages Software Delivery Roll out New FeaturesUpdates Built in security capabilitiesConsistencyLinux and Windows based apps "},{"title":"Docker for Developers","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#docker-for-developers","content":"Portability Focus on writing code. Doesn't matter what system it will run on.Collaboration Sharing apps Debugging Consistent, known operating environment "},{"title":"Docker for System Admins","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#docker-for-system-admins","content":"Software Delivers Deploy Bug fixesUpdatesNew features Scale Apps Automatically BuildShipTestDeploy "},{"title":"Docker for Enterprise","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#docker-for-enterprise","content":"Application Platform CloudMulti CloudHybrid Cloud On-premises Architectures Traditional Digital Transformation Cost reduction, security improved, portability Microservices "},{"title":"Docker Swarms","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#docker-swarms","content":"Native Clustering Docker Engines in a Swarm are said to run in Swarm Mode A swarm consists of manager nodes and worker nodes Worker nodes to the work and execute tasks (containers)Master node is the node that controls the worker nodes "},{"title":"Kubernetes on a Local Machine","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#kubernetes-on-a-local-machine","content":"Learning Environment Minikube: Greatly simplifies process of getting kubernetes up and running on a local machine. "},{"title":"Kubectl Command Line Interface","type":1,"pageTitle":"Kubernetes Introduction","url":"docs/infrastructure/containerization/kubernetes/kubernetes-introduction#kubectl-command-line-interface","content":"kubectl [command][TYPE] [NAME][flags] "},{"title":"Elasticsearch Fundamentals","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/elasticsearch/elasticsearch-fundamentals","content":"What is Elasticsearch? How do you deploy a cluster? How do you install, configure, and manage that cluster? How do you manage that cluster? How do you use your deployment to develop a powerful search and analytic solution? How do you troubleshoot issues you may encounter during set up? QueriesAnalyzersText Analysis and MappingsNodes and ShardsAggregations","keywords":""},{"title":"Elasticsearch on Minikube","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/elasticsearch/elasticsearch-on-minikube","content":"kubectl apply -f https://download.elastic.co/downloads/ec minikube dashboard kubectl run elasticsearch --image=docker.elastic.co/elasticsearch/elasticsearch:6.2.1 --env=\"discovery.type=single-node\" --port=9200 kubectl apply -f ./quickstart-es.yml# kubectl get elasticsearch kubectl apply -f ./quickstart-kibana.yml kubectl get kibana kubectl port-forward service/quickstart-kibana 5601 echo $(kubectl get secret quickstart-elastic-user -o=json)","keywords":""},{"title":"Elasticsearch Overview","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/elasticsearch/elasticsearch-overview","content":"Elasticsearch is a real-time, distributed storage, search, and analytic engine. It can be used for multiple purposes, but it excels in indexing streams of semi-structured data, such as logs or decoded network packets. Elasticsearch is a distributed document store. Elasticsearch stores complex data structures as serialized JSON documents, rather than in rows and columnar data. When a document is stored, it is indexed and fully searchable in near real-time due to its use of the inverted index data structure.","keywords":""},{"title":"Install Elasticsearch","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/elasticsearch/install-elasticsearch","content":"","keywords":""},{"title":"Install Elasticsearch 7.6.1 on Mac","type":1,"pageTitle":"Install Elasticsearch","url":"docs/monitor-alert/elastic/elasticsearch/install-elasticsearch#install-elasticsearch-761-on-mac","content":"Copy curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.1-darwin-x86_64.tar.gz tar -xzvf elasticsearch-7.6.1-darwin-x86_64.tar.gz cd elasticsearch-7.6.1 ./bin/elasticsearch "},{"title":"Start Elasticsearch","type":1,"pageTitle":"Install Elasticsearch","url":"docs/monitor-alert/elastic/elasticsearch/install-elasticsearch#start-elasticsearch","content":"In a terminal session in the directory where Elasticsearch is installed, run: bin\\elasticsearch.bat "},{"title":"Ensure Elasticsearch is Running","type":1,"pageTitle":"Install Elasticsearch","url":"docs/monitor-alert/elastic/elasticsearch/install-elasticsearch#ensure-elasticsearch-is-running","content":"To test that the Elasticsearch daemon is up and running, try sending an HTTP GET request on port 9200. Example: curl http://127.0.0.1:9200 "},{"title":"Install Kibana","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/kibana/install-kibana","content":"","keywords":""},{"title":"Install Kibana 7.6.1 on Mac","type":1,"pageTitle":"Install Kibana","url":"docs/monitor-alert/elastic/kibana/install-kibana#install-kibana-761-on-mac","content":"Copy curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-7.6.1-darwin-x86_64.tar.gz tar xzvf kibana-7.6.1-darwin-x86_64.tar.gz cd kibana-7.6.1-darwin-x86_64/ ./bin/kibana "},{"title":"Start Kibana","type":1,"pageTitle":"Install Kibana","url":"docs/monitor-alert/elastic/kibana/install-kibana#start-kibana","content":"In a terminal session in the directory where Kibana is installed, run: bin\\kibana.bat "},{"title":"Ensure Kibana is Running","type":1,"pageTitle":"Install Kibana","url":"docs/monitor-alert/elastic/kibana/install-kibana#ensure-kibana-is-running","content":"To launch the Kibana web interface, navigate a web browser to localhost, port 5601. Example: http://127.0.0.1:5601 "},{"title":"Kibana Overview","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/kibana/kibana-overview","content":"Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. Use Kibana to search, view, and interact with data stored in Elasticsearch indices. The Elastic vendor recommends Kibana and Elasticsearch be installed on the same server, but it is not required. If Elasticsearch and Kibana are installed on different servers, change the URL (IP:PORT) of the Elasticsearch server in the Kibana configuration file, kibana.yml, before starting Kibana. ","keywords":""},{"title":"Grafana","type":0,"sectionRef":"#","url":"docs/monitor-alert/grafana-index","content":"Placeholder for new Grafana documentation.","keywords":""},{"title":"Jaeger","type":0,"sectionRef":"#","url":"docs/monitor-alert/jaeger/jaeger-index","content":"Placeholder for new Jaeger documentation.","keywords":""},{"title":"Elastic APM Overview","type":0,"sectionRef":"#","url":"docs/monitor-alert/elastic/elastic-apm-overview","content":"Elastic APM agents capture spans, transactions, errors, and metrics (also known as Events) within their instrumented applications. Spans - https://www.elastic.co/guide/en/apm/get-started/7.0/transaction-spans.html Spans contain information about a specific code path that has been executed. They measure from start to end of an activity and they can have a parent/child relationship with other spans. Agents automatically instrument a variety of libraries to capture these spans from within the application. In addition, you can use the Agent API for ad hoc instrumentation of specific code paths. A span contains: Copy • a transaction.id attribute that refers to their parent transaction. • a parent.id attribute that refers to their parent span, or their transaction. • start time and duration • name • type • stack trace (optional) Spans are stored in span indices. These indices are separate from transaction indices by default. Transactions - https://www.elastic.co/guide/en/apm/get-started/7.0/transactions.html Transactions are a special kind of span that have additional attributes associated with them. They describe an event captured by an Elastic APM agent instrumenting a service. You can think of transactions as the highest level of work you're measuring within a service. As an example, a transaction may be a: Copy • Request to your server • Batch job • Background job • Custom transaction type Agents decide whether to sample transactions or not and provide settings to control sampling behavior. If sampled, the spans of a transaction are sent and stored as separate documents. Within one transaction there can be 0, 1, or many spans captured. A transaction contains: Copy • the timestamp of the event • A unique id, type, and name • data about the environment in which the event is recorded: ○ Service - environment, framework, language, etc ○ Host - architecture, hostname, IP, etc ○ Process - args, PID, PPID, etc ○ URL - full, domain, port, query, etc ○ User - (if supplied) email, ID, username, etc • other relevant information depending on the agent. An example would be, the JavaScript RUM agent captures transaction marks, which are points in time relative to the start of the transaction with some label. Transactions are stored in transaction indices. Errors - https://www.elastic.co/guide/en/apm/get-started/7.0/errors.html An error event contains at least information about the original exception that occurred or about a log created when the exception occurred. For simplicity, errors are represented by a unique ID. An error contains: Copy • Both the captured exception and the captured log of an error can contain a stack trace, which is helpful for debugging • The culprit of an error indicates where it originated • An error may relate to the transaction during which it happened, via the transaction.id • Data aboht Metrics -","keywords":""},{"title":"Monitoring Pattern","type":0,"sectionRef":"#","url":"docs/monitor-alert/monitor/monitoring-pattern","content":"","keywords":""},{"title":"Context","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#context","content":"This document provides monitoring best practices and guidance for application teams architecting for resiliency. "},{"title":"Problem","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#problem","content":"Complex infrastructure and customer experiences are often difficult to instrument and measure holistically. When monitoring with application resiliency as the focus, it is vital to instrument monitoring so a thorough understanding of application infrastructure and transaction flow may be achieved. In other words, it is necessary to first understand where and why problems will and could occur before any type of resiliency practices may take place. "},{"title":"Solution","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#solution","content":"There are many monitoring solutions available and an appropriate solution or solutions will vary from application to application. "},{"title":"Monitoring","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#monitoring","content":"First we must define monitoring in a technology focus. Google SRE defines monitoring as collecting, processing, aggregating, and displaying real-time quantitative data about a system, over a period of time, to observe and check the progress or quality of the system. "},{"title":"Goal of Monitoring","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#goal-of-monitoring","content":"The main goal of monitoring is to fully understand your application. This would include understanding long term trends, comparison over time or comparing to experiment groups, alerting on anomalies, data visualization, and creating the ability to conduct adhoc retrospective analysis. These capabilities are crucial to understanding and maintaining your application. "},{"title":"A Monitoring System Should","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#a-monitoring-system-should","content":"Measure performance provided to users to meet and enforce required service level agreements (SLAs), service level objectives (SLOs), and detecting and managing problemsSend appropriate alerts or notifications to appropriate recipients Aspire to learn to contribute to and drive application reliability within what you can control, regardless of platform.If focus on infrastructure is necessary, monitor what you can control to determine your application’s underlying infrastructure components and their health. Monitor the interaction of \"neighbor\" applications and upstream and downstream dependencies to understand the relationship and communication between them and your application. An example of this would be to monitor response time between neighbor applications rather than just if a connection between components or endpoint is up or down. Another example may be to measure volume or throughout sent to your application from a downstream dependency to understand the impact on your application from a known source. It is important to understand and define what a healthy system performs like. A healthy system should be defined based on specific criteria for each application and that criteria will vary from app to app and potentially change over time. Some examples of what can define a healthy system include the following: Successful responsesResponse Times and Threshold definitionsDependency Failures Dependency SLA’sPage load times These should be very dynamic in nature and specific to each application. This should be maintained as a policy for each service and be validated by the monitoring system on a frequency basis. "},{"title":"The 4 Golden Signals","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#the-4-golden-signals","content":"The 4 Golden Signals are the basis for any well monitored system and should be prioritized. More information on the 4 Golden Signals can be found here or in the Google Site Reliability Engineering book. "},{"title":"Types of Monitoring","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#types-of-monitoring","content":"Observability - The practice of obtaining data from timeseries metrics (telemetry), logging data, and transaction traces for the purpose of discovering unexpected patterns. Observability is achieved when data made available from the monitored system provides granular insights with rich context into the behavior of a system for analysis. Telemetry - The collection of measurements or other data that is automatically transmitted to receiving equipment for monitoring purposes. In terms of information services, 'measurements or other data' could mean OS & host data, but it also can reference web logs (hits per second), network service (throughput), or anything else that signifies the 'health' of your system. A few examples of telemetry tools include Telegraf, Prometheus, and HP OpsBridge and can be visualized with tools such as Grafana, Kibana, and Geckoboard. Log Aggregation - The collection of log files to organize the data in them and make them searchable. Log aggregation is useful for the capturing of known knowns. Examples of log aggregation tools include Elastic Logs and Splunk. Application Performance Monitoring (APM) - The monitoring and management of performance and availability of software applications. APM strives to detect and diagnose complex application performance problems to maintain a defined level of service. Examples of application performance monitoring tools include Elastic APM, Dynatrace, and New Relic. Tracing - Used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance through the building blocks commonly referred to as \"spans\". Tracing is useful for the capturing of unknown unknowns. Examples of tracing tools include Jaeger and Pinpoint. Synthetic - A technique using emulation or scripted recordings of a transaction. Scripts are created to simulate an action or path that a customer or end-user would perform on a site, application, or other software. Examples of synthetic monitoring include HP BSM, New Relic Synthetics, or Dynatrace Synthetics. "},{"title":"Raw Error Rate","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#raw-error-rate","content":"Raw Error Rate is an availability metric that is designed to give an accurate indication of how your application or service is performing and is to be measured from the customer's point of view. Raw Error Rate is both an indicator of customer experience as well as the availability of the underlying technology that provides the service. Raw Error Rate is a very simple formula, although it is not necessarily easy to calculate. Copy Raw Error Rate = Failed Requests / Total Requests Where failed requests are requests that are not successful or requests that return successfully, but not in an acceptable amount of time and where total requests are any and all observable requests. Customer Success Rate is a more positive spin on Raw Error Rate. It is the inverse of Raw Error Rate... (1-RER) * 100 = Customer Success as a Percentage More information on Raw Error Rate can be found on the SRE HubConnect Blog "},{"title":"Mean Time to Detect","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#mean-time-to-detect","content":"Mean Time to Detect is a metric that measures the average time it takes to discover an issue or problem is occurring. Monitoring is the first and most essential component of having a low Mean Time to Detect as monitoring acts as the \"trip wire\" or notifier of a problem. The more accurate and detailed your monitoring is, the faster your mean time to detect score is, resulting in resolving issues or potential issues more quickly. "},{"title":"Vital Business Function (VBF) Monitoring","type":1,"pageTitle":"Monitoring Pattern","url":"docs/monitor-alert/monitor/monitoring-pattern#vital-business-function-vbf-monitoring","content":"These particular functions have been deemed essential to an application and the success of the business. It is extremely important that these functions perform well, so monitoring these should be priority over other functions or services. Setting up monitoring to capture metrics surrounding error rate or increases in response time are beneficial to gaining insight quickly to when there may be an issue with a VBF, helping to reduce Mean Time to Detection (MTTD) and Mean Time to Restoration (MTTR). This helps ensure VBFs are performing well and giving a satisfactory customer experience. More detailed information on Fundamental Monitoring Theory can be found at: Google SRE - Monitoring Distributed Systems Optum Fundamental Monitoring Theory "},{"title":"Terraform Overview","type":0,"sectionRef":"#","url":"docs/infrastructure/terraform-overview","content":"Overview Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions. The key features of Terraform are: Infrastructure as Code Infrastructure is described using a high-level configuration syntax. This allows a blueprint of your datacenter to be versioned and treated as you would any other code. Additionally, infrastructure can be shared and re-used. Execution Plans Terraform has a \"planning\" step where it generates an execution plan. The execution plan shows what Terraform will do when you call apply. This lets you avoid any surprises when Terraform manipulates infrastructure. Resource Graph Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. Change Automation Complex change sets can be applied to your infrastructure with minimal human interaction. With the previously mentioned execution plan and resource graph, you know exactly what Terraform will change and in what order, avoiding many possible human errors. If it can be captured with code, it can be automated Usage Pattern and where it's valuable. - Dig into this Reproducible Infrastructure Terraform is capable of providing the same Terraform allows us to enforce parody with other environments. Copy • By using the same set of Terraform configurations, we can provision identical, on demand staging, QA, and production environments. (This means having the ability to use the same code for stage and production). Modules Terraform modules allow us to encapsulate a set of Terraform configurations and expose only the components we want to users.","keywords":""},{"title":"Monitoring Theory","type":0,"sectionRef":"#","url":"docs/monitor-alert/monitor/monitoring-theory","content":"Monitoring Theory Alerting Theory Alerting for MTTD Observability Interpreting Monitoring Data Diagnosis/Fault Detection/Fault Isolation Dashboards/Data Visualization Understanding Cause vs Effect Reporting on Monitoring Capabilities Classes of Monitoring Tools Up/Down ChecksSyntheticsLog File AggregationApplication Performance ManagementPacket Capture/InspectionTelemetryReal User Monitoring (RUM)","keywords":""},{"title":"The 4 Golden Signals","type":0,"sectionRef":"#","url":"docs/monitor-alert/monitor/the-four-golden-signals","content":"","keywords":""},{"title":"Latency","type":1,"pageTitle":"The 4 Golden Signals","url":"docs/monitor-alert/monitor/the-four-golden-signals#latency","content":"The time it takes to service a request. It is important to distinguish between the latency of successful requests and the latency of failed requests.Slow errors are worse than fast errors! "},{"title":"Traffic","type":1,"pageTitle":"The 4 Golden Signals","url":"docs/monitor-alert/monitor/the-four-golden-signals#traffic","content":"A measure of how much demand is being placed on your system. Traffic is measured in a high-level system-specific metricFor a web service, this metric is typically HTTP requests per second. For an audio streaming system, this metric may focus on network I/O or concurrent users. "},{"title":"Errors","type":1,"pageTitle":"The 4 Golden Signals","url":"docs/monitor-alert/monitor/the-four-golden-signals#errors","content":"The rate of requests that fail. "},{"title":"Saturation","type":1,"pageTitle":"The 4 Golden Signals","url":"docs/monitor-alert/monitor/the-four-golden-signals#saturation","content":"How \"full\" your system is. When creating rules for monitoring and alerting, asking the following questions can help you avoid false positives and pager burnoutDoes this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible?Will I ever be able to ignore this alert, knowing it’s benign? When and why will I be able to ignore this alert, and how can I avoid this scenario?Does the alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren’t being negatively impacted, such as test deployments that should be filtered out?Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be a long-term fix, or just a short-term workaround?The alert system should be designed to alert on imminent real problems, alert on potential long term problems, and support rapid diagnosis.These questions reflect a fundamental philosophy on pages and pagers:Every time the pager goes off, I should be able to react with a sense of urgency. I can only react with a sense of urgency a few times a day before I become fatigued.Every page should be actionable.Every page response should require intelligence. If a page merely merits a robotic response, it shouldn’t be a page.Pages should be about a novel problem or an event that hasn’t been seen before. "},{"title":"Monitoring and Alerting","type":0,"sectionRef":"#","url":"docs/monitor-alert/monitoring-alerting","content":"Placeholder for future Monitoring and Alerting documentation.","keywords":""},{"title":"observability","type":0,"sectionRef":"#","url":"docs/monitor-alert/observability","content":"","keywords":""},{"title":"Pillars of Observability","type":1,"pageTitle":"observability","url":"docs/monitor-alert/observability#pillars-of-observability","content":"Events - Immutable record of discrete events that happen over time. Metrics - Numbers describing a particular process or activity measured over intervals of time. Tracing - Data that shows which line of code is failing to gain better visibility at the individual user level for events that have occurred. "},{"title":"Observability vs Monitoring","type":1,"pageTitle":"observability","url":"docs/monitor-alert/observability#observability-vs-monitoring","content":"Observability is often mistakenly interchanged with monitoring. Monitoring\tObservabilityTells if the system is working\tLet's you ask why it's not working A collection of metrics and logs about a system\tThe dissemination of information from that system Failure-centric\tUnderstand system behavior regardless of an outage Is \"the how\" / Something you do\tIs \"the goal\" / Something you have I monitor you\tYou make yourself observable "},{"title":"Culture of Observability","type":1,"pageTitle":"observability","url":"docs/monitor-alert/observability#culture-of-observability","content":"Observability is not a replacement for monitoring; they are complementary. It's difficult to implement effective monitoring without a culture of observability. Tools by themselves are not sufficient alone and none are going to automatically give observability. Observability as a culture is the degree to which a team or company values the ability to observe, inspect, and understand systems, their workload, and their behavior. Code isn't done until you've built analytics to observe and support it. "},{"title":"Modern Event Handling Techniques","type":1,"pageTitle":"observability","url":"docs/monitor-alert/observability#modern-event-handling-techniques","content":"Three techniques used in handling events with the end goal of shared insights, a collaborative response, data-enabled IT, and intelligent operations. Collect All Relevant Data - This allows complete visibility across stacks, technologies, and environments Cloud NativeTraditional, on-premises, monolithic, etc.Hybrid environments De-spam - Separate valuable signals from the noise. Add Context - Prioritize resolution to ensure service availability and to provide business detail. "},{"title":"Metrics that Matter","type":1,"pageTitle":"observability","url":"docs/monitor-alert/observability#metrics-that-matter","content":"Metrics System Metrics (CPU, memory, disk)Infrastructure metrics (AWS CloudWatch)Web tracking scripts (Google Analytics)Application agents (APM, error tracking)Business metrics (revenue, customer signups, bounce rate, cart abandonment) Events Events come in three forms - plain text, structured, and binary. System and server logs (syslog, journald)Firewall and intrusion detection system logsSocial media feeds (Twitter, etc)Application, platform, and server logs (log4j, log4net, Apache, MySQL, AWS) "},{"title":"Sources","type":1,"pageTitle":"observability","url":"docs/monitor-alert/observability#sources","content":"\"Observability,\" Wikipedia, 2018 Ernest Mueller, \"Monitoring and Observability,\" www.agileadmin.com Splunk Beginners Guide to Observability "},{"title":"Prometheus","type":0,"sectionRef":"#","url":"docs/monitor-alert/prometheus-index","content":"Placeholder for new Prometheus documentation.","keywords":""},{"title":"Linux Overview","type":0,"sectionRef":"#","url":"docs/os/linux/linux-overview","content":"Landing page for Linux documentation.","keywords":""},{"title":"Monitoring Glossary","type":0,"sectionRef":"#","url":"docs/monitor-alert/monitor/monitoring-glossary","content":"Application - A program in which you interact with Process - The filename that runs when you launch an application. A process can manage multiple services. Service - A process that runs in the background (you do not directly interact with it). A service can start as soon as the OS is booted and can run even if you are logged off your account. Only one instance of a service can run at one time. A service can manage multiple processes. APM - monitoring and management of performance and availability of software applications. APM strives to detect and diagnose complex application performance problems to maintain an expected level of service. Metrics (Time Series Data) - aggregated measurements over time. Examples include throughput over time, average response time over a one minute interval, or CPU utilization of time. Events - discrete events that happen at a specific moment in time. Examples include a log or error being reported or a configuration change. • Some events are aggregated over time to create metrics (for example: a count of errors over time). Logs - files that record events that occur in an operating system or software Traces - a complete picture of a single transaction, down to the database queries and exact invocation patterns. With traces, you get much deeper visibility into a single slow transaction, which can help you understand a broader problem Copy • transaction - one logical unit of work in an application. This term primarily refers to server-side transactions monitored Thread - Transaction - one of the system level calls generated by an action that is logged on your system. Transaction trace - a stack trace of a transaction. Apdex_t = the goal time of the transaction. apdex_t \"tolerating\" = a load time greater than the apdex_t value, but less than 4 times that number (e.x. if the apdex_t is .5, apdex_t \"tolerating\" is between .51 and 1.99)apdex_t \"frustrated\" = a load time 4x greater than the apdex_t value. Key transactions - transactions you have decided are really important, this enables more detailed alerting options. Telemetry - Key Performance Indicator (KPI) - a recurring saved search that returns the value of performance metric, such as CPU load percentage, memory used percentage, response time, and so on. Defining KPIs for APIs being used is a critical part of understanding not just how they work but how well they can work and the impact they have on your services. Some KPIs are 'soft' - your documentation, the formal definitions of how the API works, and other are 'hard' - focusing on the underlying functionality of the service being measured.","keywords":""},{"title":"Bash Scripting","type":0,"sectionRef":"#","url":"docs/os/linux/bash-scripting","content":"","keywords":""},{"title":"Bash","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#bash","content":"Born Again Shell "},{"title":"Environment / System Variables","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#environment--system-variables","content":"Variable - Placeholder for another value. These can be used in scripts. "},{"title":"Bash Scripting Backslash Characters","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#bash-scripting-backslash-characters","content":"\\a - alert (bell) \\b - backspace \\e - escape character \\f - form feed \\n - new line \\r - carriage return \\t - horizontal tab \\v - vertical tab \\ - backslash \\' - single quote \\nnn - the eight-bit character whose value is the octal value nnn (one to three digits) \\xHH - the eight-bit character whose value is the hexidecimal value HH (one or two hex digits) \\cx - a control-x character "},{"title":"Basic Shell Scripting","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#basic-shell-scripting","content":"Scripts need an interpreter /bin/sh - (rarely used these days. Original bash shell) /bin/bash - (Bourne Again Shell) "},{"title":"Variables","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#variables","content":"$ANYNAME "},{"title":"Arguments","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#arguments","content":"$1 - first argument $2 - second argument call these arguments in a script - $argument $? - Exit Code/Status (variable that stores 0 or non 0 numbers depending on if the previous statement was successful or not) #! - shebang /bin/bash - bourne again shell variables - we can pass variables or create Arguments to pass to scripts arguments - options we can pass to our scripts that can also be variables echo - prints/echos what we tell echo to print to the screen for - loops through and iterates through data for us if - used for conditional coding. Based on exit status code, for example exit - exit or stop a script function && - and || - or if/then if condition then command else command fi Options for if/then/else -d - Checks to see if the specified directory exists -e - Checks to see if the specified file exists -f - Checks to see if the specified file exists and it's a regular file -G - Checks to see if the specified file exists and is owned by a certain group -h or -L - Checks to see if the specified file exists and if it is a symbolic link -O - Checks to see if the specified file exists and if it is owned by a specified UID -r - Checks to see if the specified file exists and if the read permission is granted -w - Checks to see if the specified file exists and if the write permission is granted -x - Checks to see if the specified file exists and if the execute permission is granted -z - Checks if the specified file, string, argument, etc. is null (True if null) "},{"title":"Looping Structures","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#looping-structures","content":"While Loop Until Loop For Loop while loop - execute over and over until a specified condition is no longer true. structure: while condition do command done until loop - run over and over as long as the condition is false and will stop as soon as the condition is true. structure: until condition do command done for loop - will loop a specified number of times. three options for creating a number sequence with seq: if specify a single value, the sequence starts at one, increments by one, and ends at the specified value. if specify two values, the sequence starts at the first value, increments by one, and ends at the second value. if specify three values, the sequence starts at the first value, increments by the second value, and ends at the third value.  "},{"title":"Common Environment Variables","type":1,"pageTitle":"Bash Scripting","url":"docs/os/linux/bash-scripting#common-environment-variables","content":"--------------------|------------- Environment Variable Default Value BASH AND SHELL | /bin/bash CPU | spec to your system DISPLAY | the local video card monitor ENV | /etc/bash.bashrc EUID | the UID number of current user HISTFILE | ~/.bash_history HISTSIZE | 1000 HOME | current users home directory HOST and HOSTNAME | system assigned hostname LOGNAME | username of current user MAIL | /var/spool/mail/username OR /var/mail/username MANPTH | distribution dependant OLDPWD | prior working directory OSTYPE | Linux PATH | distribution dependent PSI | distribution dependent PWD | depends on current directory within USER and USERNAME | username of current user "},{"title":"OS Overview","type":0,"sectionRef":"#","url":"docs/os/os-overview","content":"Placeholder for future OS documentation.","keywords":""},{"title":"MacOS Overview","type":0,"sectionRef":"#","url":"docs/os/macos/macos-overview","content":"Landing page for MacOS documentation.","keywords":""},{"title":"regular-expression-cheatsheet","type":0,"sectionRef":"#","url":"docs/os/regular-expression-cheatsheet","content":"Regular Expression Cheatsheet#","keywords":""},{"title":"Application_Monitoring_Pattern","type":0,"sectionRef":"#","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern","content":"","keywords":""},{"title":"Context","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#context","content":"This document provides monitoring best practices and guidance for application teams architecting for resiliency. "},{"title":"Problem","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#problem","content":"Complex infrastructure and customer experiences are often difficult to instrument and measure holistically. When monitoring with application resiliency as the focus, it is vital to instrument monitoring so a thorough understanding of application infrastructure and transaction flow may be achieved. In other words, it is necessary to first understand where and why problems will and could occur before any type of resiliency practices may take place. "},{"title":"Solution","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#solution","content":"There are many monitoring solutions available and an appropriate solution or solutions will vary from application to application. "},{"title":"Monitoring","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#monitoring","content":"First we must define monitoring in a technology focus. Google SRE defines monitoring as collecting, processing, aggregating, and displaying real-time quantitative data about a system, over a period of time, to observe and check the progress or quality of the system. "},{"title":"Goal of Monitoring","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#goal-of-monitoring","content":"The main goal of monitoring is to fully understand your application. This would include understanding long term trends, comparison over time or comparing to experiment groups, alerting on anomalies, data visualization, and creating the ability to conduct adhoc retrospective analysis. These capabilities are crucial to understanding and maintaining your application. "},{"title":"A Monitoring System Should","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#a-monitoring-system-should","content":"Measure performance provided to users to meet and enforce required service level agreements (SLAs), service level objectives (SLOs), and detecting and managing problemsSend appropriate alerts or notifications to appropriate recipients Aspire to learn to contribute to and drive application reliability within what you can control, regardless of platform.If focus on infrastructure is necessary, monitor what you can control to determine your application’s underlying infrastructure components and their health. Monitor the interaction of \"neighbor\" applications and upstream and downstream dependencies to understand the relationship and communication between them and your application. An example of this would be to monitor response time between neighbor applications rather than just if a connection between components or endpoint is up or down. Another example may be to measure volume or throughout sent to your application from a downstream dependency to understand the impact on your application from a known source. It is important to understand and define what a healthy system performs like. A healthy system should be defined based on specific criteria for each application and that criteria will vary from app to app and potentially change over time. Some examples of what can define a healthy system include the following: • Successful responses • Response Times and Threshold definitions • Dependency Failures Dependency SLA’s • Page load times These should be very dynamic in nature and specific to each application. This should be maintained as a policy for each service and be validated by the monitoring system on a frequency basis. "},{"title":"The 4 Golden Signals","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#the-4-golden-signals","content":"The four golden signals are the four main metrics to focus on when monitoring. Latency - The time it takes to service a request. It is important to distinguish between latency of successful requests and failed requests. Factoring failed requests into overall latency may result in misleading latency metrics. Tracking latency of failures separately is also beneficial because understanding when slow failures are occurring compared to fast failures is important to improving customer experience.Traffic - A measure of how much demand is being placed on your system. Examples of uses of this metric may include HTTP requests per second for a web service, network I/O for an audio streaming service, or retrievals per second for a storage system.Errors - The rate of requests that fail.Saturation - How \"full\" your system is. An example of this may be if your /opt directory on your Linux server is full and can't perform necessary upgrades. "},{"title":"Types of Monitoring","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#types-of-monitoring","content":"Observability - The practice of obtaining data from timeseries metrics (telemetry), logging data, and transaction traces for the purpose of discovering unexpected patterns. Observability is achieved when data made available from the monitored system provides granular insights with rich context into the behavior of a system for analysis. Telemetry - The collection of measurements or other data that is automatically transmitted to receiving equipment for monitoring purposes. In terms of information services, 'measurements or other data' could mean OS & host data, but it also can reference web logs (hits per second), network service (throughput), or anything else that signifies the 'health' of your system. A few examples of telemetry tools include Telegraf, Prometheus, and HP OpsBridge and can be visualized with tools such as Grafana, Kibana, and Geckoboard. Log Aggregation - The collection of log files to organize the data in them and make them searchable. Log aggregation is useful for the capturing of known knowns. Examples of log aggregation tools include Elastic Logs and Splunk. Application Performance Monitoring (APM) - The monitoring and management of performance and availability of software applications. APM strives to detect and diagnose complex application performance problems to maintain a defined level of service. Examples of application performance monitoring tools include Elastic APM, Dynatrace, and New Relic. Tracing - Used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance through the building blocks commonly referred to as \"spans\". Tracing is useful for the capturing of unknown unknowns. Examples of tracing tools include Jaeger and Pinpoint. Synthetic - A technique using emulation or scripted recordings of a transaction. Scripts are created to simulate an action or path that a customer or end-user would perform on a site, application, or other software. Examples of synthetic monitoring include HP BSM, New Relic Synthetics, or Dynatrace Synthetics. "},{"title":"Raw Error Rate","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#raw-error-rate","content":"Raw Error Rate is an availability metric that is designed to give an accurate indication of how your application or service is performing and is to be measured from the customer's point of view. Raw Error Rate is both an indicator of customer experience as well as the availability of the underlying technology that provides the service. Raw Error Rate is a very simple formula, although it is not necessarily easy to calculate. Raw Error Rate = Failed Requests / Total Requests Where failed requests are requests that are not successful or requests that return successfully, but not in an acceptable amount of time and where total requests are any and all observable requests. Customer Success Rate is a more positive spin on Raw Error Rate. It is the inverse of Raw Error Rate... (1-RER) * 100 = Customer Success as a Percentage More information on Raw Error Rate can be found on the SRE HubConnect Blog "},{"title":"Mean Time to Detect","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#mean-time-to-detect","content":"Mean Time to Detect is a metric that measures the average time it takes to discover an issue or problem is occurring. Monitoring is the first and most essential component of having a low Mean Time to Detect as monitoring acts as the \"trip wire\" or notifier of a problem. The more accurate and detailed your monitoring is, the faster your mean time to detect score is, resulting in resolving issues or potential issues more quickly. "},{"title":"Vital Business Function (VBF) Monitoring and implications as relating to Optum","type":1,"pageTitle":"Application_Monitoring_Pattern","url":"docs/monitor-alert/monitor/Application_Monitoring_Pattern#vital-business-function-vbf-monitoring-and-implications-as-relating-to-optum","content":"These particular functions have been deemed essential to Optum and the success of the business, to goal of which is to provide quality healthcare. It is extremely important that these functions perform well, so monitoring these should be priority over other functions or services. A simple examples of what a common VBF at Optum may be is logging in to a healthcare portal to view healthcare information. Setting up monitoring to capture metrics surrounding error rate or increases in response time are beneficial to gaining insight quickly to when there may be an issue with a VBF, helping to reduce Mean Time to Detection (MTTD) and Mean Time to Restoration (MTTR). This helps ensure VBFs are performing well and giving a satisfactory customer experience. More detailed information on Fundamental Monitoring Theory can be found at:Google SRE - Monitoring Distributed SystemsOptum Fundamental Monitoring Theory "},{"title":"VI Cheatsheet","type":0,"sectionRef":"#","url":"docs/os/vi-cheatsheet","content":"Starting vi: vi filename start editing filename, create it if necessary Saving the file you're working on and/or leaving vi: :wq write the file to disk and quit :q! quit without saving any changes :w! newfile write all lines from the entire current file into the file 'newfile', overwriting any existing newfile :n,m w! newfile write the lines from n to m, inclusive, into the file newfile, overwriting any existing newfile Moving the Cursor Many commands take number prefixes; for example 5w moves to the right by 5 words. h one space to the left (also try left arrow) j one line down (also try down arrow) k one line up (also try up arrow) l one space to the right (also try right arrow) $ end of current line ^ beginning of current line Enter beginning first word on the next line G end of file :n line n w beginning of next word e end of next word b beginning of previous word Ctrl-b one page up Ctrl-f one page down % the matching (, ), [, ], {, or } Searching for Text /string search down for string ?string search up for string n repeat last search from present position Inserting Text a append starting right of cursor A append at the end of the current line i insert starting left of cursor I insert at beginning of the current line o open line below cursor, then enter insert mode O open line above cursor, then enter insert mode :r newfile add the contents of the file newfile starting below the current line Deleting Text x delete single character; 5x deletes 5 characters dw delete word; 5dw deletes 5 words dd delete line; 5dd deletes ... well you get the idea! cw delete word, leaves you in insert mode (i.e. change word) cc change line -- delete line and start insert mode s change character -- delete character and start insert mode D delete from cursor to end of line C change from cursor to end of line -- delete and start insert mode u undo last change U undo all changes to current line J join current line with line that follows (press Enter in insert mode to split line) Cutting and Pasting xp transpose two characters (two commands, x followed by p) yy yank (i.e. copy) one line into a general buffer (5yy to yank 5 lines) \"ayy yank into the buffer named a P put the general buffer back before the current line \"aP put from buffer a before current line p put the general buffer back after the current line \"ap put from buffer a after the current line VI COMMANDS Cheat Sheet Modes Vi has two modes insertion mode and command mode. The editor begins in command mode, where the cursor movement and text deletion and pasting occur. Insertion mode begins upon entering an insertion or change command. [ESC] returns the editor to command mode (where you can quit, for example by typing :q!). Most commands execute as soon as you type them except for \"colon\" commands which execute when you press the ruturn key. Quitting :x Exit, saving changes :q Exit as long as there have been no changes ZZ Exit and save changes if any have been made :q! Exit and ignore any changes Inserting Text i Insert before cursor I Insert before line a Append after cursor A Append after line o Open a new line after current line O Open a new line before current line r Replace one character R Replace many characters Motion h Move left j Move down k Move up l Move right w Move to next word W Move to next blank delimited word b Move to the beginning of the word B Move to the beginning of blank delimted word e Move to the end of the word E Move to the end of Blank delimited word ( Move a sentence back ) Move a sentence forward { Move a paragraph back } Move a paragraph forward 0 Move to the begining of the line $ Move to the end of the line 1G Move to the first line of the file G Move to the last line of the file nG Move to nth line of the file :n Move to nth line of the file fc Move forward to c Fc Move back to c H Move to top of screen M Move to middle of screen L Move to botton of screen % Move to associated ( ), { }, [ ] Deleting Text Almost all deletion commands are performed by typing d followed by a motion. For example, dw deletes a word. A few other deletes are: x Delete character to the right of cursor X Delete character to the left of cursor D Delete to the end of the line dd Delete current line :d Delete current line Yanking Text Like deletion, almost all yank commands are performed by typing y followed by a motion. For example, y$ yanks to the end of the line. Two other yank commands are: yy Yank the current line :y Yank the current line Changing text The change command is a deletion command that leaves the editor in insert mode. It is performed by typing c followed by a motion. For wxample cw changes a word. A few other change commands are: C Change to the end of the line cc Change the whole line Putting text p Put after the position or after the line P Put before the poition or before the line Buffers Named buffers may be specified before any deletion, change, yank or put command. The general prefix has the form \"c where c is any lowercase character. for example, \"adw deletes a word into buffer a. It may thereafter be put back into text with an appropriate \"ap. Markers Named markers may be set on any line in a file. Any lower case letter may be a marker name. Markers may also be used as limits for ranges. mc Set marker c on this line `c Go to beginning of marker c line. 'c Go to first non-blank character of marker c line. Search for strings /string Search forward for string ?string Search back for string n Search for next instance of string N Search for previous instance of string Replace The search and replace function is accomplished with the :s command. It is commonly used in combination with ranges or the :g command (below). :s/pattern/string/flags Replace pattern with string according to flags. g Flag - Replace all occurences of pattern c Flag - Confirm replaces. & Repeat last :s command Regular Expressions . (dot) Any single character except newline zero or more occurances of any character [...] Any single character specified in the set... Any single character not specified in the set ^ Anchor - beginning of the line $ Anchor - end of line \\< Anchor - begining of word > Anchor - end of word (...) Grouping - usually used to group conditions \\n Contents of nth grouping [...] - Set Examples [A-Z] The SET from Capital A to Capital Z [a-z] The SET from lowercase a to lowercase z [0-9] The SET from 0 to 9 (All numerals) [./=+] The SET containing . (dot), / (slash), =, and + [-A-F] The SET from Capital A to Capital F and the dash (dashes must be specified first) [0-9 A-Z] The SET containing all capital letters and digits and a space [A-Z][a-zA-Z] In the first position, the SET from Capital A to Capital Z In the second character position, the SET containing all letters Regular Expression Examples /Hello/ Matches if the line contains the value Hello /^TEST$/ Matches if the line contains TEST by itself /^[a-zA-Z]/ Matches if the line starts with any letter /^[a-z]./ Matches if the first character of the line is a-z and there is at least one more of any character following it /2134$/ Matches if line ends with 2134 /(21|35)/ Matches is the line contains 21 or 35 Note the use of ( ) with the pipe symbol to specify the 'or' condition /[0-9]/ Matches if there are zero or more numbers in the line /^#/ Matches if the first character is not a # in the line Notes: Regular expressions are case sensitiveRegular expressions are to be used where pattern is specified Counts Nearly every command may be preceded by a number that specifies how many times it is to be performed. For example, 5dw will delete 5 words and 3fe will move the cursor forward to the 3rd occurence of the letter e. Even insertions may be repeated conveniently with thismethod, say to insert the same line 100 times. Ranges Ranges may precede most \"colon\" commands and cause them to be executed on a line or lines. For example :3,7d would delete lines 3-7. Ranges are commonly combined with the :s command to perform a replacement on several lines, as with :.,$s/pattern/string/g to make a replacement from the current line to the end of the file. :n,m Range - Lines n-m :. Range - Current line :$ Range - Last line :'c Range - Marker c :% Range - All lines in file :g/pattern/ Range - All lines that contain pattern Files :w - Write to file :r - Read file in after line :n - Go to next file :p - Go to previous file :e - Edit file !!program - Replace line with output from program Other ~ - Toggle upp and lower case J - Join lines . - Repeat last text-changing command u - Undo last change U - Undo all changes to line","keywords":""},{"title":"Slack","type":0,"sectionRef":"#","url":"docs/slack-index","content":"Placeholder for new Slack documentation.","keywords":""},{"title":"Availability Score","type":0,"sectionRef":"#","url":"docs/sre/availability/availability-score","content":"Availability is dependent on optimal supportability. Monitoring improves service availability and MTTR by increasing supportability. What is supportability? Supportability is an ongoing effort to: Improve the accuracy and completeness of a service’s information Reduce the time to engage, time to restore and impact to users during high priority incidents Scope: All High Priority Services Ask: “Are we as ready as we can be?” Murphy’s Law …..\"Anything that can go wrong, will go wrong.\" It’s not a matter of IF, it’s a matter of WHEN impact will occur The true availability score (not Adjusted Down Time in Minutes (ADTM) represents 2 things: A measurement of the number of customers we disappointed today. A prediction of the number of customers we will disappoint tomorrow. Ability to measure this score depends heavily on following industry standards in monitoring.","keywords":""},{"title":"unix-cheatsheet","type":0,"sectionRef":"#","url":"docs/os/unix-cheatsheet","content":"Unix Cheatsheet# cd - change directory ls - list clear - clear root - administrator account sudo rm -r / - remove entire OS ll (ls -l) - list pwd - present working directory less \"filename\" to view what's in file control + c - cancel ./\"filename\" - used to start scripts man \"command\" - how to use command tail -f syslog - follows the log of the server -f -follow grep - search within a file example: less syslog | grep \"Network\" tail -f /usr/adlex/log/rtm.log Command Description cat [filename] Display fileís contents to the standard output device (usually your monitor). cd /directorypath Change to directory. chmod [options] mode filename Change a fileís permissions. chown [options] filename Change who owns a file. clear Clear a command line screen/window for a fresh start. cp [options] source destination Copy files and directories. date [options] Display or set the system date and time. df [options] Display used and available disk space. du [options] Show how much space each file takes up. file [options] filename Determine what type of data is within a file. find [pathname][expression] Search for files matching a provided pattern. grep [options] pattern [filesname] Search files or output for a particular pattern. kill [options] pid Stop a process. If the process refuses to stop, use kill -9 pid. less [options][filename] View the contents of a file one page at a time. ln [options] source [destination] Create a shortcut. locate filename Search a copy of your filesystem for the specified filename. lpr [options] Send a print job. ls [options] List directory contents. man [command] Display the help information for the specified command. mkdir [options] directory Create a new directory. mv [options] source destination Rename or move file(s) or directories. passwd [name [password]] Change the password or allow (for the system administrator) to change any password. ps [options] Display a snapshot of the currently running processes. pwd Display the pathname for the current directory. rm [options] directory Remove (delete) file(s) and/or directories. rmdir [options] directory Delete empty directories. ssh [options] user@machine Remotely log in to another Linux machine, over the network. Leave an ssh session by typing exit. su [options]user [arguments]] Switch to another user account. tail [options][filename] Display the last n lines of a file (the default is 10). tar [options] filename Store and extract files from a tarfile (.tar) or tarball (.tar.gz or .tgz). top Displays the resources being used on your system. Press q to exit. touch filename Create an empty file with the specified name. who [options] Display who is logged on. man -k [keyword] Search a database for commands that involve the keyword. Can also be used as apropos [keyword]. info [command] Display a fileís help information in an alternate format. man [command] Display a fileís help information. whatis [command] Display a short blurb about the command. OVO / ETC COMMANDS ABOVE snmpget <system> system.sysUpTime.0 get system uptime snmpwalk <system> ipAddrTable get ipAddrTable details. Useful if you cannot ovome/add box to OVO manually. Continuous scrolling of this command indicates in my experience 1 ip bound to 2 interfaces HPUX lanscan show list of network interfaces ifconfig <ifname> show config of a specific network interface swlist -l product | grep PRODUCTNAME show list of installed products and grep for a specific PRODUCTNAME swlist -s <full path to .depot> list contents of a depot file swlist -s <full path to .depot> BUNDLE list details of the bundle swlist -s <full path to .depot> BUNDLE.subbundle list details of the sub-bundle /sbin/init.d/Rpcd 11.00 dce/rpc software, /etc/rc.confid.d/Rpcd /sbin/init.d/ncs 10.20 dce/rpc software AIX alog -o -t boot | more To view the boot log in AIX: alog -o -t console | more To view the console log in AIX: more /var/adm/syslog To view the system log in AIX: nmon To view disks, top processes, etc: lslpp -l | grep dce check DCE version oslevel -r Check OS Level. exportfs -v list NFS exported filesystems Misc /opt/epage/get_persons list epage ID information for /F %i in (path to your line separated file) do <command> %i Windows 'for i in' loop for /F %i in (h:\\ovis.txt) do systeminfo /S %i /U ms\\<username> /P <MYPASSWORD> H:\\projects\\ovis\\%i_systeminfo.txt nslookup $i | grep Address | grep -v 10.7.136.103 | awk '{print $2}' print out IP address from DNS only one line visudo opcuitadm process that runs when someone logs in as TA (template admin) /home/oracle/admin/scripts/local/bin/extract_ov.sh Terry's extract script for PDR ssh -o stricthostkeychecking=no no key checking for SSH ssh -o stricthostkeychecking=no -o numberofpasswordprompts=0 no key, no password prompt for SSH grep ZIG agent_status_process_problem_report.csv | grep AIX | grep coda | awk -F\",\" '{print $1}' check agent status report for all AIZX Mgtp8013:/usr/local/bin/addbox.sh –h <hostname> -e <employeeid> UNIX way to add box to the box database. Mgtp8013:/usr/local/bin/request-ov.sh –h <hostname> -e <employeeid> open HPSD ticket with Infrastructure Monitoring. cat /var/opt/OV/share/dbdumps/a* | grep -v ^P | awk '{print $4}' | sort | uniq > <file> look through user logins and sort unique IDs<meta HTTP-EQUIV=\"REFRESH\" content=\"0; url=yoururl\"> redirect to yoururl from no content Solaris kstat bge:0 | grep duplex check duplex setting of interface using kernel statistics program ndd /dev/bge0 link_speed list link_speed of interface, 0=10, 1=100, 1000=1000 (Gig) ndd -set \\? network device confir sysdef -i list system info, look for IPC @ bottom for Semaphore config ipcs -s list active semaphores /etc/system configuration that'll be used for system next reboot, see also sysdef -i set: list show hidden chars set: nolist turn off hidden chars Copy 12 nohup scopeux -s -r -t > /var/opt/perf/datafiles/scope.log 2>&1 & run scopeux w/ diag flags and log it in background nohup ./myprogram > /tmp/foo.out 2>&1 & Copy stdout to file 2> stderr to file 1>&2 stdout to stderr 2>&1 stderr to stdout &> stderr to file eventcreate create a windows eventlog event ova distriball <platform> distrib to all of platform (or all). Output in /tmp/distribution.txt on each manager tdeploy download today's config from dv to Zig RUN THIS FIRST tdist distrib today's date YYYY-MM-DD from zig and upload to all managers sudo ovdeploy -cmd \"cmd /c dir c:\\temp\" -host apse8279 run command on remote node ovoinstall cmds <platform><script path> install script to cmds \"for i in cat /tmp/nes-06.list.podo echo $i >> /tmp/nes-06.list.po.out ssh -o stricthostkeychecking=no -o numberofpasswordprompts=0 $i \"/var/opt/OV/bin /instrumentation/lpardsi setup 2>&1\" >> /tmp/nes-06.list.po.out done \" SSH to box, run command, echo server name and ssh output to .out file /netops/jolso22/oracle_proc_mapper.pl |grep <PID> find ps -ef output of oracle PID and the client connected to that PID grep zdata server.1.log | grep \"processed on\" | awk '{print $3,$4,$5,$11,$12}' greps out key information from VAS log about zdata processing Copy \"for i in apsw1746 apsw1767 apsw8664 apsw8665 do for x in 14445 24445 34445 44445 54445 do echo \"Testing port $x on host $i\" telnet $i $x <<EOF ^V^] quit EOF done done\" port test via telnet script sp_MSforeachtable @command1 = \"DROP TABLE ?\" Drop all tables from MSSQL","keywords":""},{"title":"chaos-engineering","type":0,"sectionRef":"#","url":"docs/sre/chaos-engineering","content":"Chaos Engineering# http://principlesofchaos.org/?lang=ENcontent","keywords":""},{"title":"Meaningful Availability","type":0,"sectionRef":"#","url":"docs/sre/availability/meaningful-availability","content":"","keywords":""},{"title":"Abstract","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#abstract","content":"High availability is critical for applications because without it, users cannot rely on it for important work. "},{"title":"Related Work","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#related-work","content":"Three desirable properties of an availability metric: meaningful, proportional, and actionable. "},{"title":"Motivation","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#motivation","content":"Availability is the ability of a service to perform its required function at an agreed instant or over an agreed period of time. At a high level, all availability metrics have the following form: availability = good service / total demanded service Availability metrics are valuable for users because they tell them whether or not a service is suitable for their use case. Availability metrics are valuable for developers because they help prioritize their work to improve the system. Proportional metrics enable developers to quantify the benefit of an incremental improvement. Actionable metrics enable developers to zero in on episodes of worst availability and find problems that need addressing. "},{"title":"Time based availability metrics","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#time-based-availability-metrics","content":"availability = MTTF / (MTTF + MTTR) Where MTTF = mean-time-to-failure and MTTR = mean-time-to-recovery This measure is based on the duration of the system being up or down. uptime = good service downtime = bad service availability = uptime / (uptime + downtime) Commonly used time-based availability metrics: Are not proportional to the severity of the system's unavailability (a downtime with 100% failure rate weighs the same as one with 10% failure rate)Are not proportional to the number of affected users (a downtime at night has the same weight as downtime during a peak period)Are not actionable because they do not, in themselves, provide developers guidance into the sources of failuresAre not meaningful in they rely on arbitrary thresholds or manual judgments "},{"title":"Count-based availability metrics","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#count-based-availability-metrics","content":"Success ratio is the ratio of successful requests to total requests. Since success-ratio does not use a threshold, it is more proportional than commonly used time-based metrics. Success ratio is as an availability measure is popular because it is easy to implement and is a reasonable measure. Count-based (success-ratio) availability metrics: Are not meaningful in they are not based on timeAre biased by highly active usersAre biased because of different client behavior during outages "},{"title":"Probes","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#probes","content":"Synthetic probes may mitigate some of the shortcomings of success-ratio. A metric which uses synthetic probes may not be representative of real-user experience Availability metrics based on synthetic probes: Are not representative of user activityAre not proportional to what users experience "},{"title":"Actionable metrics","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#actionable-metrics","content":"Availability metrics mentioned above represent different points in the tradeoffs between proportional and meaningful. They all have a similar weakness though: actionable. A single number with a reporting period does not allow enough insight into the source or shape of unavailability. "},{"title":"Proportional and Meaningful Availability: User-Uptime","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#proportional-and-meaningful-availability-user-uptime","content":"As discussed, prior metrics mentioned for availability are not meaningful or proportional. This equation satisfies both proportionality and meaningfulness: user-uptime = summation uptime per user / summation (uptime per user + downtime per user) The calculation of this metric is not straightforward as it requires the definition of uptime and downtime per user. "},{"title":"Events and Durations","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#events-and-durations","content":"Use user requests as probes. A user's perception of a system being up or down depends on the response they receive when interacting with it. Successful == up, unsuccessful == down If there is any chance that a user may perceive a failure, consider it as a failure. "},{"title":"Challenges with User Uptime","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#challenges-with-user-uptime","content":"How would you label duration as up or down? How do you address if users are active or not? Labeling Durations# If back to back events differ, there are 3 choices for labels: After a successful (failed) operation, assume the system is up (down) until the user sees evidence to indicate otherwiseBefore a successful (failed) operation, assume the system is up (down) until the previous eventSplit the duration between events. Half the time us uptime and half the time is downtime Active and Inactive Periods# Cutoff Duration - if a user has been inactive for more than the specified (cutoff) duration, consider the user as inactive and stop recording uptime and downtime for that user. Definition (uptime, downtime): A segment between two consecutive events originating from the same user is: inactive if the two events are further apart than the cutoff durationuptime if the first of the two events was successfuldowntime if the first of the two events was unsuccessful (failed) For each user and a measurement period of interest, uptime is the sum of lengths of uptime segments and downtime is the sum of lengths of downtime segments. "},{"title":"Actionable Availability: Windowed User Uptime","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#actionable-availability-windowed-user-uptime","content":"Monthly or quarterly availability reporting does not distinguish between a system that routinely fails a small percentage of requests from a system where failures are rare, but experience longer outages. To distinguish long outages from shorter, more frequent outages, the timescale of outages must be looked at. Windowed User Uptime addresses that by combining information from all timescales simultaneously. "},{"title":"Calculating Windowed User Uptime","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#calculating-windowed-user-uptime","content":"Windowed User Uptime iterates over all time windows fully included in the period of interest and it computes availability score for each window size. The score for a window size w is the availability of the worst window size w in the period of interest. Availability for a particular window from t1 to t2 is calculated as follows: A(t1,t2) = good service between t1 and t2 / total service between t1 and t2 To obtain the score for a window size w, enumerate all windows of duration w and compute the availability for each of them and take the lowest value. The result is a score for each window size. This score is the Minimal Cumulative Ratio (MCR). MCR = Minimal Cumulative Ratio MCR picks the worst availability for each window size because that is the window that had the most impact on overall availability. "},{"title":"Monotonicity with Integer Multiple-Sized Windows","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#monotonicity-with-integer-multiple-sized-windows","content":"Expect larger windows to have better availability. "},{"title":"Monotonicity in the General Case","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#monotonicity-in-the-general-case","content":"The worst availability of a day is always better than the worst availability of an hour or of a minute. "},{"title":"Evaluation","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#evaluation","content":""},{"title":"Availability Due to Hyperactive Users","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#availability-due-to-hyperactive-users","content":"Bias due to hyper active users can negatively affect success-ratio. When this bias occurs, success-ratio can mislead us towards thinking that an incident is much more or much less severe than it actually is. "},{"title":"Availability and Hyper Active Retries","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#availability-and-hyper-active-retries","content":"Retries can drive success ratio down even though only a small number of users are impacted. Users (and the clients they use) may behave differently during incidents than during normal operations. Clients may make many more requests during incidents or they may just decide to give up on the system and try few hours later. In both cases, success-ratio over or under estimates the magnitude of an outage while user-uptime matches user perception. "},{"title":"Quantifying Impact of Outages","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#quantifying-impact-of-outages","content":"When using success ratio, quantifying the impact of an outage is difficult because it is not based on time. Seconds of downtime, which we compute as part of user uptime, provides more insight. From there we see the minutes of downtime that our users experience. "},{"title":"Combining User Uptime and Success Ratio","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#combining-user-uptime-and-success-ratio","content":"Sometimes, combining user-uptime with success-ratio yields valuable insights. "},{"title":"Windowed Uptime Causes Burstiness of Unavailability","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#windowed-uptime-causes-burstiness-of-unavailability","content":"When looking at availability metrics aggregated over longer periods of time, it is difficult to see short periods of poor availability. Teams can use windowed user uptime to identify root-cause and then fix the sources of these shorter episodes, improving overall availability. "},{"title":"Applicability of Windowed User Uptime","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#applicability-of-windowed-user-uptime","content":"To calculate windowed user-uptime, fine-grained logs of individual user operations are required. These logs must include a key that enables chaining together operations for each user, the timestamp of the operation, and the status of each operation (success or failure). In the simplest case, retaining the cumulative count of up and down minutes for each minute is needed to calculate windowed user-uptime over any time duration. "},{"title":"Conclusion","type":1,"pageTitle":"Meaningful Availability","url":"docs/sre/availability/meaningful-availability#conclusion","content":"User Uptime combines the advantages of per-user aggregation with those of using a time-based availability measure. User Uptime avoids multiple kinds of bias: hyper-active users contribute similarly to the metric as regular users, and even behavioral changes during an outage result in a proportional and meaningful measurement that in many cases is even more precise than success-ratio. windowed availability allows for the study of multiple time-scales from single minutes to a full quarter in an easy to understand graph. "},{"title":"Linux Syntax","type":0,"sectionRef":"#","url":"docs/os/linux/linux-syntax","content":"","keywords":""},{"title":"Basic Commands","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#basic-commands","content":"echo - Linux equivalent of print command ls - list list flags -a - lists all files, including hidden files -l - long list of files-p - -R -recursive listing. Lists contents of folders and contents withing folders and so on.  halt reboot shutdown clear top which whoami route env uname - shows operating system being used. su - super user. Changes user to different account. cd - change directory pwd - present working directory "},{"title":"init Commands","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#init-commands","content":"init0 - shuts down Linux OS. init6 - same as reboot. Must be root to run this. "},{"title":"Network Commands","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#network-commands","content":"netstat ifconfig ipaddr .bash_history shows history of commands entered. export HISTFILESIZE = <variable number> command completion = tab key "},{"title":"Shell Configuration Files","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#shell-configuration-files","content":"Environment/System Variables variable - placeholder for another value. These can be used in scripts. Variables live in the areas of your systems RAM that is reserved to store whatever values you want to put in it. Its like a container in memory.an environment is a set of variables that are used in configuring the system computing environments. There are user defined and system defined variables.  etc - contains global profiles User Defined Variables Created variables should use all caps Created variables cannot start with a number, but can contain letters, numbers, and hyphens.  "},{"title":"Globbing","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#globbing","content":"Globbing is the process of expanding a non specific file name containing a wildcard character into a set of specific filenames that exist on the storage of a computer, server, or network. Examples: Copy ls *.txt (asterik are wildcard characters. Matches any character or set of characters) ls ?.txt (question mark are for placeholders of any single character) ls [F] (brackets indicate 'starts with') ls [f]*.txt (text files that begin with lowercase f) ls f[igh][lfz]e*.txt (text files that begin with f and end with e with the 2nd character either i,g,h and the 3rd character being l,f, or z) "},{"title":"Quoting","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#quoting","content":"Double Quote Substitutes the value of variables and commandsExample: echo \"Username is $USER\" will print: Username is <username> Single Quote Preserves the literal meaning of each character of a given stringThis will turn off the (special) meaning of all charactersExample: echo 'Username is $USER' will print: Username is $USER Backslash This takes away or removes the special meaning from a single character and can be used as an escape character.If we did not have the \\ character before $5.00 in the following string, it would be interpreted as a variable.Example: echo \"The cheeseburger is going to cost you $5.00\" will print: The cheeseburger is going to cost you .00If we use the backslash, we negate the special character meaning.Example: echo \"The cheeseburger is going to cost you \\$5.00\" will print: The cheeseburger is going to cost you $5.00 "},{"title":"Formatting Commands","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#formatting-commands","content":"Operation for a Linux command can be thought of in three ways: First, the computer waits for the user inputSecond, the user selects a command and enters it via the keyboard or mouseFinally, the computer then executes the command "},{"title":"Working with Options","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#working-with-options","content":"Linux command parameters can be divided into two types: Parameters with a dash (\"-\") are called optionsParameters with no leading dash are called arguments Conceptually, formatting commands look like this: Command - \"What to do?\"Options - \"How to do it?\"Arguments - \"What to do with it?\" "},{"title":"Man","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#man","content":"Linux manual for commands, configuration files, and more. Linux man pages are not meant to be tutorials. Man pages are a reference format. Man pages come in 9 sections. The most frequently used section is section 1. Executable programs and shell commandsSystem calls provided by the kernalLibrary calls provided by the program librariesDevice File (usually stored in /dev)File FormatsGamesMiscellaneous (macro packages, conventions, and so on)System administration commands (programs run mostly or exclusively by root)Kernel routines Example man command: man cp (manual for copy command) To view a specific section: man 5 cp (shows file format for cp) Man pages are also organized like so: NameSynopsisDescriptionOptionsFilesSee AlsoBugsHistoryAuthor When viewing man pages, Linux uses less to view them. Helpful command to navigate man pages are: H or h (displays help)Page Down, Spacebar, Ctrl+V, Ctrl+F (moves down one screen)Page Up, Esx+V, Ctrl+B (moves up one screen)Down Arrow, Enter, Ctrl+N, Ctrl+E, Ctrl+J (moves down one line)Up Arrow, y, Ctrl+Y, +P, +K (moves up one line)/pattern (searches forward on pattern)?pattern (searches backward on pattern)n or / (repeats previous search)Q or ZZ (quits)G (go to end of file)g (go to beginning of file) "},{"title":"Info Pages","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#info-pages","content":"Similar to man, but the goal is to support functions in which man cannot. Most notable of which are hyperlinks. You can use info to read man pages if you wish. Navigating and using the info utility: ? (Displays help info)N (Moves to the next node in a linked series or level)P (Moves back in a series or level)U (Moves up one level in the mode hierarchy)Arrow Keys (Moves the cursor around to select links)Page Up, Page Down (These keys scroll up and down within a single node)Enter (Moves to a new node once you have selected it)L (Displays the last info page you read)T (Displays the top page for a topic)Q (Exists from the info page system) "},{"title":"The Linux Filesystem","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#the-linux-filesystem","content":"Where the data is stored on a storage device within a certain manner Data is organized and easily locatedData can be saved in a persistent mannerData integrity is preservedData can be quickly retrieved for a user at a later point in time The Linux filesystem and the filesystem hierarchy standard (FHS) The Linux filesystem uses a hierarchy structure to organize data The systems have a standard in which at the root directory has several of the same sub directories in a certain order or fashionThe root directory is the highest point of hierarchy Standard directories that almost always live under root, no matter what the distribution is: bin - contains executable files that are necessary to manage and run a Linux system. Includes shell, copy, remove, etc. boot - contains boot loader files to boot the Linux system. dev - contains special files that are used to represent the various hardware devices. etc - contains text based configuration files as well as services running on the system. Common etc directories and their contents: /etc/aliases (contains a table used to redirect all to local users)/etc/exports (configured filesystems to be exported to remove NFS clients)/etc/fstab (lists te partitions and filesystems that will automatically be mounted when the Linux system boots)/etc/ftpusers (controls user access to FTP service running on a Linux system)/etc/group (contains local group definitions)/etc/grub.conf (contains configuration parameters for the init process)/etc/hosts (contains a list of hostnames to IP address mappings that can be used to resolve certain hostnames)/etc/inittab (contains configuration parameters for the init process)/etc/init.d (a sub directory that contains startup scripts and services)/etc/rc.d/init.d (Red Hat of CentOS based systems startup scripts)/etc/passwd (this is our Linux systems users accounts file)/etc/shadow (contains encrypted passwords for user accounts)/etc/resolv.conf (where we specify what DNS server and domain suffix that the system is going to use)/etc/X11 (has the X windows configuration files) home - contains sub directories that server as home directories for users on the Linux server. lib - library. This directory (dir) contains code libraries used by programs that live within the bin or sbin folders. media - directory used to mount external devices mnt - mount. Directory used to mount external drives opt - contains files for some programs that you may need to install manually proc - sudo filesystem that is dynamically created whenever it is accessed. It is used to access process and other system information for the Linux Kernel. Proc directory doesn't actually exist on the filesystem. root - root user's home directory sbin - contains important system management and administration files. Similar to the /bin directory srv - contains subdirectories where services running on the server save their files sys - contains information about the hardware on the system tmp - directory that contains temporary files that are created by the filesystem usr - contains application files var - contains Linux variable data, including Linux log files "},{"title":"Linux Disk File Systems","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#linux-disk-file-systems","content":"ext2 - oldest, most stable Linux disk ext3 - very similar to ext2. ext3 has journaling reiser - alternative to ext3. Performs faster than ext2 or ext3 ext4 - 4th release of extended filesystem "},{"title":"Hidden Files","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#hidden-files","content":"Periods in front of filename or folder name. Example: touch .file.txt Hidden files exist so you don't accidentally edit them. To view hidden files, use the -a parameter. "},{"title":"Absolute and Relative Paths","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#absolute-and-relative-paths","content":"Relative Paths - path that you are currently in. Absolute Path - complete path from root tree down. "},{"title":"Files and Directory","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#files-and-directory","content":"touch command touch filename.txt will create a file called filename.txt if the files does not exist. touching a file will update the timestamp if the file does exist. touch -d will change the date/timestamp if desired.  Example: touch -d \"February 1 2020: file.txt This will change the timestamp of file.txt to Feb 1 00:00 cp command (copy) useful parameters (flags) for cp command: -i, -r, -p, -a, -u, and -dview with man cp mv command (move) works as rename command when ran in the same dir can pick up and move a file when moving to a new dir  rm command (remove) Example: rm file1.txt mkdir command (make directory) can create a parent directory if run with -p flag (nested subdirectories)cannot use touch to create a directory rmdir (remove directory) Example: rmdir directoryname "},{"title":"Case Sensitivity","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#case-sensitivity","content":"Linux is a case sensitive operating system. Commands, directories, and file names are ALL case sensitive. "},{"title":"Simple Globbing and Quoting","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#simple-globbing-and-quoting","content":"Globbing - Expanding a nonspecific filename. Example: ls *.txt => shows any files in the current dir ending in .txt asterik () stands for 0 or more characters question mark (?) is a wildcard for any single character [a - z] .txt prints any file starting with a through z and ending in .txt Quoting double quote - The double quote (\"quote\") protects everything enclosed between two double quote marks except $, ', \", and \\ Use double quotes when you want when you want variables and command substitution. single quote - The single quote ('quote') protects everything enclosed between single quote marks. It is used to turn OFF special meaning of all characters. Prints literal value between quotes. backslash () - negates special meaning of characters encapsulating within backslashes turns on special meaning/variables "},{"title":"Archiving Files and Directories","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#archiving-files-and-directories","content":"tar a Linux utility that archives things. It does not compress or compact files. It just sticks all your files together into one file. This is known as archiving. traditionally, tar was used to create tape backups. We used it to archive data on to tape backup drives. Tar actually stands for 'tape archive.\"tar -cf (creates file) file.tartar -xf (extracts file) file.tar "},{"title":"Archives and Compression","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#archives-and-compression","content":"There are three main ways to compress on Linux. gzip, bzip2, and zip. gzip gzip and gunzip bzip2 (most efficient) bzip2 and bunzip zip zip (only command that archives and compresses) and unzip "},{"title":"zip Syntax","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#zip-syntax","content":"Create a new zip file: zip filezip.zip file.txt zips up file.txt and sends to filezip.zip Unzip a file: unzip filezip.zip Zip a folder: zip -r folderzip.zip foldername zip will archive and compress files "},{"title":"gzip Syntax","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#gzip-syntax","content":"You can compress tar files with gzip or bzip2 gzip folder.tar - this will make the file appear like: folder.tar.gz Uncompress a gzip file. gunzip folder.tar.gz - this will get you your original tar file. untar with tar -xvf - x is extract, v is verbose, f is file. "},{"title":"bzip2 Syntax","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#bzip2-syntax","content":"bzip2 folder.tar - this will make the file appear like: folder.tar.bz2 Uncompress a bzip2 file. bunzip folder.tar.bz2 - this will get you the original tar file. untar with tar -xvf "},{"title":"tar","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#tar","content":"The tar utility does have flags to call gzip or bzip2. So it's possible to archive and compress with the following: gzip - tar -zcf - file will look like file.tar.gz or file.tgz bzip2 - tar -jcf - file will look like file.tar.bz2 or file.tb2 z flag is an argument that tells tar to call gzip j flag is an argument that tells tar to call bzip2 Example: tar -jcf folder.tb2 folder - archives and compresses Example: tar -zxv folder.tb2 - decompresses and extracts "},{"title":"Commands (Revisited)","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#commands-revisited","content":"cat - displays the contents of a file less - reads a file head/tail - reads first 10 lines of a file or last 10 lines of a file find - locates files on a system grep - searches a string in a file sort - organizes text in a file cut - manipulates data by columns wc - can be used to do a word count on a file grep -i - flag tells grep not to care about case sensitivity grep -in - flag gives line number of search item default sort is alphabeticalsort -r sorts backwards alphabeticalsort -n sorts numericallythe power of the cut command comes when using delimiterscut -d \" \" -f2 file.txt this command shows the field 2 contents. The space is what separates the fields. wc -c displays character count "},{"title":"Command Line Pipes","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#command-line-pipes","content":"input ---------> Command ----------> Output (stdin0) (stdin1) string commands together with pipes standard output is 1, standard input is 0, standard error is 2 "},{"title":"I/O Redirection","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#io-redirection","content":"Redirecting data by using > (greater than) Example: Tail /var/log/messages > logtemp.txt this will send the last 10 lines of /var/log/messages to logtemp.txtif a files does not exist, Linux will create a new file to output to. (two greater than signs) appends an existing file (adds output to the bottom of an existing file). "},{"title":"Regular Expressions (Regex)","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#regular-expressions-regex","content":"* Matches any character of . Any single character ? Matches zero or one of the proceeding characters ^ Matches expression if it appears at the beginning $ Matches expression if it appears at the end [nnn] Matches any one character between the braces nnn Matches any expression that doesn't contain any one of the characters specified [n-n] Matches any single character [1-10] Any character between 1 and 1, or 0 Example: grep cc$ file.txt Returns everything that ends in cc in the file.txt file. "},{"title":"Basic Text Editors","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#basic-text-editors","content":"GNU nano Kate Gnome gedit vim vi "},{"title":"nano","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#nano","content":"ctrl+k - deletes current line ctrl+x - saves file "},{"title":"vi","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#vi","content":"by default, vi opens in command (normal) mode. different modes are command, command line, insert, and replace mode. hitting the i key, insert key, s key, o key, or a key will put vi in insert mode. while in insert mode, you can add, remove, or replace text. You cannot, however, perform any file operations. For example, you cannot save the file in insert mode. navigation within vi is very powerful. You can page up and down with the page up and page down keys and use the home and end keys to navigate to the beginning and end of the line. There are also many more navigation commands you may use. vi command mode command line mode - type a colon (:) save a file with w, example: :wsave a file with a new name :w newfilename.txt:wq - saves and quits (exits) :q - closes the file without saving :e! - disregards changes made to the file (does not save)  vi insert mode dw - deletes entire word immediately after your cursor and the space de - deletes entire word immediately after your cursor without the space / - searches for string after /  "},{"title":"Hardware","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#hardware","content":"Processor (Central Processing Unit or CPU) RAM (Random Access Memory) Primary Memory - place on the computer where programs currently in use, store data. CPU can access data in RAM and is very fast. RAM is non persistent. Will delete data if power is not sent to RAM every few milliseconds.  Graphics Cards Mother Boards Power Supply - Converts A/C current to D/C current and sends to various computer components Hard Disks - Magnetic Disks, Solid State Drives Optical Drives - CD/DVD/BlueRay Drives Displays - CRT and LCD Displays "},{"title":"Kernel","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#kernel","content":"The Linux Kernel is a Unix-like operating system. Linux was developed and created by Linus Torvalds. The Linux Kernel is the core of any Linux installation. The Kernel is responsible for messaging every piece of software on a running Linux computer. To maintain order on a chaotic Linux system of process, the Kernel imposes order by using hierarchy. When the system boots, typically one process called the init process, starts up the /sbin/init that in turn manages child processes. "},{"title":"Processes","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#processes","content":"Every process has an associated process ID (PID) Every parent process has a parent ID (PPID) We can identify these PIDs and PPIDs with the ps command. syslog, klog, dmesg ./var/log boot.log cron - Linux scheduling service  /lib, /usr/lib, /etc, /var/log /lib - Linked library files used by binaries in /bin and /usr/bin /usr/lib - Linked library files used by binaries in /bin and /usr/bin /etc - Configuration files for our Linux operating system /var/log - Log files for our Linux operating system "},{"title":"Root and Standard Users","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#root-and-standard-users","content":"Filesystem access controls this is accomplished by using users, groups, ownership, and permissions.  Finger application yum install -y finger gives information about a user  /etc/passwd used for local authentication and contains system users. gid - group ID number pwck utility - validates that files are in sync. pwconv utility - /etc/shadow system password file sudo - similar to su, but works one command at a time. who - command run with flags to give information about system and users. last - user flag will show users last activity. System Users differences between system users and regular users.  User IDs id <username> gives info on user given.  "},{"title":"User Commands","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#user-commands","content":"User add command - useradd <username> creates a user with default values defaults for users added are found and can be edited in the /etc/default/useradd directory can also print out user defaults using the -D flag - useradd -D/etc/login.defs contains default values for the user when it is being created /etc/skel contains files that get copied to every new user created more info and flags can be found at man useradd  "},{"title":"Group Commands","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#group-commands","content":"/etc/group default values for groups/etc/gshadow file stores passwords for groups common commands are groupadd, groupmod, and groupdel "},{"title":"File/Directory Permissions and Owners","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#filedirectory-permissions-and-owners","content":"Permissions, USER, Group, Other drwxrwxr -x . 2 user user -rwxrwxr -x . 2 user user d = directory - = file r = read w = write x = executable rwx (binary permission numbers) or (octal notations) 421 -> 7 -rwxr-xr-x On directories, X means we can change directories into it (cd) On files, X means we can execute it. -rw-r--r-- 420400400 6 4 4 -> 644 Setting permissions using the octal notation: chmod 555 <filename> or chmod ugo+w <filename> (this adds write permissions to the user, group, and other) Before you can change the permissions of a file, you first either need to already be the owner of a file or you need to be the root account. To change ownership of a file, you use the chown command. chown username.groupname <filename> Example: chown stephen.accounting filename.txt Removing Permissions chmod o-r filename.txt (removes read permissions from the others) chmod g-w filename.txt (removes write permissions from groups) chmod u-r filename.txt (removes read permissions from users)  Adding Permissions chmod o+r filename.txt (adds read permissions to others) chmod g+w filename.txt (adds write permissions to groups) chmod u+r filename.txt (adds read permissions to users)  "},{"title":"Symbolic Links","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#symbolic-links","content":"Creates a pointer or link to an actual file. ln -s file.txt myfilelink.txt link -> symbolic -> file linking to -> new file link name Useful for linking to files deep in the system "},{"title":"System Files, Special Files, and Sticky Bits","type":1,"pageTitle":"Linux Syntax","url":"docs/os/linux/linux-syntax#system-files-special-files-and-sticky-bits","content":"Special directories and files /var - contains files that change often, such as mail, logs, etc /var/tmp - contains files that do not get deleted on reboot /tmp - contains temporary files that get deleted on reboot. Every user on a Linux system can write to or delete files on /tmp  The problem with temporary folders that have 777 permissions. We can add sticky bits to a folder which makes it so only users that create their own files and folders can delete theirs and not other users - even if the folder has 777 (rwx rwx rwx)  "},{"title":"Incident Response","type":0,"sectionRef":"#","url":"docs/sre/incidents/incident-response","content":"Ideas for Demonstration - What keeps an application team from achieving a shorter mean time to resolution? Copy • Communicate issues in flow or correlate issues in tracking system ○ Multiple war rooms opened due to multiple applications being impacted by one issue. ○ Communicate with other SREs via ChatOps channel or TCC tracking system about other known applications impacted § App/chatbot that assists with on call rotation and scheduling per channel ○ Add value where you can in a ChatOps channel • On call process and procedure in EPS group • What roadblocks or issues do application teams continually run into or questions are asked over and over again that keep application teams from achieving a better mean time to resolution? • External vendor contacts and helpdesk information available for applications that depend upon external resources. Visibility into vendor code changes • Set up monitoring for New Relic infrastructure to show traffic being sent from pods to bluecoat proxy. Pods to host pods run on Incident Response Topics Before an Incident Copy • Playbooks / Runbooks • On call strategy • Tabletop exercises • Incident Planning and Preparedness During an Incident Copy • Decision Making • Troubleshooting • Recovery Strategy • Communication • Confidence After an Incident Copy • Post Mortem / Post Incident Review (PIR) • Measuring the incident ○ MTTR - Mean Time to Restore § MTTD - Mean Time to Detect § MTTA - Mean Time to Acknowledge § MTTI - Mean Time to Identify § MTTF - Mean Time to Fix ○ Raw Error Rate (RER) • Learning from an incident Incident Management at Optum https://hub.uhg.com/sites/hub/Optum/Businesses/Optum-Technology/Support/ITSM/Incident-Management/Pages/IncidentManagementGettingStarted_SN.aspx Incident management is the IT Service Management process to restore normal service operation as quickly as possible and minimize adverse impacts to business customers. The goal is to maintain the best possible service quality and availability at all times. Information Technology Service Management Incident Management Process and Policy Key Terminology Copy • Incident - An incident is any event which is not part of the standard operation of a service and which causes, or may cause, an interruption to, or degradation in the quality of that service. • Work Around - A method to reduce or eliminate the impact of an Incident or Problem for which a full resolution is not let available. • Solution - A permanent method of removing the impact of an Incident. The solution is the \"final fix\" to the Incident. • Problem - An unknown underlying cause of one or more Incidents. • Service Request - A user request for new or additional service or for information in general. Goal of Incident Management Copy • To restore normal service operation as quickly as possible and minimize the adverse impacts on business operations, ensuring that the nest possible levels of service quality and availability are maintained. Relationship Between Incident and Problem Management Copy • Incident management is reactive. • Incident management process is focused on restoring service and minimizing the impact incidents have on the business within established service targets. Roles and Responsibilities at Optum Client / Caller Copy • Detects an Incident • Reports the Incident to the Technology Support Center • Provides detail about affected components, systems and/or applications, symptoms, impact Service Desk Analyst Copy • Creates Incident records for Incidents reported by clients • Looks for active Parent Incidents related to the new Incident • Uses Knowledge to identify workarounds and solutions • Restores and closes Incident records when possible • Assigns Incidents to appropriate Workgroups when restoration at the TSC is not possible • Engages an Incident Manager for new Priority 1 and 2 Incidents Incident Manager Copy • Reviews multiple user incidents to determine high priority eligibility • Creates Parent Incidents • Provides parent incident information to the Help Desk • Transfers high priority incidents to the Command Center • Performs escalations per process • Follows up on client complaints • Provides workgroup to workgroup paging (as needed) • Assists in reassigning incidents IT Technician (Level 2/3 Support) For Priority 1 and 2 Incidents: Copy • Contacts the TSC Incident Management Team when a new Priority 1 or 2 Incident is identified • Attends War Room activities as requested • Completes restoration activities as assigned • Communicates status of assigned activities to the TCC Incident Manager For Priority 3 and 4 Incidents: Copy • Investigates assigned Incidents • Creates an Incident record for Incidents that are not reported by a client. Clients reporting Incidents should be referred to the TSC or the TSC Self Service Portal • Keeps assigned Incident records up-to-date with current information • Keeps the client informed of Incident status, and documents all client interactions in the Incident record • Optionally, opens and manages Parent Incident records for Low Priority Incidents. • Confirms Incident restoration with the client • Utilizes the Change Management Process as necessary to implement workarounds and solutions • Closes the Incident when a solution has been implemented • As appropriate, opens a Problem record when further root cause analysis is required Technology Command Center For Priority 1 and 2 Incidents: Copy • Validates the Priority of the Incident • Creates and manages the Incident record and associated Incident Alert record throughout the life of the Incident • Facilitates the War Room (as needed) • Identifies and engages appropriate resources • Gathers and documents Service Availability data • Creates a Problem record in response to all Priority 1 Incidents and for Priority 2 Incidents when it was caused by a Change, impacted a Critical Business Application (CBA), resulted in adjusted downtime minutes (ADTM), or upon request • Creates an Incident Change record if a Change is needed to restore a High Priority Incident • Closes the Incident and Incident Alert records Assignment group Owner Copy • Maintains the Workgroup record: membership, classification codes, description • Monitors active Incidents assigned to the Workgroup • Serves as the escalation point for service issues associated with the items assigned to the Workgroup • Reassigns Incidents to appropriate group when necessary • Opens and assigns related Incident Tasks when necessary Service Level Owner / Delegates Copy • Responsible for attending a War Room where an appropriate service is impacted • Validates service is restored • Validates amount of service impact during War Room closure • Decides if a problem is necessary when process doesn’t require one • Enters business impact statements for High Priority incidents impacting their service","keywords":""},{"title":"New Relic SRE","type":0,"sectionRef":"#","url":"docs/sre/new-relic-sre","content":"","keywords":""},{"title":"Introduction","type":1,"pageTitle":"New Relic SRE","url":"docs/sre/new-relic-sre#introduction","content":"The day to day responsibilities of developers and operations engineers are increasing and evolving as companies look for new ways to improve stability, reliability, and automation-first practices. Because of the need to reduce downtime as systems scale, the SRE role is taking shape in many organizations. "},{"title":"Chapter 1: SRE Philosophy and Principles","type":1,"pageTitle":"New Relic SRE","url":"docs/sre/new-relic-sre#chapter-1-sre-philosophy-and-principles","content":"According to Matthew Flaming of New Relic, SRE is the purest distillation of DevOps principles in a single role. The fundamental goal of SRE is greater reliability with less manual intervention as a system scales. The two axes of scaling Workload the number of physical hosts, VMs, and other resources that must be able to grow efficiently in relation with the services that run on them. Complexity the number of dependencies between those services and the growth of the organization itself. SRE is about enabling both forms of scalability. "},{"title":"Chapter 2: What Makes an SRE Successful?","type":1,"pageTitle":"New Relic SRE","url":"docs/sre/new-relic-sre#chapter-2-what-makes-an-sre-successful","content":"SREs see the big picture outside of the day to day and understand and interpret things at a higher level. Consider how work is going to affect the rest of a system. SREs automate at every opportunity. The SRE role is thinking about inefficient and time-consuming things people are doing and stopping them as soon as possible. SREs embraces new tools and approaches (when necessary) SREs are change agents Great SREs have to be effective sales people. They have to be able to sell their colleagues on processes and projects that appear to involve some near term pain or that go against legacy norms.For an SRE, part of being pragmatic means being willing to dump processes, procedures, and tools that are no longer productive. "},{"title":"Chapter 3: SRE Tools and Processes","type":1,"pageTitle":"New Relic SRE","url":"docs/sre/new-relic-sre#chapter-3-sre-tools-and-processes","content":"DevOps and SRE Toolchain Create: Integrated Development Environments (IDEs), text editors, and shared library components are used as \"building blocks\" to build applications. Source control tools such as GitHub and Subversion help erase boundaries between development and operations roles. Verify: Build and continuous integration/continuous delivery (CI/CD) tools such as Jenkins or CircleCI Package: Tools to manage the build, packaging, release staging, and approval process of production ready software, such as Rake or JFrog Release: Tools to manage releases and the lifecycle of an application, like Apache, Maven, or XebiaLabs Configure: Tools like Terraform, Chef, and Ansible fit the \"automate everything\" SRE philosophy. These tools also help automate away much of the manual work formerly needed to implement necessary rules and processes. The increasing use of containers may ultimately reduce the need for these tools and many organizations. Because containerized applications include all of their dependencies and configurations in immutable configurations, container platforms like Docker and orchestration services like Kubernetes or Mesosphere are becoming indispensable to SREs Monitor: Tools like Dynatrace and New Relic that collect metrics from applications, infrastructure, logs, or analytics data and alerts on that data via dashboards and queries. Use Service Level Objectives (SLOs) and Service Level Indicators (SLIs) to Measure Reliability Service Level Objectives are a common way to measure the performance of a service provider. Clearly defined and measured SLO metrics at the product and service level help organizations to: Tune investment and overall prioritization to meet reliability goals and to meaningfully adjust those high-level reliability goals to fit company strategy.Maintain and build customers' confidenceHelp teams decide when and how to focus efforts on reliabilityHelp engineering make better assumptions about risk tolerance and how fast they can go, as well as reason better about dependencies and reduce toil. Service Level indicators can also be used to measure reliability. These performance metrics track some facet of the business. To measure reliability, teams turn to metrics like mean time between failures (MTBF), mean time to repair (MTTR), and mean time to detect (MTTD), all of which help organizations define their \"risk matrices.\" These become powerful tools for prioritizing issues and risks that will have a quantifiable impacts on SLOs, but they also allow organizations to downshift on issues that may not be especially urgent. "},{"title":"Chapter 4: The SRE Role at New Relic","type":1,"pageTitle":"New Relic SRE","url":"docs/sre/new-relic-sre#chapter-4-the-sre-role-at-new-relic","content":"Different descriptions and roles for SRE will be different for different organizations. Set SREs up for success! "},{"title":"Conclusion","type":1,"pageTitle":"New Relic SRE","url":"docs/sre/new-relic-sre#conclusion","content":"Once the SRE role is defined and the right organizational structure and incentives are in place, it all comes down to execution. A successful SRE team depends on a variety of skills and traits. You can always teach technical skills, but you can't necessarily impart essential qualities like empathy and curiosity. Teams and individual SREs need organizational support, communication, and trust in order to thrive. You can't prevent things from ever breaking. Instead, work to see the big picture, incorporate automation, encourage healthy patterns, learn new skills and tools, and improve reliability in everything that you do. Perfection may be unattainable, but constantly striving to do things better is the way to get as close as possible. "},{"title":"Be A Better SRE","type":0,"sectionRef":"#","url":"docs/sre/be-a-better-sre","content":"","keywords":""},{"title":"Behaviors","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#behaviors","content":"SREs are Curious SREs are Motivated SREs are Lazy* SREs are Cautious SREs are Aggressive SREs are Leaders "},{"title":"Curious","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#curious","content":"Consistently question things Question how things are doneAsk if this is the best thing or right thingQuestion yourself and your dependencies Benefits Helps to exercise your brainIncreases levels of dopamineIncreases level of attentionInnovative ways of problem solving Pet Peeves Being told that something can't be done because it \"hasn't been done before\" "},{"title":"Motivated","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#motivated","content":"SREs enjoy experimentation Looks at new technology and its applicability of internal useOn time or company time or bothEngineers look for assignments. They don't wait for them (but they give assigned tasks priority.) Self Training Doesn't wait for a class. They teach themselvesCreate their own use cases and work toward resolutionYouTube, blog posts, documentation Pet Peeves When dependencies say they don't have time "},{"title":"Lazy","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#lazy","content":"Engineers look to automate whenever they can Looks to automate even the little thingsDoesn't like doing the same manual thing twiceLanguage doesn't matter (Ruby, Python, PowerShell)Automation is shared with the public (code repositories) Benefits Even the smallest scrips save time Pet Peeve Systems without an API "},{"title":"Cautious","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#cautious","content":"SREs should not be phased by: Leadership WhimsIndustry Fads They explore opportunities They make up their own mindsRequires curiosity and motivationWhen leadership suggestions come in, they carefully evaluate the recommendation before blindly proceeding Pet Peeves Executive Mandates "},{"title":"Aggressive","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#aggressive","content":"An SRE should not be shy. They should: Ensure their opinion is heard and understood on reliability issuesBe willing to change their minds with new informationNot just be understood, but they should understand as well Pet Peeves Buzzwords and empty phrases. "},{"title":"Leaders","type":1,"pageTitle":"Be A Better SRE","url":"docs/sre/be-a-better-sre#leaders","content":"SREs are flexible. They can: Work as a team memberWork as a team leadCommunicate in all directions: Peers, staff, leadership. Pet Peeves Blame, either on people of circumstancesExcuses "},{"title":"Incident Command System","type":0,"sectionRef":"#","url":"docs/sre/incidents/incident-command-system","content":"","keywords":""},{"title":"Purpose","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#purpose","content":"The intention of this document is to help understand how to build a foundation for an effective incident response process in our organization. To accomplish this, this document covers suggested practices needed for successful incident response, practices that limit damage and reduce recovery time and costs, how to scale the Incident Command System (ICS) up or down as necessary for your teams, mapping ICS roles to real responders in your organization as well as describe considerations when taking on multiple roles, and how to effectively communicate with stakeholders during major incidents. "},{"title":"Contents","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#contents","content":"History and Introduction of Incident Response People Roles and Incident Categorization Best Practices for Incident Commanders Other Roles in Incident Command Incident Response at Scale Incident Response Pitfalls and How to Avoid Them Follow Up and Postmortem Summary Other Incident Response Resources "},{"title":"History and Introduction of Incident Response","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#history-and-introduction-of-incident-response","content":"Consumers expect perfection. Consumers wait less than one minute before abandoning an application. Teams face massive complexity. 5 hours is the average amount of time it takes organizations to resolve a major (customer-impacting) incident. Vital to have a strategy to minimize the impact of incidents and reduce their recovery time and associated cost. Incident Response is the continuous process of developing, practicing, and refining that strategy. Incident - An incident is any unplanned disruption or event that is actively affecting customers' ability to use the product. The goal of incident response is to handle a situation in a way that limits damage and reduces recovery time and cost. Replace chaos with calm. Incident Response is an organized approach to addressing and managing an incident in a way that reduces damages, recovery time, and cost. To accomplish this goal, you must: Mobilize and inform only the right people at the right time. Use systematic learning and improvement. Work toward total automation.  "},{"title":"The Incident Command System (ICS)","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#the-incident-command-system-ics","content":"National model for command structures for any major incident. Used so emergency personnel can have a standardized response everyone is familiar with. It helps to prevent confusion and chaos during an incident. What our version of ICS should not be in technology. Inflexible set of rules Culture and processes are constantly evolvingFrameworks need to allow for improvement and growth Indication of Future Failure Organizations can adapt from any starting pointOnly prerequisite is keeping an open mind Do or Die Mentality Every company is set up differentlyCulture change requires buy-in and time to adjust "},{"title":"When to use a system like ICS","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#when-to-use-a-system-like-ics","content":"An incident is any unplanned disruption or event that is affecting customer ability to use the product. Commonalities of Minor Incidents Not Urgent; can be handled during normal business hours. Situation is a known failure mode with a known fix. Within the responsibilities of a small team. Commonalities of Major Incidents Timing is a surprise: typically little to no warning. Time matters. Need to respond quickly. Situation is rarely perfectly understood at the start. Require coordination and mobilization; often cross-functional. A major incident requires a coordinated response between multiple teams. This definition may differ for different teams and/por organizations. Need to define what a major incident is to your team or organization. Use the Incident Command System for Major Incidents. Major Incidents rarely start out as Major Incidents. Major Incidents usually evolve from a series of minor incidents. "},{"title":"People Roles and Incident Categorization","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#people-roles-and-incident-categorization","content":""},{"title":"Anatomy of Incident Command","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#anatomy-of-incident-command","content":"Severity can often be calculated by how severely our metrics are impacted. "},{"title":"Criteria for Severity","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#criteria-for-severity","content":"Severity 1 - Critical issue that warrants public notification and liaison with executive teams. Criteria: The system is in a critical state and is actively impacting a large number of customers. Functionality has been severely impaired for a long time, breaking SLA. Customer-data-exposing security vulnerability has come to our attention. Major incident response.  Severity 2 - Critical system issue actively impacting many customers' ability to use the product. Criteria: Notification pipeline is severely impaired. Incident response functionality (ack, resolve, etc) is severely impaired. App is unavailable or experiencing severe performance degradation for most/all users. Monitoring of PagerDuty systems for major incident conditions is impaired. Any other event to which a PagerDuty employee deems necessary for incident response. Major incident response.  Severity 3 - Stability or minor customer-impacting issues that require immediate attention from service owners. Criteria: Partial loss of functionality, not affecting majority of customers. Something that has the likelihood of becoming a Sev-2 if nothing is done. No redundancy in a service (failure of 1 more node will cause outage). High-Urgency page to service team.  Severity 4 - Minor issue requiring action, but not affecting customer ability to use the product. Criteria: Performance issues (delays, etc.) (I would argue performance IS a requirement for applications these days and is a more severe page in many cases). Individual host failure (i.e one node out of a cluster). Delayed job failure (not impacting event and notification pipeline). Cron failure (not impacting event and notification pipeline). Low-Urgency page to service team.  Severity 5 - Cosmetic issues or bugs. not affecting customer ability to use the product. Criteria: Bugs not impacting the immediate ability to use the system. Jira/ServiceNow ticket (or GitHub issue).  Anyone can trigger the Incident Response Process at any time. Automatic incident detection, triggering, and resolution whenever it can be done reliably, effectively, and accurately to reduce the human time to evaluate an incident is highly encouraged. But encouraging humans to trigger the incident response process is also highly encouraged when an incident not caught through automation is observed. It will lead to more incidents being resolved more quickly. For example, someone in marketing things a chart looks wrong, it should be encouraged they start the incident response process to notify the correct team of potential issue. Create a culture of transparency and trust from all areas of the organization. PagerDuty example: They use Slack command and a bot to trigger the Incident Response process. !ic page Use business metrics to drive automation of incident creation. Use metrics that let you know how your business is doing, not how a piece of equipment is doing. Examples: Netflix may use stream starts per second. Amazon may use orders per second. Avoid using metrics that are not tied to business (such as high CPU Utilization) for incidents. It is very difficult if not impossible to determine severity if using non business driven metrics. "},{"title":"The Difference in Emergency Operations","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#the-difference-in-emergency-operations","content":"Hierarchy, clear order, work fast. Resolution team has the highest authority. Team works together to resolve incident, document what happened, and keep stakeholders updated. Emergency mode until the incident is officially resolved. One and only one person/role in charge. Avoid Decision Paralysis Taking the wrong action is better than taking no action. "},{"title":"Response Team Goals","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#response-team-goals","content":"Work to resolve the incident quickly. (Deploy immediately to production instead of a typical build process). Work to resolve the incident efficiently. (Methodical in approach by following the tenets of Incident Command). Document decisions and follow up items. (Incidents should be documented real time to avoid hindsight bias). Keep stakeholders informed. (Incidents impact business functionality. Keeping stakeholders informed helps manage the impact felt by other parts of the organization). Stay in wartime until the incident is officially resolved. (Incidents impact business functionality. Keeping stakeholders informed helps manage the impact felt by other parts of the organization).  "},{"title":"Role of Incident Commander","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#role-of-incident-commander","content":"Keep the process on track. Focus on taking actions and keep the process moving in an organized, calm, and effective manner. They are in charge and the single source of reference. Permission to perform any task should go through the incident commander. This is less about authority and more about voicing tasks and actions and giving the scribe and other members of the command layer to understand and document what is happening. The incident commander is not a resolver, they coordinate and delegate. Gain consensus of actions to be taken. \"Are there any strong objections?\" Make a decision. Again, making the wrong decision is better than making no decision. Ask for a status, decide action, gain consensus, assign task, follow up on task completion. Repeat. Size-up, stabilize, update, verify. Repeat.  Importance of Incident Commander Role: Keeps everyone focusedKeeps decision-making movingHelps to avoid the bystander effect (specifically select/assign a person to do a specific task)Keep things moving towards a resolution during a major incidentA common framework for heterogeneous teamsDedicated role(s) for communicating stakeholder updatesDedicated role(s) for customer responseA team focused on helping to mitigate future incidentsTrained team creates space for responders to work as they normally would  "},{"title":"Best Practices for Incident Commanders","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#best-practices-for-incident-commanders","content":"Stabilization LoopSize-up the situation, stabilize the situation, give an update on the situation, and verify if the situation has been stabilized. Repeat. Within the stabilize loop: Ask for a status, decide action, gain consensus, assign task, follow up on task completion. Repeat. The incident commander is not a resolver, they coordinate and delegate. The incident commander is the highest authority (even higher than the CEO). Practice the two ears and one mouth rule. 1st rule of incidents. Don't Panic Good communication is essential. Introduce yourself by name. Say \"Incident Commander\" - Establishes you are the one running the war room (in charge) \"Hello, this is Greg. I am the Incident Commander.\" Avoid acronyms if possible. Clear is better than concise. If no one with Incident Commander training is on the call, it is best to not assign an Incident Commander role. Size-Up Stabilize Update Verify Stabilize: Ask for a statusDecide action, gain consensusAssign TaskFollow up on task completion Ask what actions we can take. Ask the experts of their service. What are the risks involved with the proposed action? Make a decision. Making a wrong decision is better than not making a decision. Gain consensus. \"I propose <action>. Are there any strong objections to this <action>? It is difficult to gain consensus on large calls. As an incident commander, you only care about those who strongly disagree with the proposed action(s). Clear Ownership. Clearly delegate tasks. Assign tasks to a specific person. It is ok to assign tasks to a role if names of people on the call are unknown. Make sure you get acknowledgement from the assigned person they understand the task and they are the one to do it. Time box all tasks What if they need more time? May be good to ask the experts how much time they think they will need to complete the task. What's wrong? What action can we take? What are the risks? Clear ownership with a timebox. What's the status? "},{"title":"Other Roles in Incident Command","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#other-roles-in-incident-command","content":"These roles are the minimum bases to cover. Bigger organizations may have other roles that fit with their business. "},{"title":"Deputy Role","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#deputy-role","content":"Keeps the Incident Commander focused They are not a shadow to the Incident Commander. They are expected to perform important tasks. Takes on any and all additional tasks as necessary Serves to follow up on reminders and ensure tasks aren't missed Acts as a \"hot standby\" for the Incident Commander "},{"title":"Scribe Role","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#scribe-role","content":"Documents the incident timeline and important events as they occur The incident log will be used during the postmortem process Note when important actions are taken, follow-up items, and status updates Anyone can be a Scribe - role is typically assigned by the Incident Commander at the start of the call "},{"title":"Communications Liaison","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#communications-liaison","content":"Can be external, internal, or both Notifies customers of current conditions and informs the Incident Commander of relevant feedback Crafts language appropriate status updates and notification messages Typically a member of the Support team Notify Stakeholders Recommended to not send updates/notifications more frequently than every 20 to 30 minutes unless a big update occurs.  "},{"title":"Incident Response at Scale","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#incident-response-at-scale","content":""},{"title":"Setting this up at scale","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#setting-this-up-at-scale","content":"For a department-wide Incident Response process, you will need a few things set up to begin. This includes: An on-call schedule for a primary and backup Incident Commander (this role is team agnostic) On-call schedules for primary and backup subject matter experts (one primary and one backup for each team) Additional on-call rotations for other roles A method of paging team members (response mobilization) "},{"title":"Incident Response - Typical Sequence of Events","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#incident-response---typical-sequence-of-events","content":"Subject Matter Expert (SME) - Usually first individual to respond and solve (or escalate) If the incident is not resolved by the SME and the incident worsens and becomes a major incident (war room situation), an escalation will be triggered. Once that incident is triggered, the Incident Commander, Scribe, Deputy, and Liaisons should be paged in. If needed, additional SMEs should be paged in.  "},{"title":"How Do Roles Scale Down","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#how-do-roles-scale-down","content":"For a small team-based incident response, you will beed a few things set up to begin. This includes: An on call schedule for primary and backup subject matter experts A method of paging out other team members  "},{"title":"How to Prepare to Manage Incident Response Teams","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#how-to-prepare-to-manage-incident-response-teams","content":"Step 1 - Ensure explicit processes and expectations exist. Step 2 - Set up runbooks and automated actions Create priority based triages Step 3 - Find ways to create more space for your teams to work Step 4 - Make Checklists (refer to Checklist Manifesto Notes) Mitigate risk and minimize mistakes wherever possible. Checklists also create break points and opportunities to step back where needed. Step 5 - Practice Running Major Incidents as a Team. (Have a failure Friday? Intentionally break something and create an incident and practice as a team) "},{"title":"Incident Response Pitfalls and How to Avoid Them","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#incident-response-pitfalls-and-how-to-avoid-them","content":""},{"title":"Anti-patterns","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#anti-patterns","content":"Executive joins the call. \"Ignore the IC, Do what I say!\" - remember the IC is the ultimate authority on the war room. Strategy to avoid this: You can ask that executive, \"Do you wish to take command?\" You can state, \"We understand your concern, but your slowing down the process of resolving the issue.\" Executive joins the call. \"Can I get a spreadsheet of all the affected customers?\" Strategy to avoid this: You can state, \"We can either get you that list, or fix the incident. Not both. The incident takes priority.\" Executive joins the call. \"Is this really a Sev-1?\" Strategy to avoid this: Do not discuss severity during the call. Don't debate severity during call. When it doubt, default to a higher severity. The place for the severity discussion is during the post mortem. Failure to Notify Customers Getting Everyone on the Call / Forcing Everyone to Stay on the Call Don't do this. No one wants to be on calls they are not needed for. Let people drop off if they are not needed.  Being overly focused on an issue. Keep bigger picture in mind. Try not to get tunnel vision.  Requiring deeply technical Incident Commanders Don't need deep technical knowledge. Rely on SMEs for tech know how.  Taking on multiple roles Don't take on multiple roles. Do your job and trust others to do theirs.  Litigating policy during an incident. Don't try and adjust policy or process during an incident. This should be handled during the postmortem.  The Belligerent Responder If there is an unruly person on the call (strong egos, big opinions), be firm and let the person know they are being disruptive. Let them know of their behavior and state they will be removed if they continue. No second chances.  Handoffs are encouraged Incident commanders are people too. When fatigue sets in, it's important to hand off the incident commander role to someone else.  "},{"title":"Follow Up and Postmortem","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#follow-up-and-postmortem","content":""},{"title":"After Incident Resolution","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#after-incident-resolution","content":"Don't neglect the postmortem. Without a postmortem you will fail to recognize what is going right and, most importantly, how to avoid the same situation in future. An incident is not considered complete until a postmortem is published. BLAMELESS - you can't fire your way to reliability When there is a culture of blame, engineers will hesitate to speak up and this results in longer resolution times. Psychological safety. Review the incident response process during postmortem. "},{"title":"Summary","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#summary","content":"An incident is any unplanned disruption or event that is actively affecting customers' ability to use the product. A major incident requires a coordinated response between multiple teams. Anyone can trigger the Incident Response Process at any time. Incident Response is the practice of developing, practicing, and refining incident strategy. The goals of Incident Response are to limit damage and its associated costs. Use ICS only for major incidents. Don't neglect the postmortem. The postmortem is just as important as the resolution process. Incident Command is a cycle that repeats until an incident is resolved. Size-Up Stabilize Update Verify  Each role specializes in a specific task to streamline response. Automate responses and workflows based on machine learning.  Getting started takes buy-in, preparation, and practice. Create clearly defined definitions and process Set up runbooks and other automated actions Make Checklists Practice running major incidents as a team.  "},{"title":"Other Incident Response Resources","type":1,"pageTitle":"Incident Command System","url":"docs/sre/incidents/incident-command-system#other-incident-response-resources","content":"PagerDuty Incident Response Optum SRE Incident Response Google - Managing Incidents Optum Percipio - Exploring Incident Management "},{"title":"SRE","type":0,"sectionRef":"#","url":"docs/sre/sre","content":"Placeholder for new SRE documentation.","keywords":""},{"title":"observability","type":0,"sectionRef":"#","url":"docs/sre/observability","content":"","keywords":""},{"title":"Pillars of Observability","type":1,"pageTitle":"observability","url":"docs/sre/observability#pillars-of-observability","content":"Events - Immutable record of discrete events that happen over time. Metrics - Numbers describing a particular process or activity measured over intervals of time. Tracing - Data that shows which line of code is failing to gain better visibility at the individual user level for events that have occurred. "},{"title":"Observability vs Monitoring","type":1,"pageTitle":"observability","url":"docs/sre/observability#observability-vs-monitoring","content":"Observability is often mistakenly interchanged with monitoring. Monitoring\tObservabilityTells if the system is working\tLet's you ask why it's not working A collection of metrics and logs about a system\tThe dissemination of information from that system Failure-centric\tUnderstand system behavior regardless of an outage Is \"the how\" / Something you do\tIs \"the goal\" / Something you have I monitor you\tYou make yourself observable "},{"title":"Culture of Observability","type":1,"pageTitle":"observability","url":"docs/sre/observability#culture-of-observability","content":"Observability is not a replacement for monitoring; they are complementary. It's difficult to implement effective monitoring without a culture of observability. Tools by themselves are not sufficient alone and none are going to automatically give observability. Observability as a culture is the degree to which a team or company values the ability to observe, inspect, and understand systems, their workload, and their behavior. Code isn't done until you've built analytics to observe and support it. "},{"title":"Modern Event Handling Techniques","type":1,"pageTitle":"observability","url":"docs/sre/observability#modern-event-handling-techniques","content":"Three techniques used in handling events with the end goal of shared insights, a collaborative response, data-enabled IT, and intelligent operations. Collect All Relevant Data - This allows complete visibility across stacks, technologies, and environments Cloud NativeTraditional, on-premises, monolithic, etc.Hybrid environments De-spam - Separate valuable signals from the noise. Add Context - Prioritize resolution to ensure service availability and to provide business detail. "},{"title":"Metrics that Matter","type":1,"pageTitle":"observability","url":"docs/sre/observability#metrics-that-matter","content":"Metrics System Metrics (CPU, memory, disk)Infrastructure metrics (AWS CloudWatch)Web tracking scripts (Google Analytics)Application agents (APM, error tracking)Business metrics (revenue, customer signups, bounce rate, cart abandonment) Events Events come in three forms - plain text, structured, and binary. System and server logs (syslog, journald)Firewall and intrusion detection system logsSocial media feeds (Twitter, etc)Application, platform, and server logs (log4j, log4net, Apache, MySQL, AWS) "},{"title":"Sources","type":1,"pageTitle":"observability","url":"docs/sre/observability#sources","content":"\"Observability,\" Wikipedia, 2018 Ernest Mueller, \"Monitoring and Observability,\" www.agileadmin.com Splunk Beginners Guide to Observability "},{"title":"Postmortems","type":0,"sectionRef":"#","url":"docs/sre/incidents/postmortems","content":"","keywords":""},{"title":"Purpose","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#purpose","content":"The intention of this document is to help with understanding the value behind running consistent postmortems, defining what a postmortem is and best practices for them, identifying different forms of biases in the workplace and how to combat them, and how to build a culture of blamelessness and continuous learning. "},{"title":"Contents","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#contents","content":"Postmortem Basics and Introduction Blame and Biases in the Workplace Introducing Postmortems to your Organization The Postmortem Report Putting Concept Into Practice Quick Tips for Success Summary Other Postmortem Resources "},{"title":"Postmortem Basics and Introduction","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#postmortem-basics-and-introduction","content":""},{"title":"Why to Do Postmortems","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#why-to-do-postmortems","content":"Postmortems provide a peacetime opportunity to dig deep into what what wrong and what went well to improve your incident response process. A postmortem is a process intended to help you learn from incidents. It typically involves a blame free analysis and discussion soon after an incident takes place. Other ways to describe a postmortem: After-Action ReviewPost-Incident ReviewLearning ReviewIncident ReviewIncident Report Avoid Root Cause Analysis (RCA) whenever possible. This is because there are often many contributing factors to the cause of an incident. The postmortem process drives focus, instills a culture of learning, and identifies opportunities for improvement that otherwise would be lost. Write postmortems in a way in which people actually want to read them. "},{"title":"When to Do Postmortems","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#when-to-do-postmortems","content":"Do a postmortem after every major incident. "},{"title":"Who is Responsible for the Postmortem","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#who-is-responsible-for-the-postmortem","content":"The Incident Commander selects and directly notifies one responder to own the postmortem. The owner of the postmortem is not the sole person responsible for completing the postmortem. Writing a postmortem is a collaborative effort and should include those involved in the incident resolution process. Important to designate a single owner. Avoid the bystander effect. "},{"title":"Ownership Criteria","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#ownership-criteria","content":"Designate a single owner. Took leadership role during the incidentPerformed a task that led to stabilizing the serviceWas the primary on-call responder for the most heavily affected serviceManually triggered the incident to initiate incident response  The owner of a postmortem is an accountable individual who performs the administrative tasks needed to complete the postmortem. They follow up for information, they drive the postmortem to completion. Postmortems are not a punishment Very rarely a single cause to any incident. More often than not, multiple sources can be identified. "},{"title":"Blame and Biases in the Workplace","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#blame-and-biases-in-the-workplace","content":"Blame is not healthy for our team. Failure is inevidible in large systems. Old way of thinking about human error: human error causes problems. New way of thinking about human error: human error is a symptom of a systemic problem. The impulse to blame and punish has the unintended effect of disincentivizing the knowledge sharing required to learn from incidents. The goal of the postmortem is to understand the systemic factors leading to the incident and identify actions that can improve the resiliency of the affected systems. "},{"title":"Why Blamelessness is Hard","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#why-blamelessness-is-hard","content":"J. Paul Reed argues that blamelessness is a myth and is unachievable by humans. \"Humans are hardwired through millions of years of evolutionary neurobiology and thousands of years of social conditioning to use the technique of blaming as a way to give voice to painful and uncomfortable feelings, in order to effectively disperse them from our psyches.\" It is more productive to be blame aware. Becoming aware of our biases, we can identify when they occur and work to move past them. "},{"title":"Common Biases","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#common-biases","content":"Fundamental Attribution Error - Tendency to believe what people do reflects their character rather than their circumstances. This bias is tied to the old way of thinking about human error (assigning responsibility to bad actors). To combat this bias, try to intentionally focus on situational causes rather than on discrete actions individuals take. Discuss 'what' questions instead of 'who'. Focus on the system, infrastructure, and the situation - not the people involved.  Confirmation Bias - Tendency to favor information that reinforces existing beliefs/positions. To combat this bias, appoint someone to play devils advocate to take a different view during the investigation.  Hindsight Bias - Seeing the incident as inevitable despite there having been little or no objective basis for predicting it because we know the outcome. Memory distortion where we recall events to form a judgement. We often recall events in a way that makes ourselves look better. To combat this bias, explain events in terms of foresight rather than the hindsight. Start your timeline analysis from a point before the incident and work forward, rather than from resolution and working backwards.  Negativity Bias - Notion that things of a more negative nature have a greater effect on our mental state than those things that are of neutral or positive nature. Things tend to go right much more frequently than they go wrong, but it is in our nature as humans to focus much more energy and importance on what goes wrong.Focusing on, exaggerating, and internalizing incidents as negative events can be demoralizing and can lead to burnout. Rethink/Rephrase incidents as learning opportunities and an opportunity to make our applications and services better.  "},{"title":"How to Avoid Blame","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#how-to-avoid-blame","content":"Ask \"what\" and \"how\" questions rather than \"who\" or \"why\". Ask why a reasonable, rational, and decent person may have taken a particular action. Avoid victim vs villain mentalities. Consider multiple and diverse perspectives. Abstract to an inspecific responder. "},{"title":"Introducing Postmortems to Your Organization","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#introducing-postmortems-to-your-organization","content":"As leaders, you must help introduce blameless postmortems. Driving culture change is difficult. Get buy in from contributors and leadership. Sell the business value of blamelessness. Acknowledge that practicing blamelessness is difficult for everyone. Blame is harmful to trust and collaboration. Work together to be more blame aware. Build a culture of continuous learning. Adopt culture change that promotes blamelessnessRecognize and reduce patterns that make a team susceptible to blame "},{"title":"Psychological Safety","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#psychological-safety","content":"For a blameless culture to exist, people need to feel comfortable and safe when talking about failure. They need to feel safe before they will be willing to talk about incidents. \"Psychological safety is a sense of confidence the team will not embarrass, reject, or punish someone for speaking up.\" - Amy Edmondson, Harvard Business School The number 1 thing you can do for your teams is to build a culture of psychological safety with blameless postmortems. Psychological safety is a key driver of high performing software delivery teams. "},{"title":"The Postmortem Report","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#the-postmortem-report","content":"Start analysis by looking at the monitoring attached to affected services. Search for irregularities from the baseline such as spikes or flatlines when the incident began and leading up to the incident. Ask why the system was designed in a way to make the incident possible. Why did those design decisions seem to be the best decision at the time? Answering these questions can be helpful in uncovering the contributing factors that lead to the incident. Participants in a postmortem meeting: Incident Commander Incident Commander Shadow, Scribe, Deputy, Liaison Service Owners Product Managers Engineering Managers "},{"title":"Important Concepts to Include in Every Analysis","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#important-concepts-to-include-in-every-analysis","content":"Avoid the concept of \"human error\"Discuss incidents within the frame of the health and resiliency of affected services Conducting effecting meetings Documenting an accurate timeline of eventsEffective action items An individual's action should never be considered a root cause. "},{"title":"Helpful Questions","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#helpful-questions","content":"Is this an isolated incident or part of a trend? Was there a specific bug, a failure in a class of problem we anticipated, or did it uncover a class of issue we did not architecturally anticipate? Was there work the team chose not to do in the past that contributed to this incident? Research if there were any similar or related incidents in the past. Does this incident demonstrate a larger trend in your system? Will this class of issue get worse or more likely as you continue to grow and scale the use of the service?  "},{"title":"Follow Up Actions","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#follow-up-actions","content":"Actionable Phrase each action item as a sentence start with a verbThe action should result in a useful outcome Specific Define each action item's scope as narrowly as possibleMake it clear what is in and out of scope Bounded Word each action item to indicate how to tell when it is finishedAvoid open-ended or ongoing tasks "},{"title":"External Messaging","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#external-messaging","content":"Different than internal messaging. External messaging is a summarized and sanitized version of information used internally. The goal of external messaging is to build trust with customers. Give enough information about what happened and what you are doing about it without revealing too much information. Summary - Two or three sentences that summarize the duration of the incident, and the observable customer impact. What Happened - Summary of contributing factors. Summary of customer-facing impact during the incident. Summary of mitigation efforts during the incident. What Are We Doing About This - Summary of action items. "},{"title":"Postmortem Review","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#postmortem-review","content":"Postmortem Writing & Collaboration Community Does the postmortem provide enough detail? Rather than just pointing out what went wrong, does it drill down to the underlying causes of the issue? Does it separate \"What happened?\" from \"How to fix it?\" Do the proposed action items make sense? Are they well-scoped enough? Is the postmortem well-written and understandable? Does the external message resonate with customers or is it likely to cause outrage? "},{"title":"Do's and Don'ts of Postmortem Review","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#dos-and-donts-of-postmortem-review","content":"Dos: Make sure timeline is an accurate representation of events. Define technical lingo for newcomers. Separate what happened from how to fix it. Discuss how incidents fit into health and resiliency of services. Don'ts: Don't use the word \"outage\" unless it really was a full outage. Don't change details to make things \"look better.\" Don't name and shame. Avoid the concept of \"human error.\" "},{"title":"Putting Concept into Practice","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#putting-concept-into-practice","content":"Create a community of experienced postmortem writers to review drafts and spread good practices. Information Sharing Scale culture through sharing. Adopt practices that promote openness and sharing of information. People want to share their successes and people want to replicate practices that result in success. Information sharing and transparency promotes a culture of accountability. Schedule postmortem meetings on a shared calendar. Email completed postmortems to all teams involved in incident response. An alternative to this may be to put postmortems in a public GitHub repository. "},{"title":"Quick Tips for Success","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#quick-tips-for-success","content":"Develop good facilitators. The facilitator has a different role than others on the postmortem call/meeting/discussion. They will not share their opinions or participate in the discussion. They will keep the meeting on track and nothing more. Facilitator Responsibilities: Reads non-verbal cues to assess how people are feeling. Sees who might have something to say. Paraphrases what is said to clarify for self and others. Asks open questions that stimulate deeper thinking. Comfortable interrupting when discussion gets off track or someone is dominating the conversation. Redirects conversation to focus on goals. Drives discussion to decision making and action items. Develop a community of experienced facilitators to develop good practices.  Start Small Being transparent about system failure reinforces a culture of blamelessness. Clarify policy and ownership of postmortem action items. Engage leaders that prioritize work. An essential outcome of the postmortem meeting is buy-in for the action plan. There is no single root cause of major failure in complex systems, but rather a combination of contributing factors that together lead to failure. An individual's action should never be considered a root cause. \"All practitioner actions are actually gambles, that is, acts that take place in the face of uncertain outcomes.\" - Dr Richard Cook, Department of Integrated Systems Engineering at Ohio State University. Avoid Blame. Practice makes perfect. "},{"title":"Summary","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#summary","content":"Improve Team Culture Postmortems can drive a culture of continuous improvement. The number one thing you can do for your teams is support a culture of psychological safety.  Reduce Blame Blame disincentivizes the knowledge sharing required to improve system resiliency. An individual's actions should never be considered a root cause.  Focus on Circumstances Ask \"what\" and \"how\" questions, rather than \"who\" and \"why.\" There is no single root cause, but a combination of contributing factors.  Take Action Understand systemic factors that led to the incident An essential outcome of the postmortem meeting is buy-in for the action plan "},{"title":"Other Postmortem Resources","type":1,"pageTitle":"Postmortems","url":"docs/sre/incidents/postmortems#other-postmortem-resources","content":"PagerDuty Postmortems Google Postmortem Culture: Learning from Failure Awesome SRE Postmortem Etsy Blameless Postmortem "},{"title":"Reliability Roadmap","type":0,"sectionRef":"#","url":"docs/sre/reliability/reliability-roadmap","content":"","keywords":""},{"title":"SRE Phases Uplift Alignment","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#sre-phases-uplift-alignment","content":""},{"title":"Phase 0 - Base-level understanding of current capabilities, categorization of application","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#phase-0---base-level-understanding-of-current-capabilities-categorization-of-application","content":"Review current monitoring tools in place Review current alerting strategy Review Incident Response Review On Call strategy Review previous incidents for critical issues/known-problems Review Application Architecture & Design "},{"title":"Phase 1 - Extinguish immediate fires, immediate risk mitigation, configure basic monitoring if missing","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#phase-1---extinguish-immediate-fires-immediate-risk-mitigation-configure-basic-monitoring-if-missing","content":"Telemetry Monitoring Host Level Checks - Multiple checks using tools like Sensu, Nagios, etc to check on various host statistics. (CPU, Memory/Heap, GC, Network I/O, Disk I/O, etc) Log File Analysis - Use of a tool such as Splunk or ELK stack to parse log files in real-time Application Performance Monitor configuration (APM) Synthetics - Use of a custom agent to \"probe\" a web site, execute complex customer actions, and report on success/failure of those actions. Implement emergency Architecture changes Validate Readiness for Performance Testing LTM Checks - HealthChecks On-Call Rotation - Who is responsible for being notified of a potential problem Alerting coverage & MTTD - Ability for teams to be systematically notified when specific KPIs are outside normal operating thresholds Validate Built/Hosted Environments (QA, Stage, Prod) are identical "},{"title":"Phase 2 - Prepare for Long-Term Success","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#phase-2---prepare-for-long-term-success","content":"Metric Resolution & Data Types - Ability for metrics to be collected and reviewed, in real time, with less than a 30 second delay VIP-Level Checks - Multiple checks against the VIP level for Health-Check and Service availability Real-User-Monitoring (RUM) - Capture of live, real-customer render timings, usage, analytics. Captured as Telemetry data and used for various purposes (Session monitoring, customer engagement, browser performance Advanced Splunk Configuration (Splunk ITSI if available) WHAT DOES THIS CONSIST OF? Advanced Data Visualization WHAT DOES THIS CONSIST OF? Advanced Alerting and configuration WHAT DOES THIS CONSIST OF? "},{"title":"Phase 3 - Advanced Operations","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#phase-3---advanced-operations","content":"Automated Recovery Automated Release Advanced Machine-Learning analytics GTM/LTM Control of Hosts - Is the LTM able to selective remove Application servers from it's eligibility pool? Experimentation - What type of tests are conducted to validate application performance? KPIs/Custom Metrics - Identification of Key-Performance-Indicators and custom metrics to support their measurement KPI Predictions - Use of advanced techniques to predict KPI trends over X time "},{"title":"Reliability Checklist","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#reliability-checklist","content":""},{"title":"Document in-use monitoring tools","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#document-in-use-monitoring-tools","content":"Identify and document all of the monitoring tools that are presently implemented to support the application for: Host Level Checks (CPU/Mem/Disk/Swap/etc)Up/Down Probes (assuming this means non-http checks such as snmp, ping/icmp, tcp, udp, etc)SyntheticsApplication Performance Monitoring (APM)Log file aggregationTelemetryReal User Monitoring (RUM)Javascript Front End LoggingDashboard solutionsAny other relevant monitoring tools Completion Criteria: Produce a report with these findings articulated. "},{"title":"Document current alerting strategy","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#document-current-alerting-strategy","content":"Analyze alerts configured in production to determine the following: What tools are being used to generate alerts? (SiteScope, Alertmanager, Splunk, etc) What metrics are being leveraged to generate alerts? (CPU Utilization, Response Time, HTTP 5xx, RER, etc) How are alerts distributed (OpsBridge, PagerDuty, direct email, sms, ChatOps, etc)Who do the alerts go to? (Distribution List, Individuals, on-call rotation, etc)How are alerts classified (P1/P2, Alert/Ticket, Sev1, etc)How does the team decide on alert thresholds?Are alert thresholds static or dynamic?How many alerts fire per week?What percentage of alerts are actionable?If able to determine, what is the Mean Time to Detect (MTTD)? Completion Criteria: Produce a report with these findings articulated. "},{"title":"Document Supply Chain and Dependencies","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#document-supply-chain-and-dependencies","content":"Identify and document all of the back end dependencies of the application. What applications does our application rely on?Who manages back end applications?What monitoring do back end dependencies leverage?How are alerts implemented in downstream applications?Implement observability of dependencies "},{"title":"Establish and document business impact measurements","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#establish-and-document-business-impact-measurements","content":"Determine and document last 6 months of the following metrics for the application: Raw Error Rate (ReR)Mean Time To Restore (MTTR) (including sub components if possible - MTTD, MTTA, MTTI, MTTF)Service Level Agreements / Service Level Objectives / Service Level ExpectationsAdjusted Down Time in Minutes (ADTM)Net Promoter Score (NPS) Completion Criteria: Produce a report with these metrics articulated. "},{"title":"Document Incident Response Procedures","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#document-incident-response-procedures","content":"By using any available reporting and by working with the DevOps team, determine the following: Are there playbooks available for common tasks, if so where are they stored? (Application restart, DC traffic switching, etc).How does the team handle Post Incident Reviews (PIRs)?How successful are root cause analysis reviews?Average time to problem ticket closureDoes the team leverage an \"Incident Commander\" or \"Technical Lead\" during war-rooms?Any other relevant info about Incident Response Completion Criteria: Produce a report with these findings articulated. "},{"title":"Discover and Document Application Architecture & Design","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#discover-and-document-application-architecture--design","content":"Work with the application team and utilize any existing architectural documentation to determine the current design of the application. This should include: What is the hosting infrastructure (Public / Private cloud, Managed Services, XaaS, etc)Is this a commercial product, or are there commercial products in the ecosystem? (i.e. Adobe Experience Manager)Describe vendor support model and contentEnumerate all the nodes in the eco systemDocument the network topology (Data Center, GTM, LTM, Web Servers, App Servers, DB Servers, Caching, Queuing, etc)How does traffic normally flow?What are the connections between nodesIs the app available in Navigator with configuration up to dateDescribe load balancing strategyDC load balancingApp load balancingDescribe any dependencies or single points of failureAny other relevant architectural considerations Completion Criteria: Produce a report with an overview of these findings, along with links to all the most updated documents and diagrams. Call out any obvious gaps that should be addressed in future phases. "},{"title":"Implement emergency Architecture changes","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement-emergency-architecture-changes","content":"Emergency architecture recommendations should address the following: Any system or network architecture that is a single point of failure and can be easily modified at low cost to reduce or eliminate SPOF riskAny system or network architecture change that can be implemented to enable High AvailabilityHost level load balancing strategyData Center Load balancing StrategyCloud Region load balancing strategyCapacity concerns (vertical/horizontal scaling) Completion Criteria: All emergency architecture changes have been implemented and a document has been produced describing each of the recommended changes and validating it's completion. "},{"title":"Document On-Call strategy","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#document-on-call-strategy","content":"Work with the DevOps team to determine the following: Does the team have a structure and plan for handling an on-call rotationIs it a single human?Do they have \"always on\" follow the sun?How are on-call schedules determined?How often is someone on call?How long is someone on call for?Are there defined SLAs for acknowledging a page?On average how many alerts fire during a given on-call shift?How do people find out if they are on-call or not?How are escalations handled?How are schedule overrides handled?If available, what is MTTA?Any other relevant info about On-Call strategy Completion Criteria: Produce a report with these findings articulated. "},{"title":"Review and Improve Alerting Strategy","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-alerting-strategy","content":"Copy - Produce an Alerting Strategy document that describes the strategy and approach the team will take when crafting alerts in each monitoring system. If necessary implement changes to the strategy that enable the following: - All alerts should be set with the SLA/SLE in mind - All alerts that page a human should be actionable - Alerts that are not actionable should be converted to either tickets or logging - Thresholds are set for alerts, and should be revisited - Dynamic thresholds for alerts should be utilized when appropriate - The team should construct alerts in service of the shortest reasonable MTTD (Strive for <10 min) - Alerting strategy should minimize noise (alert storms / non-actionable alerts) - Who will alerts go to? Does this change under certain circumstances (Priority, service, etc)? - All members of the team have reviewed the alerting strategy document and have committed to following it in the future, or updating it as a team - Completion Criteria: SRE and the DevOps team should work together to produce a document that describes the teams Alerting Strategy. The epic is complete when the document has been completed and agreed to by the team. "},{"title":"Review and Improve On-Call Rotation","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-on-call-rotation","content":"Copy - Work with the team to set up an On-Call rotation that minimizes MTTA, while being realistic about the human factors. Produce a document that specifies the on-call process and expectations. The on-call rotation should have the following attributes: - On-call rotation should be in a system that the organization has access to. - This team as well as others must know who is on call at any given moment - One specific human being must be on call at any given time - No one should be on call all the time - There should be a clear delineation of when a person goes on-call and goes off-call - There should be a handoff procedure to share context between the person going off and the person coming on - Everyone in the rotation should have correct notification preferences/settings (email, SMS, etc) - There should be an escalation process - There should be an override process - There should be a clear definition about what it means to be on-call - Always have your laptop with you - Always have your phone with you and be in service area - Always be near an internet connection and able to leverage VPN - Can be online and connected in less than 20 minutes - MTTA less than 10 minutes - Be in a mental state conducive to complex problem solving - Determine if there are any special considerations for on-call employees - Do we need to pay them differently? - Do we need to provide phone/laptop, etc? - Ensure all team members understand the on-call process and the expectations of being on-call - Completion Criteria: An on-call rotation has been setup to SRE standards and is in practice. The SRE and the DevOps team have produced a document that describes the on-call process and expectations. "},{"title":"Review and Improve Host-Level Checks","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-host-level-checks","content":"Copy - Ensure that all hosts are observable in a tool like OpsBridge, Sensu or Nagios. Ensure critical host statistics are available: - CPU - Memory/Heap - GC - Network I/O - Disk I/O - Any other relevant host level metrics - Additional required actions: - Alerts should be set up following the Alerting Strategy - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Completion Criteria: All hosts must have relevant statistics available in a monitoring tool and be configured to alert according to the E1.2 alerting strategy. . Preferably with the ability to view time-charts by host or in aggregate. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Synthetic Monitoring","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-synthetic-monitoring","content":"Copy - Synthetic monitoring is an important part of the \"defense in depth\" strategy of observability. Depending on the \"customer\" of the app (internal or external) synthetic recommendations may vary. The App should utilize a tool like BSM, SiteScope, Dynatrace or Catchpoint for synthetic transactions. Identify the non-negotiable items and work to have them implemented. - Required: - The application must leverage simple internal synthetic transactions to validate expected response on a key endpoints (Up/Down). - The team must have a cadence and process to periodically review all synthetic transactions as they may change as the app evolves. - There must be no regularly occurring maintenance windows were transactions are paused. - Alerts should be set up following the Alerting Strategy - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Recommended: - If the app is user facing (i.e. has a user interface for humans to interact with) it must leverage complex synthetic transactions to simulate critical customer journeys and exercise key integrations - If the app is internet facing it must leverage both internal synthetics (run from inside the network) and external synthetics (run from outside the network). - Additionally, the app should run external synthetics from at least two geographic regions (for example, east coast / west coast) - Synthetic probes / transactions should find a balance between testing individual hosts and validating VBFs and Customer Objectives - Synthetic transactions should be staggered so as to minimize gaps between runs, there by maximizing MTTD. - Synthetic transaction results should be visualized on a dashboard, preferably a time-series dashboard - Completion Required synthetic monitoring must be implemented and configured to alert according to the E1.2 alerting strategy. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Log File Aggregation solution","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-log-file-aggregation-solution","content":"Copy - Identify the non-negotiable items and work to have them implemented. - Required: - The application must leverage a log file aggregation tool (Splunk / Elk), if not implement one. - All nodes in the app must be shipping logs to the aggregation tool - Log formats must comply with industry standards and Optum best practices - There should be no blind spots or gaps in logs - Logs should be properly parsed by the aggregation tool (status codes, response times, etc) - Build dashboards based on log data to visualize KPIs and VBFs - Must have an RER Dashboard - Alerts should be set up following the Alerting Strategy in E1.2 - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Recommended: - Prefer Splunk - Prefer phi-splunk instance - Gain access to all supply-chain log aggregation tools - Build app specific dashboards on supply chain log aggregation tools - Completion Criteria: All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Application Performance Monitor (APM) solution","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-application-performance-monitor-apm-solution","content":"Copy - Identify the non-negotiable items and work to have them implemented. - Required: - The application should leverage an APM solution (Dynatrace / New Relic). If the app and team are sufficiently mature, an APM may not be required, but this is rare and should be highly scrutinized. In most cases an APM solution should be utilized. - Asses the solution being used (DTSaaS, AppMon, etc) is it that right one for this app? Are changes needed? If so implement them. - All nodes should have the APM agent installed and properly configured - Ensure all KPIs and VBFs are observable in the APM - Build dashboards based to visualize KPIs and VBFs - Alerts should be set up following the Alerting Strategy in E1.2 - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Recommended: - Prefer Dynatrace - Gain Access to all supply-chain APM solutions - Build app specific dashboards on supply chain APM tools - Completion Criteria: All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Web/App Analytics solution","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-webapp-analytics-solution","content":"Copy - This epic is only applicable if the application has a user interface used by humans. If it does, the app must leverage a front-end analytics solution of some kind (Adobe Analytics, Google Analytics etc). This is applicable for web applications, mobile applications and thick client applications. Identify the non-negotiable items and work to have them implemented. Front-end analytics are in important part of the defense-in-depth monitoring strategy. Front-end analytics enable observability at the node closest to the end customer. - Required: - The app must leverage a front-end analytics solution of some kind (Adobe Analytics, Google Analytics) - Ensure all KPIs and VBFs are observable in the analytics solution (all pages or screens should be tagged for inclusion - Build dashboards based to visualize KPIs and VBFs - All error pages must be tagged and included in the analytics solution (4xx, 5xx) - The team should understand how analytics indicators relate to NPS scores - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Recommended: - Build dashboards to visualize key customer journeys (funnels) - Tag and track all error \"states\". These are scenario's that result in a 200 status code, but an error of some kind is shown to the customer (invalid actions, etc) - Completion Criteria: All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Performance Testing Strategy","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-performance-testing-strategy","content":"Copy - Determine if the team has a dedicated performance testing strategy and plan. Observe at least one production performance test and document any improvements that are identified. Identify the non-negotiable items and work to have them implemented. - Required: - Performance tests should be executed at least 4 times per year - The performance testing strategy must mirror real customer interactions as closely as possible - The application must be sufficiently instrumented with monitoring tools to draw conclusions from the performance test about the scalability of the application - The application should have a scaling strategy to meet additional load (horizontal scaling, vertical scaling, auto-scaling, manual scaling, etc) - Document and evaluate the metrics that trigger a scaling event - Performance test load should simulate 1x Peak Season predictions - Recommended: - There should be specific monitoring dashboards used to monitor KPI's during a performance test - Performance tests should be executed in production environments - Performance test load should simulate 2x Peak Season predictions - Completion Criteria: The SRE has observed at least one performance test and documented recommendations. All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Local Traffic Manager (LTM) Checks","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-local-traffic-manager-ltm-checks","content":"Copy - Determine if/how the LTM is monitored. Generate and document recommendations. - Validate that monitoring exists for the Load Balancer LTM - If not, implement monitoring - Ensure load balancer is truly 'balanced' and not overworking nodes - Make sure all key teams have access to the monitoring tool or dashboard for the LTM - Completion Criteria: LTM is properly monitored. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Incident Response","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-incident-response","content":"Copy - Observe at least one P1 or P2 War Room, then generate and document recommendations. Guide the team towards a culture of responsibility and ownership of their service that leads to urgency during a War Room. Identify the non-negotiable items and work to have them implemented. - Required: - The team is accurately measuring and analyzing MTTD and MTTR - There are playbooks available for common tasks stored in a common location. (Application restart, DC traffic switching, etc). - The team is following the Alerting Strategy from E1.2 to trigger war rooms - The team is following the On-Call Strategy from E1.3 to respond to Alerts - The team is conducting blameless Post Incident Reviews (PIRs) for every P1 or P2 - Root Cause Analysis is completed for every P1 and P2 - Findings and recommendations from PIRs and RCAs were shared with the team - A culture of learning is established recognizing every incident not as a failure but an opportunity to better understand our complex systems - Recommended: - The team designates an \"Incident Commander\" or \"Technical Lead\" during war-rooms - The team accurately measures and analyzes MTTR and all sub-components (MTTD, MTTA, MTTI, MTTF) - Completion Criteria: All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Review and Improve Environmental Equivalency","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#review-and-improve-environmental-equivalency","content":"Copy - Work with the team to understand the various relevant environments for the application (build, QA, Stage, Prod-Like, Productions, etc). Lower environments should be as similar to production as possible. Are there concerns from the team that testing is not valuable because lowers are too dis-similar from Prod? Generate and document recommendations. - Lower environments should be sufficiently similar to production to make testing in lowers valid - Ideally lower env config should be identical to production - Lower env should be refreshed at that same time as production - Lower env data should be refreshed on a regular schedule to closely mirror production schemas - If the lower env use stub services to simulate supply chain dependencies, the stubs should be periodically reviewed for equivalency to their production counterparts (data contract, response time, etc) - Completion Criteria: The SRE should assess that the environmental equivalency meets the needs of the application. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate Real User Monitoring (RUM) Solution","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-real-user-monitoring-rum-solution","content":"Copy - Generate and document recommendations for Real User Monitoring. The application should leverage some type of RUM Solution (Bucky.js, Dynatrace UEM, etc). RUM is an important tool in the defense-in-depth monitoring strategy. It serves to capture technical data on the clients actual device. It gives us the most accurate and actionable data about the Real User Experience. The Rum Solution should contain the following data / attributes: - Required: - Capture data at the client end (Browser, mobile device) - Capture data like: - Browser Type/Version/Capabilities - Browser Timings (Network Time, DOM Complete, Page Interactive, etc) - Have the ability to generate alerts based on the Alerting Strategy - The team should understand how RUM indicators relate to NPS scores - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Recommended: - Implement dashboard to visualize RUM data, preferably Time-Series dashboards - Capture data in a mechanism consistent with monitoring telemetry. Store the data in a time series database like InfluxDB or Graphite - Leverage the RUM Solution to implement Front End Logging to catch client side errors - Completion Criteria: RUM data is being collected on all front end experiences. All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate Telemetry Monitoring Solution","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-telemetry-monitoring-solution","content":"Copy - Generate and document recommendations for Telemetry monitoring. Identify the non-negotiable items and work to have them implemented. The application should leverage some type of Telemetry Solution (TICK, Prometheus, etc). Telemetry is an important tool in the defense-in-depth monitoring strategy. It captures critical metrics about system state at a very high frequency and and resolution. Telemetry enables significantly improved MTTD, as well as advanced KPI observability. - Required: - Method to capture telemetry data at each host at a frequency every 30 seconds or less - TimeSeries Database to store the data collected from each host - Dashboard solution to visualize the telemetry data either by individual metric or in aggregate - Method to alert on the telemetry data in accordance with the Alerting Strategy defined in E1.2 - Data should be able to be viewed in \"real-time\" with no more than a 1 minute lag. - Solution implemented on all application and web servers that support the app/service - Captured metrics should include host level data like CPU, Mem, Disk, etc - For web applications and services where applicable: Status codes, Response times, Threads, Workers - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Recommended: - Collect data at a frequency of every 10 seconds - Alert triggers configured to support a MTTD of less than 1 minute (assuming sufficient traffic loads) - Solution implemented on all nodes in the applications ecosystem - Custom Telemetry data implemented in the application for KPIs and VBFs - Solution should support zero-configuration for new hosts. Should be able to build the host with the agent and an endpoint and start collecting metrics. - This is required for auto-scaling applications - Completion Criteria: Telemetry data is being collected on all web and app servers, dashboards are available, and alerts are configured. All \"required\" items have been implemented or completed. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate VIP-Level Checks","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-vip-level-checks","content":"Copy - Determine if/how the VIP's are monitored. Generate and document recommendations. - Validate that monitoring exists for the associated VIP's - If not, implement monitoring - Make sure all key teams have access to the monitoring tool or dashboard for the VIP - Completion Criteria: VIP's are properly monitored. Produce a report that outlines all of the recommendation that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate Advanced Splunk Configuration","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-advanced-splunk-configuration","content":"Copy - If the application is using a log file aggregation tool other than Splunk, get the app on-boarded to phi-splunk.optum.com. Enable advanced Splunk features like Splunk AI, and IT Service Intelligence (ITSI). Generate and document recommendations. - Application must on-boarded to phi-splunk.optum.com - Enable and configure ITSI and Glass Tables - Enable Splunk AI Event Analytics - Configure advanced dashboards that tie back to customer experiences and VBFs - Leverage high quality and efficient splunk searches (base search, selection boxes, etc) - Configure alerts in accordance with the Alerting Strategy defined in E1.2 - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Completion Criteria: Splunk advanced features have been enabled and the team understands how they fit into the defense-in-depth monitoring strategy. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate Advanced Data Visualization","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-advanced-data-visualization","content":"Copy - Based on the all the previous epics, data acquired across multiple monitoring tools should be aggregated and visualized as dashboards to highlight KPIs and VBFs. Dashboards show actionable metrics & avoid ego metrics. Generate and document recommendations. - Leverage a dashboard solution that can pull data from multiple sources, such as Grafana, Kibana, etc. - Integrate as many key metrics from different systems into the dashboard solutions as feasible to eliminate tools switching for high level observability - Focus on the metrics that matter. If you could pick only one metric to visualize the health of the system what would it be? - In e-commerce, this might be orders per minute. For Netflix it is Stream Starts per second - The metric should encapsulate many of the key services that make up the application - The metric should represent an action that you would sacrifice other pieces of functionality to preserve - Enable High Level dashboards with the ability to drill in - These dashboards should be open and available to anyone in the organization, promoting data transparency - Build clusters of dashboards around VBFs or activities (Release Dashboards, Incident Dashboards, etc) - Make sure all key teams have access to the tool - Train all key teams on the tool and how it fits into the monitoring strategy - Completion Criteria: An advanced data visualization tool has been implemented. Useful dashboards have been built. The team knows how to use the dashboards, update them and create new ones. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate Advanced Alerting","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-advanced-alerting","content":"Enable advanced alerting techniques. This is highly specific to the application, thus the SRE will need to generate and document specific recommendations for the team using the below items as guidelines. Copy - The team leverages a persistent chat tool like flowdock or Slack or Cisco Spark - The team understands the core concepts of ChatOps - The team is leveraging bots in their chat tool to pipe in alerts and is able to take action like acknowledging or escalating them - Achieve <10 min MTTD - The team is using dynamic thresholds for most alerts - Alerts configured on absence conditions (0 orders placed, etc) - Minimal false positive alerts - Majority of alerts are actionable - Alerts are consistently tuned as the applications changes over time - Confidence in alerts is high enough to enable automated actions Completion Criteria: Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant tools, systems and dashboards. "},{"title":"Implement / Validate Advanced Incident response","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-advanced-incident-response","content":"Copy - Observe at least one war room. Asses and shape the team's Incident response culture and performance. Generate and document recommendations. - The team is mature in their on-call rotation strategy, with only one person on call at a time - The team is leveraging a multi-modal user defined notification tool (sms, phone, push, chat, email, etc) with automatic timeout escalation - MTTA is less than 5 minutes - The team is using an Incident Commander model, clearly identifying the one person on the War Room who is responsible for making decisions - The Incident commander leverages resources at their disposal to nominate a second in command, scribe, external comms resource if necessary - The team focuses on fast path to resolution, making hard decisions in service of the customer experience - The team is fully engaged in the chat ops model. They keep track of timelines and execute actions via persistent chat, while also utilizing a voice call - The team is leveraging self-service war-rooms - The team is conducting simulated war-rooms at least 4 times a year - PIRs are blameless and productive learning sessions - Root cause or contributing causes are identified within one week - Completion Criteria: The team has demonstrated a mature Incident response process and culture. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Introduce Chaos Engineering","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#introduce-chaos-engineering","content":"Based on all the previous epics, the application should be sufficiently reliable and resilient to meet it's SLA/SLEs. At this stage it is necessary to introduce the concepts of Chaos Engineering in order to better understand the behavior of the system. Teach and train the team on the core concepts of Chaos EngineeringPick a scenario for an initial Chaos ExperimentThis should be a relatively simple and well controlled experiment like switching traffic from one data center to the otherPlan out the experimentHypothesisSimulated P1 incidentPlaybook for actionsIdentify and limit blast radiusIndicator metricsRoll back stepsSuccess criteriaTable Top Chaos ExperimentProduction Chaos ExperimentProcedure and rhythm established for future chaos experiments Completion Criteria: A successful Chaos Experiment has been conducted in production. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Implement / Validate Automated Recovery","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-automated-recovery","content":"Copy - Based on the all the previous epics, identify the top 3 likely conditions to result in an incident and implement an automated recovery process. Generate and document recommendations. - Leverage advanced alerting techniques to identify issues - Implement a fully automated recovery process - Ensure proper logging of the automated recovery - Define a fall back recovery process if the automated process fails - Execute a Chaos Experiment to validate performance of the recovery process - Completion Criteria: A successful Chaos Experiment has been conducted to validate the automated recovery process works as expected. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Implement / Validate Advanced AI/ML","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-advanced-aiml","content":"Copy - Identify areas to implement AI/ML and advanced heuristics for analyzing trends and war room avoidance. Generate and document recommendations. - The team has implemented advanced heuristics and statistical models for alerting thresholds - Holt Winters Forecasting - Machine Learning algorithms to predict acceptable trend thresholds - Identify KPIs and make predictions about their trends and behaviors under certain circumstances - Leverage Change Control Risk Prediction - Leverage intelligent agents to manage infrastructure - Completion Criteria: Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Implement / Validate GTM/LTM Control of Hosts","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-gtmltm-control-of-hosts","content":"Copy - Implement fine-grain GTM/LTM controls so that that the team can selectively remove hosts from the pool. Generate and document recommendations. - Team should be able to selectively remove any given host from the pool - Team should be able to route traffic to a DC, AZ or pool. - Make sure all key teams have access to the tool - Train all key teams on the tool and how to utilize it in an incident scenario - Execute a Chaos Experiment to validate fine grain controls - Completion Criteria: Successfully execute a chaos experiment that shows the ability to selectively remove a host from the pool. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Implement / Validate Advanced Performance Testing","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-advanced-performance-testing","content":"Copy - Implement processes, procedures and controls to enable advanced performance testing practices. Observe at least one performance test. Generate and document recommendations. - Performance / Load tests should be run in the production environment - Performance test scripts should exercise all production dependencies - The team should be running a performance test after every release, or if CI/CD, every two weeks - Simulate at least 2x predicted Peak Season Load - At least four times a year, execute a performance test that pushes the application to the breaking point to determine max load - Completion Criteria: Perf testing in production at high frequency. At least one max load test has been performed. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Implement / Validate Immutable Infrastructure","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-immutable-infrastructure","content":"Copy - If possible, the application should shift to an immutable infrastructure architecture. This eliminates config drift and drastically simplifies trouble-shooting. Generate and document recommendations. - NEVER make a change to a production instance. - Only make change to the Pod/Container/Image, then build a new instance and deploy the new instance - Completion Criteria: Only immutable infrastructure is used in production. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "},{"title":"Implement / Validate Supply Chain Observability","type":1,"pageTitle":"Reliability Roadmap","url":"docs/sre/reliability/reliability-roadmap#implement--validate-supply-chain-observability","content":"Copy - Leverage a common construct to pass context up and down the stack. Note: there is currently no Optum Standard for how this should be implemented, hopefully one is forthcoming. Generate and document recommendations. - The below items should be present and logged in every request. If the application doesn't receive them, it should generate them and pass them downstream. - Transaction GUID - Session GUID - Top Level First Seen Timestamp - Top Level Timeout - Additionally, if the application is anything other than the first hop, it should compare the current time with the remaining time left on the top level timeout and cancel the request if it can't be completed in time to meet the Top Level Timeout. - Completion Criteria: The above listed values are logged for captured, logged and sent to downstream requests for every transaction. Produce a report that outlines all of the recommendations that SRE made, along with whether or not they were implemented (If not, why not?). Include in the report links to the relevant reports, tools, systems and dashboards. "}]